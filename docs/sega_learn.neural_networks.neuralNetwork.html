<!DOCTYPE html>
<html lang="en">
<head>
<style>
body { background-color: #f0f0f8; }
table.heading tr { background-color: #7799ee; }
.decor { color: #ffffff; }
.title-decor { background-color: #ffc8d8; color: #000000; }
.pkg-content-decor { background-color: #aa55cc; }
.index-decor { background-color: #ee77aa; }
.functions-decor { background-color: #eeaa77; }
.data-decor { background-color: #55aa55; }
.author-decor { background-color: #7799ee; }
.credits-decor { background-color: #7799ee; }
.error-decor { background-color: #bb0000; }
.grey { color: #909090; }
.white { color: #ffffff; }
.repr { color: #c040c0; }
table.heading tr td.title, table.heading tr td.extra { vertical-align: bottom; }
table.heading tr td.extra { text-align: right; }
.heading-text { font-family: helvetica, arial; }
.bigsection { font-size: larger; }
.title { font-size: x-large; }
.code { font-family: monospace; }
table { width: 100%; border-spacing: 0; border-collapse: collapse; border: 0; }
td { padding: 2; }
td.section-title, td.multicolumn { vertical-align: bottom; }
td.multicolumn { width: 25%; }
td.singlecolumn { width: 100%; }
</style>
<meta charset="utf-8">
<title>Python: module sega_learn.neural_networks.neuralNetwork</title>
</head><body>

<table class="heading">
<tr class="heading-text decor">
<td class="title">&nbsp;<br><strong class="title"><a href="sega_learn.html" class="white">sega_learn</a>.<a href="sega_learn.neural_networks.html" class="white">neural_networks</a>.neuralNetwork</strong></td>
</tr></table>
    <p></p>
<p>
<table class="section">
<tr class="decor pkg-content-decor heading-text">
<td class="section-title" colspan=3>&nbsp;<br><strong class="bigsection">Modules</strong></td></tr>
    
<tr><td class="decor pkg-content-decor"><span class="code">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span></td><td>&nbsp;</td>
<td class="singlecolumn"><table><tr><td class="multicolumn"><a href="numpy.html">numpy</a><br>
</td><td class="multicolumn"></td><td class="multicolumn"></td><td class="multicolumn"></td></tr></table></td></tr></table><p>
<table class="section">
<tr class="decor index-decor heading-text">
<td class="section-title" colspan=3>&nbsp;<br><strong class="bigsection">Classes</strong></td></tr>
    
<tr><td class="decor index-decor"><span class="code">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span></td><td>&nbsp;</td>
<td class="singlecolumn"><dl>
<dt class="heading-text"><a href="builtins.html#object">builtins.object</a>
</dt><dd>
<dl>
<dt class="heading-text"><a href="sega_learn.neural_networks.neuralNetwork.html#Activation">Activation</a>
</dt><dt class="heading-text"><a href="sega_learn.neural_networks.neuralNetwork.html#Layer">Layer</a>
</dt><dt class="heading-text"><a href="sega_learn.neural_networks.neuralNetwork.html#NeuralNetwork">NeuralNetwork</a>
</dt></dl>
</dd>
</dl>
 <p>
<table class="section">
<tr class="decor title-decor heading-text">
<td class="section-title" colspan=3>&nbsp;<br><a name="Activation">class <strong>Activation</strong></a>(<a href="builtins.html#object">builtins.object</a>)</td></tr>
    
<tr><td class="decor title-decor" rowspan=2><span class="code">&nbsp;&nbsp;&nbsp;</span></td>
<td class="decor title-decor" colspan=2><span class="code">This&nbsp;class&nbsp;contains&nbsp;various&nbsp;activation&nbsp;functions&nbsp;and&nbsp;their&nbsp;corresponding&nbsp;derivatives&nbsp;for&nbsp;use&nbsp;in&nbsp;neural&nbsp;networks.<br>
relu:&nbsp;Rectified&nbsp;Linear&nbsp;Unit&nbsp;activation&nbsp;function.&nbsp;Returns&nbsp;the&nbsp;input&nbsp;directly&nbsp;if&nbsp;it's&nbsp;positive,&nbsp;otherwise&nbsp;returns&nbsp;0.<br>
leaky_relu:&nbsp;Leaky&nbsp;ReLU&nbsp;activation&nbsp;function.&nbsp;A&nbsp;variant&nbsp;of&nbsp;ReLU&nbsp;that&nbsp;allows&nbsp;a&nbsp;small&nbsp;gradient&nbsp;when&nbsp;the&nbsp;input&nbsp;is&nbsp;negative.&nbsp;<br>
tanh:&nbsp;Hyperbolic&nbsp;tangent&nbsp;activation&nbsp;function.&nbsp;Maps&nbsp;input&nbsp;to&nbsp;range&nbsp;[-1,&nbsp;1].&nbsp;Commonly&nbsp;used&nbsp;for&nbsp;normalized&nbsp;input.<br>
sigmoid:&nbsp;Sigmoid&nbsp;activation&nbsp;function.&nbsp;Maps&nbsp;input&nbsp;to&nbsp;range&nbsp;[0,&nbsp;1].&nbsp;Commonly&nbsp;used&nbsp;for&nbsp;binary&nbsp;classification.<br>
softmax:&nbsp;Softmax&nbsp;activation&nbsp;function.&nbsp;Maps&nbsp;input&nbsp;into&nbsp;a&nbsp;probability&nbsp;distribution&nbsp;over&nbsp;multiple&nbsp;classes.<br>&nbsp;</span></td></tr>
<tr><td>&nbsp;</td>
<td class="singlecolumn">Static methods defined here:<br>
<dl><dt><a name="Activation-leaky_relu"><strong>leaky_relu</strong></a>(z, alpha=0.01)</dt><dd><span class="code">Leaky&nbsp;ReLU&nbsp;activation&nbsp;function:&nbsp;f(z)&nbsp;=&nbsp;z&nbsp;if&nbsp;z&nbsp;&gt;&nbsp;0,&nbsp;else&nbsp;alpha&nbsp;*&nbsp;z<br>
Allows&nbsp;a&nbsp;small,&nbsp;non-zero&nbsp;gradient&nbsp;when&nbsp;the&nbsp;input&nbsp;is&nbsp;negative&nbsp;to&nbsp;address&nbsp;the&nbsp;dying&nbsp;ReLU&nbsp;problem.</span></dd></dl>

<dl><dt><a name="Activation-leaky_relu_derivative"><strong>leaky_relu_derivative</strong></a>(z, alpha=0.01)</dt><dd><span class="code">Derivative&nbsp;of&nbsp;the&nbsp;Leaky&nbsp;ReLU&nbsp;function:&nbsp;f'(z)&nbsp;=&nbsp;1&nbsp;if&nbsp;z&nbsp;&gt;&nbsp;0,&nbsp;else&nbsp;alpha<br>
Returns&nbsp;1&nbsp;for&nbsp;positive&nbsp;input,&nbsp;and&nbsp;alpha&nbsp;for&nbsp;negative&nbsp;input.</span></dd></dl>

<dl><dt><a name="Activation-relu"><strong>relu</strong></a>(z)</dt><dd><span class="code">ReLU&nbsp;(Rectified&nbsp;Linear&nbsp;Unit)&nbsp;activation&nbsp;function:&nbsp;f(z)&nbsp;=&nbsp;max(0,&nbsp;z)<br>
Returns&nbsp;the&nbsp;input&nbsp;directly&nbsp;if&nbsp;it's&nbsp;positive,&nbsp;otherwise&nbsp;returns&nbsp;0.</span></dd></dl>

<dl><dt><a name="Activation-relu_derivative"><strong>relu_derivative</strong></a>(z)</dt><dd><span class="code">Derivative&nbsp;of&nbsp;the&nbsp;ReLU&nbsp;function:&nbsp;f'(z)&nbsp;=&nbsp;1&nbsp;if&nbsp;z&nbsp;&gt;&nbsp;0,&nbsp;else&nbsp;0<br>
Returns&nbsp;1&nbsp;for&nbsp;positive&nbsp;input,&nbsp;and&nbsp;0&nbsp;for&nbsp;negative&nbsp;input.</span></dd></dl>

<dl><dt><a name="Activation-sigmoid"><strong>sigmoid</strong></a>(z)</dt><dd><span class="code">Sigmoid&nbsp;activation&nbsp;function:&nbsp;f(z)&nbsp;=&nbsp;1&nbsp;/&nbsp;(1&nbsp;+&nbsp;exp(-z))<br>
Maps&nbsp;input&nbsp;to&nbsp;the&nbsp;range&nbsp;[0,&nbsp;1],&nbsp;commonly&nbsp;used&nbsp;for&nbsp;binary&nbsp;classification.</span></dd></dl>

<dl><dt><a name="Activation-sigmoid_derivative"><strong>sigmoid_derivative</strong></a>(z)</dt><dd><span class="code">Derivative&nbsp;of&nbsp;the&nbsp;sigmoid&nbsp;function:&nbsp;f'(z)&nbsp;=&nbsp;<a href="#Activation-sigmoid">sigmoid</a>(z)&nbsp;*&nbsp;(1&nbsp;-&nbsp;<a href="#Activation-sigmoid">sigmoid</a>(z))<br>
Used&nbsp;for&nbsp;backpropagation&nbsp;through&nbsp;the&nbsp;sigmoid&nbsp;activation.</span></dd></dl>

<dl><dt><a name="Activation-softmax"><strong>softmax</strong></a>(z)</dt><dd><span class="code">Softmax&nbsp;activation&nbsp;function:&nbsp;f(z)_i&nbsp;=&nbsp;exp(z_i)&nbsp;/&nbsp;sum(exp(z_j))&nbsp;for&nbsp;all&nbsp;j<br>
Maps&nbsp;input&nbsp;into&nbsp;a&nbsp;probability&nbsp;distribution&nbsp;over&nbsp;multiple&nbsp;classes.&nbsp;Used&nbsp;for&nbsp;multiclass&nbsp;classification.</span></dd></dl>

<dl><dt><a name="Activation-tanh"><strong>tanh</strong></a>(z)</dt><dd><span class="code">Hyperbolic&nbsp;tangent&nbsp;(tanh)&nbsp;activation&nbsp;function:&nbsp;f(z)&nbsp;=&nbsp;(exp(z)&nbsp;-&nbsp;exp(-z))&nbsp;/&nbsp;(exp(z)&nbsp;+&nbsp;exp(-z))<br>
Maps&nbsp;input&nbsp;to&nbsp;the&nbsp;range&nbsp;[-1,&nbsp;1],&nbsp;typically&nbsp;used&nbsp;for&nbsp;normalized&nbsp;input.</span></dd></dl>

<dl><dt><a name="Activation-tanh_derivative"><strong>tanh_derivative</strong></a>(z)</dt><dd><span class="code">Derivative&nbsp;of&nbsp;the&nbsp;tanh&nbsp;function:&nbsp;f'(z)&nbsp;=&nbsp;1&nbsp;-&nbsp;<a href="#Activation-tanh">tanh</a>(z)^2<br>
Used&nbsp;for&nbsp;backpropagation&nbsp;through&nbsp;the&nbsp;tanh&nbsp;activation.</span></dd></dl>

<hr>
Data descriptors defined here:<br>
<dl><dt><strong>__dict__</strong></dt>
<dd><span class="code">dictionary&nbsp;for&nbsp;instance&nbsp;variables</span></dd>
</dl>
<dl><dt><strong>__weakref__</strong></dt>
<dd><span class="code">list&nbsp;of&nbsp;weak&nbsp;references&nbsp;to&nbsp;the&nbsp;object</span></dd>
</dl>
</td></tr></table> <p>
<table class="section">
<tr class="decor title-decor heading-text">
<td class="section-title" colspan=3>&nbsp;<br><a name="Layer">class <strong>Layer</strong></a>(<a href="builtins.html#object">builtins.object</a>)</td></tr>
    
<tr><td class="decor title-decor" rowspan=2><span class="code">&nbsp;&nbsp;&nbsp;</span></td>
<td class="decor title-decor" colspan=2><span class="code"><a href="#Layer">Layer</a>(input_size,&nbsp;output_size,&nbsp;activation='relu')<br>
&nbsp;<br>
Initializes&nbsp;a&nbsp;<a href="#Layer">Layer</a>&nbsp;<a href="builtins.html#object">object</a>.<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;input_size&nbsp;(int):&nbsp;The&nbsp;size&nbsp;of&nbsp;the&nbsp;input&nbsp;to&nbsp;the&nbsp;layer.<br>
&nbsp;&nbsp;&nbsp;&nbsp;output_size&nbsp;(int):&nbsp;The&nbsp;size&nbsp;of&nbsp;the&nbsp;output&nbsp;from&nbsp;the&nbsp;layer.<br>
&nbsp;&nbsp;&nbsp;&nbsp;activation&nbsp;(str):&nbsp;The&nbsp;activation&nbsp;function&nbsp;to&nbsp;be&nbsp;used&nbsp;in&nbsp;the&nbsp;layer.<br>&nbsp;</span></td></tr>
<tr><td>&nbsp;</td>
<td class="singlecolumn">Methods defined here:<br>
<dl><dt><a name="Layer-__init__"><strong>__init__</strong></a>(self, input_size, output_size, activation='relu')</dt><dd><span class="code">Initialize&nbsp;self.&nbsp;&nbsp;See&nbsp;help(type(self))&nbsp;for&nbsp;accurate&nbsp;signature.</span></dd></dl>

<dl><dt><a name="Layer-activate"><strong>activate</strong></a>(self, Z)</dt><dd><span class="code">Apply&nbsp;the&nbsp;activation&nbsp;function&nbsp;based&nbsp;on&nbsp;the&nbsp;layer's&nbsp;configuration.</span></dd></dl>

<dl><dt><a name="Layer-activation_derivative"><strong>activation_derivative</strong></a>(self, Z)</dt><dd><span class="code">Apply&nbsp;the&nbsp;derivative&nbsp;of&nbsp;the&nbsp;activation&nbsp;function&nbsp;for&nbsp;backpropagation.</span></dd></dl>

<dl><dt><a name="Layer-zero_grad"><strong>zero_grad</strong></a>(self)</dt><dd><span class="code">Reset&nbsp;the&nbsp;gradients&nbsp;of&nbsp;the&nbsp;weights&nbsp;and&nbsp;biases&nbsp;to&nbsp;zero.</span></dd></dl>

<hr>
Data descriptors defined here:<br>
<dl><dt><strong>__dict__</strong></dt>
<dd><span class="code">dictionary&nbsp;for&nbsp;instance&nbsp;variables</span></dd>
</dl>
<dl><dt><strong>__weakref__</strong></dt>
<dd><span class="code">list&nbsp;of&nbsp;weak&nbsp;references&nbsp;to&nbsp;the&nbsp;object</span></dd>
</dl>
</td></tr></table> <p>
<table class="section">
<tr class="decor title-decor heading-text">
<td class="section-title" colspan=3>&nbsp;<br><a name="NeuralNetwork">class <strong>NeuralNetwork</strong></a>(<a href="builtins.html#object">builtins.object</a>)</td></tr>
    
<tr><td class="decor title-decor" rowspan=2><span class="code">&nbsp;&nbsp;&nbsp;</span></td>
<td class="decor title-decor" colspan=2><span class="code"><a href="#NeuralNetwork">NeuralNetwork</a>(layer_sizes,&nbsp;dropout_rate=0.2,&nbsp;reg_lambda=0.01,&nbsp;activations=None)<br>
&nbsp;<br>
Neural&nbsp;network&nbsp;class&nbsp;for&nbsp;training&nbsp;and&nbsp;evaluating&nbsp;a&nbsp;custom&nbsp;neural&nbsp;network&nbsp;model.<br>
Parameters:<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;layer_sizes&nbsp;(list):&nbsp;A&nbsp;list&nbsp;of&nbsp;integers&nbsp;representing&nbsp;the&nbsp;sizes&nbsp;of&nbsp;each&nbsp;layer&nbsp;in&nbsp;the&nbsp;neural&nbsp;network.<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;dropout_rate&nbsp;(float):&nbsp;The&nbsp;dropout&nbsp;rate&nbsp;to&nbsp;be&nbsp;applied&nbsp;during&nbsp;training.&nbsp;Default&nbsp;is&nbsp;0.2.<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;reg_lambda&nbsp;(float):&nbsp;The&nbsp;regularization&nbsp;lambda&nbsp;value.&nbsp;Default&nbsp;is&nbsp;0.01.<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;activations&nbsp;(list):&nbsp;A&nbsp;list&nbsp;of&nbsp;activation&nbsp;functions&nbsp;for&nbsp;each&nbsp;layer.&nbsp;Default&nbsp;is&nbsp;['relu',&nbsp;'relu',&nbsp;...&nbsp;'softmax'].<br>&nbsp;</span></td></tr>
<tr><td>&nbsp;</td>
<td class="singlecolumn">Methods defined here:<br>
<dl><dt><a name="NeuralNetwork-__init__"><strong>__init__</strong></a>(self, layer_sizes, dropout_rate=0.2, reg_lambda=0.01, activations=None)</dt><dd><span class="code">Initialize&nbsp;self.&nbsp;&nbsp;See&nbsp;help(type(self))&nbsp;for&nbsp;accurate&nbsp;signature.</span></dd></dl>

<dl><dt><a name="NeuralNetwork-__repr__"><strong>__repr__</strong></a>(self)</dt><dd><span class="code">Return&nbsp;repr(self).</span></dd></dl>

<dl><dt><a name="NeuralNetwork-__str__"><strong>__str__</strong></a>(self)</dt><dd><span class="code">Return&nbsp;str(self).</span></dd></dl>

<dl><dt><a name="NeuralNetwork-apply_dropout"><strong>apply_dropout</strong></a>(self, A)</dt><dd><span class="code">Applies&nbsp;dropout&nbsp;regularization&nbsp;to&nbsp;the&nbsp;input&nbsp;array.<br>
Parameters:&nbsp;A:&nbsp;numpy.ndarray:&nbsp;Input&nbsp;array&nbsp;to&nbsp;apply&nbsp;dropout&nbsp;regularization&nbsp;to.<br>
Returns:&nbsp;numpy.ndarray:&nbsp;Array&nbsp;with&nbsp;dropout&nbsp;regularization&nbsp;applied.</span></dd></dl>

<dl><dt><a name="NeuralNetwork-backward"><strong>backward</strong></a>(self, y)</dt><dd><span class="code">Performs&nbsp;backward&nbsp;propagation&nbsp;to&nbsp;calculate&nbsp;the&nbsp;gradients&nbsp;of&nbsp;the&nbsp;weights&nbsp;and&nbsp;biases&nbsp;in&nbsp;the&nbsp;neural&nbsp;network.<br>
Parameters:&nbsp;y&nbsp;(numpy.ndarray):&nbsp;Target&nbsp;labels&nbsp;of&nbsp;shape&nbsp;(m,&nbsp;1),&nbsp;where&nbsp;m&nbsp;is&nbsp;the&nbsp;number&nbsp;of&nbsp;samples.<br>
Returns:&nbsp;None</span></dd></dl>

<dl><dt><a name="NeuralNetwork-calculate_loss"><strong>calculate_loss</strong></a>(self, X, y, class_weights=None)</dt><dd><span class="code">Calculates&nbsp;the&nbsp;loss&nbsp;of&nbsp;the&nbsp;neural&nbsp;network&nbsp;model.<br>
Formula:&nbsp;loss&nbsp;=&nbsp;loss_fn(outputs,&nbsp;y)&nbsp;+&nbsp;reg_lambda&nbsp;*&nbsp;sum([sum(layer.weights**2)&nbsp;for&nbsp;layer&nbsp;in&nbsp;self.<strong>layers</strong>])<br>
Parameters:<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;X&nbsp;(numpy.ndarray):&nbsp;Input&nbsp;data&nbsp;of&nbsp;shape&nbsp;(num_samples,&nbsp;num_features).<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;y&nbsp;(numpy.ndarray):&nbsp;Target&nbsp;labels&nbsp;of&nbsp;shape&nbsp;(num_samples,).<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;class_weights&nbsp;(numpy.ndarray,&nbsp;optional):&nbsp;Weights&nbsp;for&nbsp;each&nbsp;class.&nbsp;Default&nbsp;is&nbsp;None.<br>
Returns:&nbsp;loss&nbsp;(float):&nbsp;The&nbsp;calculated&nbsp;loss&nbsp;value.</span></dd></dl>

<dl><dt><a name="NeuralNetwork-create_optimizer"><strong>create_optimizer</strong></a>(self, optimizer_type, learning_rate)</dt><dd><span class="code">Creates&nbsp;an&nbsp;optimizer&nbsp;instance&nbsp;based&nbsp;on&nbsp;the&nbsp;specified&nbsp;type&nbsp;and&nbsp;learning&nbsp;rate.<br>
&nbsp;<br>
Parameters:<br>
&nbsp;&nbsp;&nbsp;&nbsp;optimizer_type&nbsp;(str):&nbsp;The&nbsp;type&nbsp;of&nbsp;optimizer&nbsp;(e.g.,&nbsp;'SGD',&nbsp;'Adam').<br>
&nbsp;&nbsp;&nbsp;&nbsp;learning_rate&nbsp;(float):&nbsp;The&nbsp;learning&nbsp;rate&nbsp;for&nbsp;the&nbsp;optimizer.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;optimizer:&nbsp;An&nbsp;instance&nbsp;of&nbsp;the&nbsp;specified&nbsp;optimizer.</span></dd></dl>

<dl><dt><a name="NeuralNetwork-evaluate"><strong>evaluate</strong></a>(self, X, y)</dt><dd><span class="code">Evaluates&nbsp;the&nbsp;performance&nbsp;of&nbsp;the&nbsp;neural&nbsp;network&nbsp;model&nbsp;on&nbsp;the&nbsp;given&nbsp;input&nbsp;data.<br>
Parameters:<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;X&nbsp;(numpy.ndarray):&nbsp;The&nbsp;input&nbsp;data&nbsp;for&nbsp;evaluation.<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;y&nbsp;(numpy.ndarray):&nbsp;The&nbsp;target&nbsp;labels&nbsp;for&nbsp;evaluation.<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;accuracy&nbsp;(float):&nbsp;The&nbsp;accuracy&nbsp;of&nbsp;the&nbsp;model's&nbsp;predictions.<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;predicted&nbsp;(numpy.ndarray):&nbsp;The&nbsp;labels&nbsp;predicted&nbsp;by&nbsp;the&nbsp;model.</span></dd></dl>

<dl><dt><a name="NeuralNetwork-forward"><strong>forward</strong></a>(self, X)</dt><dd><span class="code">Performs&nbsp;forward&nbsp;propagation&nbsp;through&nbsp;the&nbsp;neural&nbsp;network.<br>
Args:&nbsp;X&nbsp;(ndarray):&nbsp;Input&nbsp;data&nbsp;of&nbsp;shape&nbsp;(batch_size,&nbsp;input_size).<br>
Returns:&nbsp;ndarray:&nbsp;Output&nbsp;predictions&nbsp;of&nbsp;shape&nbsp;(batch_size,&nbsp;output_size).</span></dd></dl>

<dl><dt><a name="NeuralNetwork-train"><strong>train</strong></a>(self, X_train, y_train, X_test=None, y_test=None, optimizer=&lt;sega_learn.neural_networks.optimizers.AdamOptimizer object at 0x000001C885C91450&gt;, epochs=100, batch_size=32, early_stopping_threshold=10, lr_scheduler=None, p=True)</dt><dd><span class="code">Trains&nbsp;the&nbsp;neural&nbsp;network&nbsp;model.<br>
Parameters:<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;X_train&nbsp;(numpy.ndarray):&nbsp;Training&nbsp;data&nbsp;features.<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;y_train&nbsp;(numpy.ndarray):&nbsp;Training&nbsp;data&nbsp;labels.<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;X_test&nbsp;(numpy.ndarray):&nbsp;Test&nbsp;data&nbsp;features,&nbsp;optional&nbsp;(default:&nbsp;None).<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;y_test&nbsp;(numpy.ndarray):&nbsp;Test&nbsp;data&nbsp;labels,&nbsp;optional&nbsp;(default:&nbsp;None).<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;optimizer&nbsp;(Optimizer):&nbsp;The&nbsp;optimizer&nbsp;used&nbsp;for&nbsp;updating&nbsp;the&nbsp;model&nbsp;parameters&nbsp;(default:&nbsp;Adam,&nbsp;lr=0.0001).<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;epochs&nbsp;(int):&nbsp;Number&nbsp;of&nbsp;training&nbsp;epochs&nbsp;(default:&nbsp;100).<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;batch_size&nbsp;(int):&nbsp;Batch&nbsp;size&nbsp;for&nbsp;mini-batch&nbsp;gradient&nbsp;descent&nbsp;(default:&nbsp;32).<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;early_stopping_threshold&nbsp;(int):&nbsp;Number&nbsp;of&nbsp;epochs&nbsp;to&nbsp;wait&nbsp;for&nbsp;improvement&nbsp;in&nbsp;training&nbsp;loss&nbsp;before&nbsp;early&nbsp;stopping&nbsp;(default:&nbsp;5).<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;p&nbsp;(bool):&nbsp;Whether&nbsp;to&nbsp;print&nbsp;training&nbsp;progress&nbsp;(default:&nbsp;True).<br>
Returns:&nbsp;None</span></dd></dl>

<dl><dt><a name="NeuralNetwork-tune_hyperparameters"><strong>tune_hyperparameters</strong></a>(self, param_grid, num_layers_range, layer_size_range, output_size, X_train, y_train, X_val, y_val, optimizer_type, lr_range, epochs=100, batch_size=32)</dt><dd><span class="code">Performs&nbsp;hyperparameter&nbsp;tuning&nbsp;using&nbsp;grid&nbsp;search.<br>
&nbsp;<br>
Parameters:<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;param_grid&nbsp;(dict):&nbsp;A&nbsp;dictionary&nbsp;where&nbsp;keys&nbsp;are&nbsp;parameter&nbsp;names&nbsp;and&nbsp;values&nbsp;are&nbsp;lists&nbsp;of&nbsp;values&nbsp;to&nbsp;try.<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;num_layers_range&nbsp;(tuple):&nbsp;A&nbsp;tuple&nbsp;(min_layers,&nbsp;max_layers,&nbsp;step)&nbsp;for&nbsp;the&nbsp;number&nbsp;of&nbsp;layers.<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;layer_size_range&nbsp;(tuple):&nbsp;A&nbsp;tuple&nbsp;(min_size,&nbsp;max_size,&nbsp;step)&nbsp;for&nbsp;the&nbsp;layer&nbsp;sizes.<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;output_size&nbsp;(int):&nbsp;The&nbsp;size&nbsp;of&nbsp;the&nbsp;output&nbsp;layer.<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;X_train&nbsp;(numpy.ndarray):&nbsp;Training&nbsp;data&nbsp;features.<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;y_train&nbsp;(numpy.ndarray):&nbsp;Training&nbsp;data&nbsp;labels.<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;X_val&nbsp;(numpy.ndarray):&nbsp;Validation&nbsp;data&nbsp;features.<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;y_val&nbsp;(numpy.ndarray):&nbsp;Validation&nbsp;data&nbsp;labels.<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;optimizer_type&nbsp;(str):&nbsp;The&nbsp;type&nbsp;of&nbsp;optimizer&nbsp;(e.g.,&nbsp;'SGD',&nbsp;'Adam').<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;lr_range&nbsp;(tuple):&nbsp;A&nbsp;tuple&nbsp;(min_lr,&nbsp;max_lr,&nbsp;num_steps)&nbsp;for&nbsp;learning&nbsp;rates.<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;epochs&nbsp;(int):&nbsp;Number&nbsp;of&nbsp;training&nbsp;epochs&nbsp;(default:&nbsp;100).<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;batch_size&nbsp;(int):&nbsp;Batch&nbsp;size&nbsp;for&nbsp;mini-batch&nbsp;gradient&nbsp;descent&nbsp;(default:&nbsp;32).<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;best_params&nbsp;(dict):&nbsp;The&nbsp;best&nbsp;hyperparameters&nbsp;found&nbsp;during&nbsp;tuning.<br>
&nbsp;&nbsp;&nbsp;&nbsp;best_accuracy&nbsp;(float):&nbsp;The&nbsp;best&nbsp;validation&nbsp;accuracy&nbsp;achieved.</span></dd></dl>

<hr>
Data descriptors defined here:<br>
<dl><dt><strong>__dict__</strong></dt>
<dd><span class="code">dictionary&nbsp;for&nbsp;instance&nbsp;variables</span></dd>
</dl>
<dl><dt><strong>__weakref__</strong></dt>
<dd><span class="code">list&nbsp;of&nbsp;weak&nbsp;references&nbsp;to&nbsp;the&nbsp;object</span></dd>
</dl>
</td></tr></table></td></tr></table><p>
<table class="section">
<tr class="decor functions-decor heading-text">
<td class="section-title" colspan=3>&nbsp;<br><strong class="bigsection">Functions</strong></td></tr>
    
<tr><td class="decor functions-decor"><span class="code">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span></td><td>&nbsp;</td>
<td class="singlecolumn"><dl><dt><a name="-test_breast_cancer"><strong>test_breast_cancer</strong></a>()</dt><dd><span class="code">Test&nbsp;the&nbsp;neural&nbsp;network&nbsp;model&nbsp;on&nbsp;the&nbsp;Breast&nbsp;Cancer&nbsp;dataset.</span></dd></dl>
 <dl><dt><a name="-test_iris"><strong>test_iris</strong></a>()</dt><dd><span class="code">Test&nbsp;the&nbsp;neural&nbsp;network&nbsp;model&nbsp;on&nbsp;the&nbsp;Iris&nbsp;dataset.</span></dd></dl>
 <dl><dt><a name="-test_model"><strong>test_model</strong></a>(load_data_func, nn_layers, dropout_rate, reg_lambda, test_size=0.2)</dt><dd><span class="code">Generic&nbsp;function&nbsp;to&nbsp;test&nbsp;the&nbsp;neural&nbsp;network&nbsp;model&nbsp;on&nbsp;a&nbsp;given&nbsp;dataset.</span></dd></dl>
</td></tr></table>
</body></html>
