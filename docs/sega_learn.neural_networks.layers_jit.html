<!DOCTYPE html>
<html lang="en">
<head>
<style>
body { background-color: #f0f0f8; }
table.heading tr { background-color: #7799ee; }
.decor { color: #ffffff; }
.title-decor { background-color: #ffc8d8; color: #000000; }
.pkg-content-decor { background-color: #aa55cc; }
.index-decor { background-color: #ee77aa; }
.functions-decor { background-color: #eeaa77; }
.data-decor { background-color: #55aa55; }
.author-decor { background-color: #7799ee; }
.credits-decor { background-color: #7799ee; }
.error-decor { background-color: #bb0000; }
.grey { color: #909090; }
.white { color: #ffffff; }
.repr { color: #c040c0; }
table.heading tr td.title, table.heading tr td.extra { vertical-align: bottom; }
table.heading tr td.extra { text-align: right; }
.heading-text { font-family: helvetica, arial; }
.bigsection { font-size: larger; }
.title { font-size: x-large; }
.code { font-family: monospace; }
table { width: 100%; border-spacing: 0; border-collapse: collapse; border: 0; }
td { padding: 2; }
td.section-title, td.multicolumn { vertical-align: bottom; }
td.multicolumn { width: 25%; }
td.singlecolumn { width: 100%; }
</style>
<meta charset="utf-8">
<title>Python: module sega_learn.neural_networks.layers_jit</title>
</head><body>

<table class="heading">
<tr class="heading-text decor">
<td class="title">&nbsp;<br><strong class="title"><a href="sega_learn.html" class="white">sega_learn</a>.<a href="sega_learn.neural_networks.html" class="white">neural_networks</a>.layers_jit</strong></td>
</tr></table>
    <p></p>
<p>
<table class="section">
<tr class="decor pkg-content-decor heading-text">
<td class="section-title" colspan=3>&nbsp;<br><strong class="bigsection">Modules</strong></td></tr>

<tr><td class="decor pkg-content-decor"><span class="code">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span></td><td>&nbsp;</td>
<td class="singlecolumn"><table><tr><td class="multicolumn"><a href="numpy.html">numpy</a><br>
</td><td class="multicolumn"><a href="numba.core.types.html">numba.core.types</a><br>
</td><td class="multicolumn"></td><td class="multicolumn"></td></tr></table></td></tr></table><p>
<table class="section">
<tr class="decor index-decor heading-text">
<td class="section-title" colspan=3>&nbsp;<br><strong class="bigsection">Classes</strong></td></tr>

<tr><td class="decor index-decor"><span class="code">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span></td><td>&nbsp;</td>
<td class="singlecolumn"><dl>
<dt class="heading-text"><a href="builtins.html#object">builtins.object</a>
</dt><dd>
<dl>
<dt class="heading-text"><a href="sega_learn.neural_networks.layers_jit.html#JITRNNLayer">JITRNNLayer</a>
</dt></dl>
</dd>
<dt class="heading-text">JITConvLayer(<a href="builtins.html#object">builtins.object</a>)
</dt><dd>
<dl>
<dt class="heading-text"><a href="sega_learn.neural_networks.layers_jit.html#JITConvLayer">JITConvLayer</a>
</dt></dl>
</dd>
<dt class="heading-text">JITDenseLayer(<a href="builtins.html#object">builtins.object</a>)
</dt><dd>
<dl>
<dt class="heading-text"><a href="sega_learn.neural_networks.layers_jit.html#JITDenseLayer">JITDenseLayer</a>
</dt></dl>
</dd>
<dt class="heading-text">JITFlattenLayer(<a href="builtins.html#object">builtins.object</a>)
</dt><dd>
<dl>
<dt class="heading-text"><a href="sega_learn.neural_networks.layers_jit.html#JITFlattenLayer">JITFlattenLayer</a>
</dt></dl>
</dd>
</dl>
 <p>
<table class="section">
<tr class="decor title-decor heading-text">
<td class="section-title" colspan=3>&nbsp;<br><a name="JITConvLayer">class <strong>JITConvLayer</strong></a>(JITConvLayer)</td></tr>

<tr><td class="decor title-decor" rowspan=2><span class="code">&nbsp;&nbsp;&nbsp;</span></td>
<td class="decor title-decor" colspan=2><span class="code"><a href="#JITConvLayer">JITConvLayer</a>(*args,&nbsp;**kwargs)<br>
&nbsp;<br>
A&nbsp;convolutional&nbsp;layer&nbsp;implementation&nbsp;for&nbsp;neural&nbsp;networks&nbsp;using&nbsp;Numba&nbsp;JIT&nbsp;compilation.<br>&nbsp;</span></td></tr>
<tr><td>&nbsp;</td>
<td class="singlecolumn"><dl><dt>Method resolution order:</dt>
<dd><a href="sega_learn.neural_networks.layers_jit.html#JITConvLayer">JITConvLayer</a></dd>
<dd>JITConvLayer</dd>
<dd><a href="builtins.html#object">builtins.object</a></dd>
</dl>
<hr>
Data and other attributes defined here:<br>
<dl><dt><strong>class_type</strong> = jitclass.JITConvLayer#16fc5477310&lt;in_channels:in...2,w_out:int32,input_size:int32,output_size:int32&gt;</dl>

<hr>
Methods inherited from JITConvLayer:<br>
<dl><dt><a name="JITConvLayer-__init__"><strong>__init__</strong></a>(self, in_channels, out_channels, kernel_size, stride=1, padding=0, activation='relu')</dt><dd><span class="code">Initializes&nbsp;the&nbsp;convolutional&nbsp;layer&nbsp;with&nbsp;weights,&nbsp;biases,&nbsp;and&nbsp;activation&nbsp;function.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;in_channels:&nbsp;(int)&nbsp;-&nbsp;Number&nbsp;of&nbsp;input&nbsp;channels.<br>
&nbsp;&nbsp;&nbsp;&nbsp;out_channels:&nbsp;(int)&nbsp;-&nbsp;Number&nbsp;of&nbsp;output&nbsp;channels.<br>
&nbsp;&nbsp;&nbsp;&nbsp;kernel_size:&nbsp;(int)&nbsp;-&nbsp;Size&nbsp;of&nbsp;the&nbsp;convolutional&nbsp;kernel&nbsp;(assumes&nbsp;square&nbsp;kernels).<br>
&nbsp;&nbsp;&nbsp;&nbsp;stride:&nbsp;(int),&nbsp;optional&nbsp;-&nbsp;Stride&nbsp;of&nbsp;the&nbsp;convolution&nbsp;(default&nbsp;is&nbsp;1).<br>
&nbsp;&nbsp;&nbsp;&nbsp;padding:&nbsp;(int),&nbsp;optional&nbsp;-&nbsp;Padding&nbsp;added&nbsp;to&nbsp;the&nbsp;input&nbsp;(default&nbsp;is&nbsp;0).<br>
&nbsp;&nbsp;&nbsp;&nbsp;activation:&nbsp;(str),&nbsp;optional&nbsp;-&nbsp;Activation&nbsp;function&nbsp;to&nbsp;use&nbsp;(default&nbsp;is&nbsp;"relu").<br>
&nbsp;<br>
Attributes:<br>
&nbsp;&nbsp;&nbsp;&nbsp;weights:&nbsp;(np.ndarray)&nbsp;-&nbsp;Convolutional&nbsp;weight&nbsp;matrix&nbsp;initialized&nbsp;using&nbsp;He&nbsp;initialization.<br>
&nbsp;&nbsp;&nbsp;&nbsp;biases:&nbsp;(np.ndarray)&nbsp;-&nbsp;Bias&nbsp;vector&nbsp;initialized&nbsp;to&nbsp;zeros.<br>
&nbsp;&nbsp;&nbsp;&nbsp;activation:&nbsp;(str)&nbsp;-&nbsp;Activation&nbsp;function&nbsp;for&nbsp;the&nbsp;layer.<br>
&nbsp;&nbsp;&nbsp;&nbsp;weight_gradients:&nbsp;(np.ndarray)&nbsp;-&nbsp;Gradients&nbsp;of&nbsp;the&nbsp;weights,&nbsp;initialized&nbsp;to&nbsp;zeros.<br>
&nbsp;&nbsp;&nbsp;&nbsp;bias_gradients:&nbsp;(np.ndarray)&nbsp;-&nbsp;Gradients&nbsp;of&nbsp;the&nbsp;biases,&nbsp;initialized&nbsp;to&nbsp;zeros.<br>
&nbsp;&nbsp;&nbsp;&nbsp;input_cache:&nbsp;(np.ndarray)&nbsp;-&nbsp;Cached&nbsp;input&nbsp;values&nbsp;for&nbsp;backpropagation,&nbsp;initialized&nbsp;to&nbsp;zeros.<br>
&nbsp;&nbsp;&nbsp;&nbsp;X_cols:&nbsp;(np.ndarray)&nbsp;-&nbsp;Cached&nbsp;column-transformed&nbsp;input&nbsp;for&nbsp;backpropagation,&nbsp;initialized&nbsp;to&nbsp;zeros.<br>
&nbsp;&nbsp;&nbsp;&nbsp;X_padded:&nbsp;(np.ndarray)&nbsp;-&nbsp;Cached&nbsp;padded&nbsp;input&nbsp;for&nbsp;backpropagation,&nbsp;initialized&nbsp;to&nbsp;zeros.<br>
&nbsp;&nbsp;&nbsp;&nbsp;h_out:&nbsp;(int)&nbsp;-&nbsp;Height&nbsp;of&nbsp;the&nbsp;output&nbsp;feature&nbsp;map,&nbsp;initialized&nbsp;to&nbsp;0.<br>
&nbsp;&nbsp;&nbsp;&nbsp;w_out:&nbsp;(int)&nbsp;-&nbsp;Width&nbsp;of&nbsp;the&nbsp;output&nbsp;feature&nbsp;map,&nbsp;initialized&nbsp;to&nbsp;0.<br>
&nbsp;&nbsp;&nbsp;&nbsp;input_size:&nbsp;(int)&nbsp;-&nbsp;Number&nbsp;of&nbsp;input&nbsp;channels.<br>
&nbsp;&nbsp;&nbsp;&nbsp;output_size:&nbsp;(int)&nbsp;-&nbsp;Number&nbsp;of&nbsp;output&nbsp;channels.</span></dd></dl>

<dl><dt><a name="JITConvLayer-activate"><strong>activate</strong></a>(self, Z)</dt><dd><span class="code">Apply&nbsp;activation&nbsp;function.</span></dd></dl>

<dl><dt><a name="JITConvLayer-activation_derivative"><strong>activation_derivative</strong></a>(self, Z)</dt><dd><span class="code">Apply&nbsp;activation&nbsp;derivative.</span></dd></dl>

<dl><dt><a name="JITConvLayer-backward"><strong>backward</strong></a>(self, d_out, reg_lambda=0)</dt><dd><span class="code">Backward&nbsp;pass&nbsp;for&nbsp;convolutional&nbsp;layer.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;d_out&nbsp;(np.ndarray):&nbsp;Gradient&nbsp;of&nbsp;the&nbsp;loss&nbsp;with&nbsp;respect&nbsp;to&nbsp;the&nbsp;layer&nbsp;output<br>
&nbsp;&nbsp;&nbsp;&nbsp;reg_lambda&nbsp;(float,&nbsp;optional):&nbsp;Regularization&nbsp;parameter<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;dX:&nbsp;Gradient&nbsp;with&nbsp;respect&nbsp;to&nbsp;the&nbsp;input&nbsp;X</span></dd></dl>

<dl><dt><a name="JITConvLayer-forward"><strong>forward</strong></a>(self, X)</dt><dd><span class="code">Forward&nbsp;pass&nbsp;for&nbsp;convolutional&nbsp;layer.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;X:&nbsp;numpy&nbsp;array&nbsp;with&nbsp;shape&nbsp;(batch_size,&nbsp;in_channels,&nbsp;height,&nbsp;width)<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;Output&nbsp;feature&nbsp;maps&nbsp;after&nbsp;convolution&nbsp;and&nbsp;activation.</span></dd></dl>

<dl><dt><a name="JITConvLayer-zero_grad"><strong>zero_grad</strong></a>(self)</dt><dd><span class="code">Reset&nbsp;the&nbsp;gradients&nbsp;of&nbsp;the&nbsp;weights&nbsp;and&nbsp;biases&nbsp;to&nbsp;zero.</span></dd></dl>

<hr>
Data descriptors inherited from JITConvLayer:<br>
<dl><dt><strong>__dict__</strong></dt>
<dd><span class="code">dictionary&nbsp;for&nbsp;instance&nbsp;variables</span></dd>
</dl>
<dl><dt><strong>__weakref__</strong></dt>
<dd><span class="code">list&nbsp;of&nbsp;weak&nbsp;references&nbsp;to&nbsp;the&nbsp;object</span></dd>
</dl>
</td></tr></table> <p>
<table class="section">
<tr class="decor title-decor heading-text">
<td class="section-title" colspan=3>&nbsp;<br><a name="JITDenseLayer">class <strong>JITDenseLayer</strong></a>(JITDenseLayer)</td></tr>

<tr><td class="decor title-decor" rowspan=2><span class="code">&nbsp;&nbsp;&nbsp;</span></td>
<td class="decor title-decor" colspan=2><span class="code"><a href="#JITDenseLayer">JITDenseLayer</a>(*args,&nbsp;**kwargs)<br>
&nbsp;<br>
Initializes&nbsp;a&nbsp;fully&nbsp;connected&nbsp;layer&nbsp;<a href="builtins.html#object">object</a>,&nbsp;where&nbsp;each&nbsp;neuron&nbsp;is&nbsp;connected&nbsp;to&nbsp;all&nbsp;neurons&nbsp;in&nbsp;the&nbsp;previous&nbsp;layer.<br>
&nbsp;<br>
Each&nbsp;layer&nbsp;consists&nbsp;of&nbsp;weights,&nbsp;biases,&nbsp;and&nbsp;an&nbsp;activation&nbsp;function.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;input_size&nbsp;(int):&nbsp;The&nbsp;size&nbsp;of&nbsp;the&nbsp;input&nbsp;to&nbsp;the&nbsp;layer.<br>
&nbsp;&nbsp;&nbsp;&nbsp;output_size&nbsp;(int):&nbsp;The&nbsp;size&nbsp;of&nbsp;the&nbsp;output&nbsp;from&nbsp;the&nbsp;layer.<br>
&nbsp;&nbsp;&nbsp;&nbsp;activation&nbsp;(str):&nbsp;The&nbsp;activation&nbsp;function&nbsp;to&nbsp;be&nbsp;used&nbsp;in&nbsp;the&nbsp;layer.<br>
&nbsp;<br>
Attributes:<br>
&nbsp;&nbsp;&nbsp;&nbsp;weights&nbsp;(np.ndarray):&nbsp;Weights&nbsp;of&nbsp;the&nbsp;layer.<br>
&nbsp;&nbsp;&nbsp;&nbsp;biases&nbsp;(np.ndarray):&nbsp;Biases&nbsp;of&nbsp;the&nbsp;layer.<br>
&nbsp;&nbsp;&nbsp;&nbsp;activation&nbsp;(str):&nbsp;Activation&nbsp;function&nbsp;name.<br>
&nbsp;&nbsp;&nbsp;&nbsp;weight_gradients&nbsp;(np.ndarray):&nbsp;Gradients&nbsp;of&nbsp;the&nbsp;weights.<br>
&nbsp;&nbsp;&nbsp;&nbsp;bias_gradients&nbsp;(np.ndarray):&nbsp;Gradients&nbsp;of&nbsp;the&nbsp;biases.<br>
&nbsp;&nbsp;&nbsp;&nbsp;input_cache&nbsp;(np.ndarray):&nbsp;Cached&nbsp;input&nbsp;for&nbsp;backpropagation.<br>
&nbsp;&nbsp;&nbsp;&nbsp;output_cache&nbsp;(np.ndarray):&nbsp;Cached&nbsp;output&nbsp;for&nbsp;backpropagation.<br>
&nbsp;<br>
Methods:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#JITDenseLayer-zero_grad">zero_grad</a>():&nbsp;Resets&nbsp;the&nbsp;gradients&nbsp;of&nbsp;the&nbsp;weights&nbsp;and&nbsp;biases&nbsp;to&nbsp;zero.<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#JITDenseLayer-forward">forward</a>(X):&nbsp;Performs&nbsp;the&nbsp;forward&nbsp;pass&nbsp;of&nbsp;the&nbsp;layer.<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#JITDenseLayer-backward">backward</a>(dA,&nbsp;reg_lambda):&nbsp;Performs&nbsp;the&nbsp;backward&nbsp;pass&nbsp;of&nbsp;the&nbsp;layer.<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#JITDenseLayer-activate">activate</a>(Z):&nbsp;Applies&nbsp;the&nbsp;activation&nbsp;function.<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#JITDenseLayer-activation_derivative">activation_derivative</a>(Z):&nbsp;Applies&nbsp;the&nbsp;derivative&nbsp;of&nbsp;the&nbsp;activation&nbsp;function.<br>&nbsp;</span></td></tr>
<tr><td>&nbsp;</td>
<td class="singlecolumn"><dl><dt>Method resolution order:</dt>
<dd><a href="sega_learn.neural_networks.layers_jit.html#JITDenseLayer">JITDenseLayer</a></dd>
<dd>JITDenseLayer</dd>
<dd><a href="builtins.html#object">builtins.object</a></dd>
</dl>
<hr>
Data and other attributes defined here:<br>
<dl><dt><strong>class_type</strong> = jitclass.JITDenseLayer#16fc544ee50&lt;weights:array...oat64, 2d, C),input_size:int32,output_size:int32&gt;</dl>

<hr>
Methods inherited from JITDenseLayer:<br>
<dl><dt><a name="JITDenseLayer-__init__"><strong>__init__</strong></a>(self, input_size, output_size, activation='relu')</dt><dd><span class="code">Initializes&nbsp;the&nbsp;layer&nbsp;with&nbsp;weights,&nbsp;biases,&nbsp;and&nbsp;activation&nbsp;function.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;input_size:&nbsp;(int)&nbsp;-&nbsp;The&nbsp;number&nbsp;of&nbsp;input&nbsp;features&nbsp;to&nbsp;the&nbsp;layer.<br>
&nbsp;&nbsp;&nbsp;&nbsp;output_size:&nbsp;(int)&nbsp;-&nbsp;The&nbsp;number&nbsp;of&nbsp;output&nbsp;features&nbsp;from&nbsp;the&nbsp;layer.<br>
&nbsp;&nbsp;&nbsp;&nbsp;activation:&nbsp;(str),&nbsp;optional&nbsp;-&nbsp;The&nbsp;activation&nbsp;function&nbsp;to&nbsp;use&nbsp;(default&nbsp;is&nbsp;"relu").<br>
&nbsp;<br>
Attributes:<br>
&nbsp;&nbsp;&nbsp;&nbsp;weights:&nbsp;(np.ndarray)&nbsp;-&nbsp;The&nbsp;weight&nbsp;matrix&nbsp;initialized&nbsp;using&nbsp;He&nbsp;initialization&nbsp;for&nbsp;ReLU&nbsp;or&nbsp;Leaky&nbsp;ReLU,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;or&nbsp;Xavier&nbsp;initialization&nbsp;for&nbsp;other&nbsp;activations.<br>
&nbsp;&nbsp;&nbsp;&nbsp;biases:&nbsp;(np.ndarray)&nbsp;-&nbsp;The&nbsp;bias&nbsp;vector&nbsp;initialized&nbsp;to&nbsp;zeros.<br>
&nbsp;&nbsp;&nbsp;&nbsp;activation:&nbsp;(str)&nbsp;-&nbsp;The&nbsp;activation&nbsp;function&nbsp;for&nbsp;the&nbsp;layer.<br>
&nbsp;&nbsp;&nbsp;&nbsp;weight_gradients:&nbsp;(np.ndarray)&nbsp;-&nbsp;Gradients&nbsp;of&nbsp;the&nbsp;weights,&nbsp;initialized&nbsp;to&nbsp;zeros.<br>
&nbsp;&nbsp;&nbsp;&nbsp;bias_gradients:&nbsp;(np.ndarray)&nbsp;-&nbsp;Gradients&nbsp;of&nbsp;the&nbsp;biases,&nbsp;initialized&nbsp;to&nbsp;zeros.<br>
&nbsp;&nbsp;&nbsp;&nbsp;input_cache:&nbsp;(np.ndarray)&nbsp;-&nbsp;Cached&nbsp;input&nbsp;values&nbsp;for&nbsp;backpropagation,&nbsp;initialized&nbsp;to&nbsp;zeros.<br>
&nbsp;&nbsp;&nbsp;&nbsp;output_cache:&nbsp;(np.ndarray)&nbsp;-&nbsp;Cached&nbsp;output&nbsp;values&nbsp;for&nbsp;backpropagation,&nbsp;initialized&nbsp;to&nbsp;zeros.<br>
&nbsp;&nbsp;&nbsp;&nbsp;input_size:&nbsp;(int)&nbsp;-&nbsp;The&nbsp;number&nbsp;of&nbsp;input&nbsp;features&nbsp;to&nbsp;the&nbsp;layer.<br>
&nbsp;&nbsp;&nbsp;&nbsp;output_size:&nbsp;(int)&nbsp;-&nbsp;The&nbsp;number&nbsp;of&nbsp;output&nbsp;features&nbsp;from&nbsp;the&nbsp;layer.</span></dd></dl>

<dl><dt><a name="JITDenseLayer-activate"><strong>activate</strong></a>(self, Z)</dt><dd><span class="code">Apply&nbsp;activation&nbsp;function.</span></dd></dl>

<dl><dt><a name="JITDenseLayer-activation_derivative"><strong>activation_derivative</strong></a>(self, Z)</dt><dd><span class="code">Apply&nbsp;activation&nbsp;derivative.</span></dd></dl>

<dl><dt><a name="JITDenseLayer-backward"><strong>backward</strong></a>(self, dA, reg_lambda)</dt><dd><span class="code">Perform&nbsp;the&nbsp;backward&nbsp;pass&nbsp;of&nbsp;the&nbsp;layer.</span></dd></dl>

<dl><dt><a name="JITDenseLayer-forward"><strong>forward</strong></a>(self, X)</dt><dd><span class="code">Perform&nbsp;the&nbsp;forward&nbsp;pass&nbsp;of&nbsp;the&nbsp;layer.</span></dd></dl>

<dl><dt><a name="JITDenseLayer-zero_grad"><strong>zero_grad</strong></a>(self)</dt><dd><span class="code">Reset&nbsp;the&nbsp;gradients&nbsp;of&nbsp;the&nbsp;weights&nbsp;and&nbsp;biases&nbsp;to&nbsp;zero.</span></dd></dl>

<hr>
Data descriptors inherited from JITDenseLayer:<br>
<dl><dt><strong>__dict__</strong></dt>
<dd><span class="code">dictionary&nbsp;for&nbsp;instance&nbsp;variables</span></dd>
</dl>
<dl><dt><strong>__weakref__</strong></dt>
<dd><span class="code">list&nbsp;of&nbsp;weak&nbsp;references&nbsp;to&nbsp;the&nbsp;object</span></dd>
</dl>
</td></tr></table> <p>
<table class="section">
<tr class="decor title-decor heading-text">
<td class="section-title" colspan=3>&nbsp;<br><a name="JITFlattenLayer">class <strong>JITFlattenLayer</strong></a>(JITFlattenLayer)</td></tr>

<tr><td class="decor title-decor" rowspan=2><span class="code">&nbsp;&nbsp;&nbsp;</span></td>
<td class="decor title-decor" colspan=2><span class="code"><a href="#JITFlattenLayer">JITFlattenLayer</a>(*args,&nbsp;**kwargs)<br>
&nbsp;<br>
A&nbsp;layer&nbsp;that&nbsp;flattens&nbsp;multi-dimensional&nbsp;input&nbsp;into&nbsp;a&nbsp;2D&nbsp;array&nbsp;(batch_size,&nbsp;flattened_size).<br>
&nbsp;<br>
Useful&nbsp;for&nbsp;transitioning&nbsp;from&nbsp;convolutional&nbsp;layers&nbsp;to&nbsp;dense&nbsp;layers.<br>&nbsp;</span></td></tr>
<tr><td>&nbsp;</td>
<td class="singlecolumn"><dl><dt>Method resolution order:</dt>
<dd><a href="sega_learn.neural_networks.layers_jit.html#JITFlattenLayer">JITFlattenLayer</a></dd>
<dd>JITFlattenLayer</dd>
<dd><a href="builtins.html#object">builtins.object</a></dd>
</dl>
<hr>
Data and other attributes defined here:<br>
<dl><dt><strong>class_type</strong> = jitclass.JITFlattenLayer#16fc5474890&lt;input_shape...put_cache:array(float64, 4d, A),input_size:int32&gt;</dl>

<hr>
Methods inherited from JITFlattenLayer:<br>
<dl><dt><a name="JITFlattenLayer-__init__"><strong>__init__</strong></a>(self)</dt><dd><span class="code">Initializes&nbsp;the&nbsp;layer&nbsp;with&nbsp;placeholder&nbsp;values&nbsp;for&nbsp;input&nbsp;and&nbsp;output&nbsp;dimensions.<br>
&nbsp;<br>
Attributes:<br>
&nbsp;&nbsp;&nbsp;&nbsp;input_shape:&nbsp;(tuple)&nbsp;-&nbsp;Shape&nbsp;of&nbsp;the&nbsp;input&nbsp;data,&nbsp;initialized&nbsp;to&nbsp;(0,&nbsp;0,&nbsp;0).<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;will&nbsp;be&nbsp;set&nbsp;during&nbsp;the&nbsp;forward&nbsp;pass.<br>
&nbsp;&nbsp;&nbsp;&nbsp;output_size:&nbsp;(int)&nbsp;-&nbsp;Size&nbsp;of&nbsp;the&nbsp;output,&nbsp;initialized&nbsp;to&nbsp;0.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;will&nbsp;be&nbsp;set&nbsp;during&nbsp;the&nbsp;forward&nbsp;pass.<br>
&nbsp;&nbsp;&nbsp;&nbsp;input_size:&nbsp;(int)&nbsp;-&nbsp;Size&nbsp;of&nbsp;the&nbsp;input,&nbsp;initialized&nbsp;to&nbsp;0.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;will&nbsp;be&nbsp;set&nbsp;during&nbsp;the&nbsp;forward&nbsp;pass.<br>
&nbsp;&nbsp;&nbsp;&nbsp;input_cache:&nbsp;(any)&nbsp;-&nbsp;Cache&nbsp;for&nbsp;input&nbsp;data,&nbsp;to&nbsp;be&nbsp;set&nbsp;during&nbsp;the&nbsp;forward&nbsp;pass.</span></dd></dl>

<dl><dt><a name="JITFlattenLayer-backward"><strong>backward</strong></a>(self, dA, reg_lambda=0)</dt><dd><span class="code">Reshapes&nbsp;the&nbsp;gradient&nbsp;back&nbsp;to&nbsp;the&nbsp;original&nbsp;input&nbsp;shape.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;dA&nbsp;(np.ndarray):&nbsp;Gradient&nbsp;of&nbsp;the&nbsp;loss&nbsp;with&nbsp;respect&nbsp;to&nbsp;the&nbsp;layer's&nbsp;output,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;shape&nbsp;(batch_size,&nbsp;flattened_size)<br>
&nbsp;&nbsp;&nbsp;&nbsp;reg_lambda&nbsp;(float):&nbsp;Regularization&nbsp;parameter&nbsp;(unused&nbsp;in&nbsp;FlattenLayer).<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;np.ndarray:&nbsp;Gradient&nbsp;with&nbsp;respect&nbsp;to&nbsp;the&nbsp;input,&nbsp;reshaped&nbsp;to&nbsp;original&nbsp;input&nbsp;shape.</span></dd></dl>

<dl><dt><a name="JITFlattenLayer-forward"><strong>forward</strong></a>(self, X)</dt><dd><span class="code">Flattens&nbsp;the&nbsp;input&nbsp;tensor.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;X&nbsp;(np.ndarray):&nbsp;Input&nbsp;data&nbsp;of&nbsp;shape&nbsp;(batch_size,&nbsp;channels,&nbsp;height,&nbsp;width)<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;np.ndarray:&nbsp;Flattened&nbsp;output&nbsp;of&nbsp;shape&nbsp;(batch_size,&nbsp;flattened_size)</span></dd></dl>

<hr>
Data descriptors inherited from JITFlattenLayer:<br>
<dl><dt><strong>__dict__</strong></dt>
<dd><span class="code">dictionary&nbsp;for&nbsp;instance&nbsp;variables</span></dd>
</dl>
<dl><dt><strong>__weakref__</strong></dt>
<dd><span class="code">list&nbsp;of&nbsp;weak&nbsp;references&nbsp;to&nbsp;the&nbsp;object</span></dd>
</dl>
</td></tr></table> <p>
<table class="section">
<tr class="decor title-decor heading-text">
<td class="section-title" colspan=3>&nbsp;<br><a name="JITRNNLayer">class <strong>JITRNNLayer</strong></a>(<a href="builtins.html#object">builtins.object</a>)</td></tr>

<tr><td class="decor title-decor" rowspan=2><span class="code">&nbsp;&nbsp;&nbsp;</span></td>
<td class="decor title-decor" colspan=2><span class="code"><a href="#JITRNNLayer">JITRNNLayer</a>(input_size,&nbsp;hidden_size,&nbsp;activation='tanh')<br>
&nbsp;<br>
A&nbsp;recurrent&nbsp;layer&nbsp;implementation&nbsp;for&nbsp;neural&nbsp;networks&nbsp;using&nbsp;Numba&nbsp;JIT&nbsp;compilation.<br>&nbsp;</span></td></tr>
<tr><td>&nbsp;</td>
<td class="singlecolumn">Methods defined here:<br>
<dl><dt><a name="JITRNNLayer-__init__"><strong>__init__</strong></a>(self, input_size, hidden_size, activation='tanh')</dt><dd><span class="code">Will&nbsp;be&nbsp;implemented&nbsp;later.</span></dd></dl>

<hr>
Data descriptors defined here:<br>
<dl><dt><strong>__dict__</strong></dt>
<dd><span class="code">dictionary&nbsp;for&nbsp;instance&nbsp;variables</span></dd>
</dl>
<dl><dt><strong>__weakref__</strong></dt>
<dd><span class="code">list&nbsp;of&nbsp;weak&nbsp;references&nbsp;to&nbsp;the&nbsp;object</span></dd>
</dl>
</td></tr></table></td></tr></table><p>
<table class="section">
<tr class="decor data-decor heading-text">
<td class="section-title" colspan=3>&nbsp;<br><strong class="bigsection">Data</strong></td></tr>

<tr><td class="decor data-decor"><span class="code">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span></td><td>&nbsp;</td>
<td class="singlecolumn"><strong>CACHE</strong> = False<br>
<strong>conv_spec</strong> = [('in_channels', int32), ('out_channels', int32), ('kernel_size', int32), ('stride', int32), ('padding', int32), ('weights', Array(float64, 4, 'A', False, aligned=True)), ('biases', Array(float64, 2, 'A', False, aligned=True)), ('activation', unicode_type), ('weight_gradients', Array(float64, 4, 'A', False, aligned=True)), ('bias_gradients', Array(float64, 2, 'A', False, aligned=True)), ('input_cache', Array(float64, 4, 'A', False, aligned=True)), ('X_cols', Array(float64, 3, 'A', False, aligned=True)), ('X_padded', Array(float64, 4, 'A', False, aligned=True)), ('h_out', int32), ('w_out', int32), ('input_size', int32), ('output_size', int32)]<br>
<strong>flatten_spec</strong> = [('input_shape', UniTuple(int32, 3)), ('output_size', int32), ('input_cache', Array(float64, 4, 'A', False, aligned=True)), ('input_size', int32)]<br>
<strong>float64</strong> = float64<br>
<strong>int32</strong> = int32<br>
<strong>spec</strong> = [('weights', Array(float64, 2, 'C', False, aligned=True)), ('biases', Array(float64, 2, 'C', False, aligned=True)), ('activation', unicode_type), ('weight_gradients', Array(float64, 2, 'C', False, aligned=True)), ('bias_gradients', Array(float64, 2, 'C', False, aligned=True)), ('input_cache', Array(float64, 2, 'C', False, aligned=True)), ('output_cache', Array(float64, 2, 'C', False, aligned=True)), ('input_size', int32), ('output_size', int32)]</td></tr></table>
</body></html>
