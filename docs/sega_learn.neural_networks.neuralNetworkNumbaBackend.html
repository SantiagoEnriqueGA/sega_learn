<!DOCTYPE html>
<html lang="en">
<head>
<style>
body { background-color: #f0f0f8; }
table.heading tr { background-color: #7799ee; }
.decor { color: #ffffff; }
.title-decor { background-color: #ffc8d8; color: #000000; }
.pkg-content-decor { background-color: #aa55cc; }
.index-decor { background-color: #ee77aa; }
.functions-decor { background-color: #eeaa77; }
.data-decor { background-color: #55aa55; }
.author-decor { background-color: #7799ee; }
.credits-decor { background-color: #7799ee; }
.error-decor { background-color: #bb0000; }
.grey { color: #909090; }
.white { color: #ffffff; }
.repr { color: #c040c0; }
table.heading tr td.title, table.heading tr td.extra { vertical-align: bottom; }
table.heading tr td.extra { text-align: right; }
.heading-text { font-family: helvetica, arial; }
.bigsection { font-size: larger; }
.title { font-size: x-large; }
.code { font-family: monospace; }
table { width: 100%; border-spacing: 0; border-collapse: collapse; border: 0; }
td { padding: 2; }
td.section-title, td.multicolumn { vertical-align: bottom; }
td.multicolumn { width: 25%; }
td.singlecolumn { width: 100%; }
</style>
<meta charset="utf-8">
<title>Python: module sega_learn.neural_networks.neuralNetworkNumbaBackend</title>
</head><body>

<table class="heading">
<tr class="heading-text decor">
<td class="title">&nbsp;<br><strong class="title"><a href="sega_learn.html" class="white">sega_learn</a>.<a href="sega_learn.neural_networks.html" class="white">neural_networks</a>.neuralNetworkNumbaBackend</strong></td>
</tr></table>
    <p></p>
<p>
<table class="section">
<tr class="decor pkg-content-decor heading-text">
<td class="section-title" colspan=3>&nbsp;<br><strong class="bigsection">Modules</strong></td></tr>

<tr><td class="decor pkg-content-decor"><span class="code">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span></td><td>&nbsp;</td>
<td class="singlecolumn"><table><tr><td class="multicolumn"><a href="numpy.html">numpy</a><br>
</td><td class="multicolumn"><a href="numba.core.types.html">numba.core.types</a><br>
</td><td class="multicolumn"><a href="warnings.html">warnings</a><br>
</td><td class="multicolumn"></td></tr></table></td></tr></table><p>
<table class="section">
<tr class="decor index-decor heading-text">
<td class="section-title" colspan=3>&nbsp;<br><strong class="bigsection">Classes</strong></td></tr>

<tr><td class="decor index-decor"><span class="code">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span></td><td>&nbsp;</td>
<td class="singlecolumn"><dl>
<dt class="heading-text"><a href="sega_learn.neural_networks.neuralNetworkBase.html#NeuralNetworkBase">sega_learn.neural_networks.neuralNetworkBase.NeuralNetworkBase</a>(<a href="builtins.html#object">builtins.object</a>)
</dt><dd>
<dl>
<dt class="heading-text"><a href="sega_learn.neural_networks.neuralNetworkNumbaBackend.html#NumbaBackendNeuralNetwork">NumbaBackendNeuralNetwork</a>
</dt></dl>
</dd>
</dl>
 <p>
<table class="section">
<tr class="decor title-decor heading-text">
<td class="section-title" colspan=3>&nbsp;<br><a name="NumbaBackendNeuralNetwork">class <strong>NumbaBackendNeuralNetwork</strong></a>(<a href="sega_learn.neural_networks.neuralNetworkBase.html#NeuralNetworkBase">sega_learn.neural_networks.neuralNetworkBase.NeuralNetworkBase</a>)</td></tr>

<tr><td class="decor title-decor" rowspan=2><span class="code">&nbsp;&nbsp;&nbsp;</span></td>
<td class="decor title-decor" colspan=2><span class="code"><a href="#NumbaBackendNeuralNetwork">NumbaBackendNeuralNetwork</a>(layers,&nbsp;dropout_rate=0.2,&nbsp;reg_lambda=0.01,&nbsp;activations=None,&nbsp;loss_function=None,&nbsp;regressor=False,&nbsp;compile_numba=True,&nbsp;progress_bar=True)<br>
&nbsp;<br>
A&nbsp;neural&nbsp;network&nbsp;implementation&nbsp;using&nbsp;Numba&nbsp;for&nbsp;Just-In-Time&nbsp;(JIT)&nbsp;compilation&nbsp;to&nbsp;optimize&nbsp;performance.<br>
&nbsp;<br>
This&nbsp;class&nbsp;supports&nbsp;forward&nbsp;and&nbsp;backward&nbsp;propagation,&nbsp;training,&nbsp;evaluation,&nbsp;and&nbsp;hyperparameter&nbsp;tuning<br>
with&nbsp;various&nbsp;optimizers&nbsp;and&nbsp;activation&nbsp;functions.<br>
&nbsp;<br>
Attributes:<br>
&nbsp;&nbsp;&nbsp;&nbsp;compiled&nbsp;(bool):&nbsp;Indicates&nbsp;whether&nbsp;Numba&nbsp;functions&nbsp;are&nbsp;compiled.<br>
&nbsp;&nbsp;&nbsp;&nbsp;trainable_layers&nbsp;(list):&nbsp;Layers&nbsp;with&nbsp;trainable&nbsp;parameters&nbsp;(weights&nbsp;and&nbsp;biases).<br>
&nbsp;&nbsp;&nbsp;&nbsp;progress_bar&nbsp;(bool):&nbsp;Whether&nbsp;to&nbsp;display&nbsp;a&nbsp;progress&nbsp;bar&nbsp;during&nbsp;training.<br>
&nbsp;<br>
Methods:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#NumbaBackendNeuralNetwork-__init__">__init__</a>(layers,&nbsp;dropout_rate,&nbsp;reg_lambda,&nbsp;activations,&nbsp;compile_numba,&nbsp;progress_bar):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Initializes&nbsp;the&nbsp;neural&nbsp;network&nbsp;with&nbsp;the&nbsp;specified&nbsp;parameters.<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#NumbaBackendNeuralNetwork-store_init_layers">store_init_layers</a>():<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Stores&nbsp;the&nbsp;initial&nbsp;layers&nbsp;and&nbsp;their&nbsp;parameters&nbsp;for&nbsp;restoration&nbsp;after&nbsp;initialization.<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#NumbaBackendNeuralNetwork-restore_layers">restore_layers</a>():<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Restores&nbsp;the&nbsp;layers&nbsp;and&nbsp;their&nbsp;parameters&nbsp;after&nbsp;initialization.<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#NumbaBackendNeuralNetwork-initialize_new_layers">initialize_new_layers</a>():<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Initializes&nbsp;the&nbsp;layers&nbsp;of&nbsp;the&nbsp;neural&nbsp;network&nbsp;with&nbsp;specified&nbsp;sizes&nbsp;and&nbsp;activation&nbsp;functions.<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#NumbaBackendNeuralNetwork-forward">forward</a>(X,&nbsp;training):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Performs&nbsp;forward&nbsp;propagation&nbsp;through&nbsp;the&nbsp;neural&nbsp;network.<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#NumbaBackendNeuralNetwork-backward">backward</a>(y):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Performs&nbsp;backward&nbsp;propagation&nbsp;to&nbsp;calculate&nbsp;gradients.<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#NumbaBackendNeuralNetwork-is_not_instance_of_classes">is_not_instance_of_classes</a>(obj,&nbsp;classes):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Checks&nbsp;if&nbsp;an&nbsp;object&nbsp;is&nbsp;not&nbsp;an&nbsp;instance&nbsp;of&nbsp;any&nbsp;class&nbsp;in&nbsp;a&nbsp;list&nbsp;of&nbsp;classes.<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#NumbaBackendNeuralNetwork-train">train</a>(X_train,&nbsp;y_train,&nbsp;X_val,&nbsp;y_val,&nbsp;optimizer,&nbsp;epochs,&nbsp;batch_size,&nbsp;early_stopping_threshold,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lr_scheduler,&nbsp;p,&nbsp;use_tqdm,&nbsp;n_jobs,&nbsp;track_metrics,&nbsp;track_adv_metrics,&nbsp;save_animation,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;save_path,&nbsp;fps,&nbsp;dpi,&nbsp;frame_every):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Trains&nbsp;the&nbsp;neural&nbsp;network&nbsp;model&nbsp;with&nbsp;the&nbsp;specified&nbsp;parameters.<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#NumbaBackendNeuralNetwork-evaluate">evaluate</a>(X,&nbsp;y):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Evaluates&nbsp;the&nbsp;neural&nbsp;network&nbsp;on&nbsp;the&nbsp;given&nbsp;data&nbsp;and&nbsp;returns&nbsp;accuracy&nbsp;and&nbsp;predictions.<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#NumbaBackendNeuralNetwork-predict">predict</a>(X):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Predicts&nbsp;the&nbsp;output&nbsp;for&nbsp;the&nbsp;given&nbsp;input&nbsp;data.<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#NumbaBackendNeuralNetwork-calculate_loss">calculate_loss</a>(X,&nbsp;y):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Calculates&nbsp;the&nbsp;loss&nbsp;with&nbsp;L2&nbsp;regularization.<br>
&nbsp;&nbsp;&nbsp;&nbsp;_create_optimizer(optimizer_type,&nbsp;learning_rate,&nbsp;JIT):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Helper&nbsp;method&nbsp;to&nbsp;create&nbsp;optimizer&nbsp;instances.<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#NumbaBackendNeuralNetwork-tune_hyperparameters">tune_hyperparameters</a>(X_train,&nbsp;y_train,&nbsp;X_val,&nbsp;y_val,&nbsp;param_grid,&nbsp;layer_configs,&nbsp;optimizer_types,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lr_range,&nbsp;epochs,&nbsp;batch_size):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Performs&nbsp;hyperparameter&nbsp;tuning&nbsp;using&nbsp;grid&nbsp;search.<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#NumbaBackendNeuralNetwork-compile_numba_functions">compile_numba_functions</a>(progress_bar):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Compiles&nbsp;all&nbsp;Numba&nbsp;JIT&nbsp;functions&nbsp;to&nbsp;improve&nbsp;performance.<br>&nbsp;</span></td></tr>
<tr><td>&nbsp;</td>
<td class="singlecolumn"><dl><dt>Method resolution order:</dt>
<dd><a href="sega_learn.neural_networks.neuralNetworkNumbaBackend.html#NumbaBackendNeuralNetwork">NumbaBackendNeuralNetwork</a></dd>
<dd><a href="sega_learn.neural_networks.neuralNetworkBase.html#NeuralNetworkBase">sega_learn.neural_networks.neuralNetworkBase.NeuralNetworkBase</a></dd>
<dd><a href="builtins.html#object">builtins.object</a></dd>
</dl>
<hr>
Methods defined here:<br>
<dl><dt><a name="NumbaBackendNeuralNetwork-__init__"><strong>__init__</strong></a>(self, layers, dropout_rate=0.2, reg_lambda=0.01, activations=None, loss_function=None, regressor=False, compile_numba=True, progress_bar=True)</dt><dd><span class="code">Initializes&nbsp;the&nbsp;Numba&nbsp;backend&nbsp;neural&nbsp;network.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;layers:&nbsp;(list)&nbsp;-&nbsp;List&nbsp;of&nbsp;layer&nbsp;sizes&nbsp;or&nbsp;Layer&nbsp;objects.<br>
&nbsp;&nbsp;&nbsp;&nbsp;dropout_rate:&nbsp;(float)&nbsp;-&nbsp;Dropout&nbsp;rate&nbsp;for&nbsp;regularization.<br>
&nbsp;&nbsp;&nbsp;&nbsp;reg_lambda:&nbsp;(float)&nbsp;-&nbsp;L2&nbsp;regularization&nbsp;parameter.<br>
&nbsp;&nbsp;&nbsp;&nbsp;activations:&nbsp;(list)&nbsp;-&nbsp;List&nbsp;of&nbsp;activation&nbsp;functions&nbsp;for&nbsp;each&nbsp;layer.<br>
&nbsp;&nbsp;&nbsp;&nbsp;loss_function:&nbsp;(callable)&nbsp;optional&nbsp;-&nbsp;Custom&nbsp;loss&nbsp;function&nbsp;(default:&nbsp;selects&nbsp;based&nbsp;on&nbsp;task).<br>
&nbsp;&nbsp;&nbsp;&nbsp;regressor:&nbsp;(bool)&nbsp;-&nbsp;Whether&nbsp;the&nbsp;model&nbsp;is&nbsp;a&nbsp;regressor&nbsp;(default&nbsp;is&nbsp;False).<br>
&nbsp;&nbsp;&nbsp;&nbsp;compile_numba:&nbsp;(bool)&nbsp;-&nbsp;Whether&nbsp;to&nbsp;compile&nbsp;Numba&nbsp;functions.<br>
&nbsp;&nbsp;&nbsp;&nbsp;progress_bar:&nbsp;(bool)&nbsp;-&nbsp;Whether&nbsp;to&nbsp;display&nbsp;a&nbsp;progress&nbsp;bar.</span></dd></dl>

<dl><dt><a name="NumbaBackendNeuralNetwork-backward"><strong>backward</strong></a>(self, y)</dt><dd><span class="code">Performs&nbsp;backward&nbsp;propagation&nbsp;to&nbsp;calculate&nbsp;the&nbsp;gradients.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;y&nbsp;(ndarray):&nbsp;Target&nbsp;labels&nbsp;of&nbsp;shape&nbsp;(m,&nbsp;output_size).</span></dd></dl>

<dl><dt><a name="NumbaBackendNeuralNetwork-calculate_loss"><strong>calculate_loss</strong></a>(self, X, y)</dt><dd><span class="code">Calculates&nbsp;the&nbsp;loss&nbsp;with&nbsp;L2&nbsp;regularization.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;X&nbsp;(ndarray):&nbsp;Input&nbsp;data.<br>
&nbsp;&nbsp;&nbsp;&nbsp;y&nbsp;(ndarray):&nbsp;Target&nbsp;labels.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;float:&nbsp;The&nbsp;calculated&nbsp;loss&nbsp;value.</span></dd></dl>

<dl><dt><a name="NumbaBackendNeuralNetwork-compile_numba_functions"><strong>compile_numba_functions</strong></a>(self, progress_bar=True)</dt><dd><span class="code">Compiles&nbsp;all&nbsp;Numba&nbsp;JIT&nbsp;functions&nbsp;to&nbsp;improve&nbsp;performance.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;progress_bar&nbsp;(bool):&nbsp;Whether&nbsp;to&nbsp;display&nbsp;a&nbsp;progress&nbsp;bar.</span></dd></dl>

<dl><dt><a name="NumbaBackendNeuralNetwork-evaluate"><strong>evaluate</strong></a>(self, X, y)</dt><dd><span class="code">Evaluates&nbsp;the&nbsp;neural&nbsp;network&nbsp;on&nbsp;the&nbsp;given&nbsp;data.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;X&nbsp;(ndarray):&nbsp;Input&nbsp;data.<br>
&nbsp;&nbsp;&nbsp;&nbsp;y&nbsp;(ndarray):&nbsp;Target&nbsp;labels.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tuple:&nbsp;Accuracy&nbsp;and&nbsp;predicted&nbsp;labels.</span></dd></dl>

<dl><dt><a name="NumbaBackendNeuralNetwork-forward"><strong>forward</strong></a>(self, X, training=True)</dt><dd><span class="code">Performs&nbsp;forward&nbsp;propagation&nbsp;through&nbsp;the&nbsp;neural&nbsp;network.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;X&nbsp;(ndarray):&nbsp;Input&nbsp;data&nbsp;of&nbsp;shape&nbsp;(batch_size,&nbsp;input_size).<br>
&nbsp;&nbsp;&nbsp;&nbsp;training&nbsp;(bool):&nbsp;Whether&nbsp;the&nbsp;network&nbsp;is&nbsp;in&nbsp;training&nbsp;mode&nbsp;(applies&nbsp;dropout).<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;ndarray:&nbsp;Output&nbsp;predictions&nbsp;of&nbsp;shape&nbsp;(batch_size,&nbsp;output_size).</span></dd></dl>

<dl><dt><a name="NumbaBackendNeuralNetwork-initialize_new_layers"><strong>initialize_new_layers</strong></a>(self)</dt><dd><span class="code">Initializes&nbsp;the&nbsp;layers&nbsp;of&nbsp;the&nbsp;neural&nbsp;network.<br>
&nbsp;<br>
Each&nbsp;layer&nbsp;is&nbsp;created&nbsp;with&nbsp;the&nbsp;specified&nbsp;number&nbsp;of&nbsp;neurons&nbsp;and&nbsp;activation&nbsp;function.</span></dd></dl>

<dl><dt><a name="NumbaBackendNeuralNetwork-predict"><strong>predict</strong></a>(self, X)</dt><dd><span class="code">Predicts&nbsp;the&nbsp;output&nbsp;for&nbsp;the&nbsp;given&nbsp;input&nbsp;data.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;X&nbsp;(ndarray):&nbsp;Input&nbsp;data.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;ndarray:&nbsp;Predicted&nbsp;outputs.</span></dd></dl>

<dl><dt><a name="NumbaBackendNeuralNetwork-restore_layers"><strong>restore_layers</strong></a>(self)</dt><dd><span class="code">Restores&nbsp;the&nbsp;layers&nbsp;after&nbsp;initialization.</span></dd></dl>

<dl><dt><a name="NumbaBackendNeuralNetwork-store_init_layers"><strong>store_init_layers</strong></a>(self)</dt><dd><span class="code">Stores&nbsp;the&nbsp;layers&nbsp;to&nbsp;restore&nbsp;after&nbsp;initialization.</span></dd></dl>

<dl><dt><a name="NumbaBackendNeuralNetwork-train"><strong>train</strong></a>(self, X_train, y_train, X_val=None, y_val=None, optimizer=None, epochs=100, batch_size=32, early_stopping_threshold=10, lr_scheduler=None, p=True, use_tqdm=True, n_jobs=1, track_metrics=False, track_adv_metrics=False, save_animation=False, save_path='training_animation.mp4', fps=1, dpi=100, frame_every=1)</dt><dd><span class="code">Trains&nbsp;the&nbsp;neural&nbsp;network&nbsp;model.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;X_train:&nbsp;(ndarray)&nbsp;-&nbsp;Training&nbsp;data&nbsp;features.<br>
&nbsp;&nbsp;&nbsp;&nbsp;y_train:&nbsp;(ndarray)&nbsp;-&nbsp;Training&nbsp;data&nbsp;labels.<br>
&nbsp;&nbsp;&nbsp;&nbsp;X_val:&nbsp;(ndarray)&nbsp;-&nbsp;Validation&nbsp;data&nbsp;features,&nbsp;optional.<br>
&nbsp;&nbsp;&nbsp;&nbsp;y_val:&nbsp;(ndarray)&nbsp;-&nbsp;Validation&nbsp;data&nbsp;labels,&nbsp;optional.<br>
&nbsp;&nbsp;&nbsp;&nbsp;optimizer:&nbsp;(Optimizer)&nbsp;-&nbsp;Optimizer&nbsp;for&nbsp;updating&nbsp;parameters&nbsp;(default:&nbsp;JITAdam,&nbsp;lr=0.0001).<br>
&nbsp;&nbsp;&nbsp;&nbsp;epochs:&nbsp;(int)&nbsp;-&nbsp;Number&nbsp;of&nbsp;training&nbsp;epochs&nbsp;(default:&nbsp;100).<br>
&nbsp;&nbsp;&nbsp;&nbsp;batch_size:&nbsp;(int)&nbsp;-&nbsp;Batch&nbsp;size&nbsp;for&nbsp;mini-batch&nbsp;gradient&nbsp;descent&nbsp;(default:&nbsp;32).<br>
&nbsp;&nbsp;&nbsp;&nbsp;early_stopping_threshold:&nbsp;(int)&nbsp;-&nbsp;Patience&nbsp;for&nbsp;early&nbsp;stopping&nbsp;(default:&nbsp;10).<br>
&nbsp;&nbsp;&nbsp;&nbsp;lr_scheduler:&nbsp;(Scheduler)&nbsp;-&nbsp;Learning&nbsp;rate&nbsp;scheduler&nbsp;(default:&nbsp;None).<br>
&nbsp;&nbsp;&nbsp;&nbsp;p:&nbsp;(bool)&nbsp;-&nbsp;Whether&nbsp;to&nbsp;print&nbsp;training&nbsp;progress&nbsp;(default:&nbsp;True).<br>
&nbsp;&nbsp;&nbsp;&nbsp;use_tqdm:&nbsp;(bool)&nbsp;-&nbsp;Whether&nbsp;to&nbsp;use&nbsp;tqdm&nbsp;for&nbsp;progress&nbsp;bar&nbsp;(default:&nbsp;True).<br>
&nbsp;&nbsp;&nbsp;&nbsp;n_jobs:&nbsp;(int)&nbsp;-&nbsp;Number&nbsp;of&nbsp;jobs&nbsp;for&nbsp;parallel&nbsp;processing&nbsp;(default:&nbsp;1).<br>
&nbsp;&nbsp;&nbsp;&nbsp;track_metrics:&nbsp;(bool)&nbsp;-&nbsp;Whether&nbsp;to&nbsp;track&nbsp;training&nbsp;metrics&nbsp;(default:&nbsp;False).<br>
&nbsp;&nbsp;&nbsp;&nbsp;track_adv_metrics:&nbsp;(bool)&nbsp;-&nbsp;Whether&nbsp;to&nbsp;track&nbsp;advanced&nbsp;metrics&nbsp;(default:&nbsp;False).<br>
&nbsp;&nbsp;&nbsp;&nbsp;save_animation:&nbsp;(bool)&nbsp;-&nbsp;Whether&nbsp;to&nbsp;save&nbsp;the&nbsp;animation&nbsp;of&nbsp;metrics&nbsp;(default:&nbsp;False).<br>
&nbsp;&nbsp;&nbsp;&nbsp;save_path:&nbsp;(str)&nbsp;-&nbsp;Path&nbsp;to&nbsp;save&nbsp;the&nbsp;animation&nbsp;file.&nbsp;File&nbsp;extension&nbsp;must&nbsp;be&nbsp;.mp4&nbsp;or&nbsp;.gif&nbsp;(default:&nbsp;'training_animation.mp4').<br>
&nbsp;&nbsp;&nbsp;&nbsp;fps:&nbsp;(int)&nbsp;-&nbsp;Frames&nbsp;per&nbsp;second&nbsp;for&nbsp;the&nbsp;saved&nbsp;animation&nbsp;(default:&nbsp;1).<br>
&nbsp;&nbsp;&nbsp;&nbsp;dpi:&nbsp;(int)&nbsp;-&nbsp;DPI&nbsp;for&nbsp;the&nbsp;saved&nbsp;animation&nbsp;(default:&nbsp;100).<br>
&nbsp;&nbsp;&nbsp;&nbsp;frame_every:&nbsp;(int)&nbsp;-&nbsp;Capture&nbsp;frame&nbsp;every&nbsp;N&nbsp;epochs&nbsp;(to&nbsp;reduce&nbsp;file&nbsp;size)&nbsp;(default:&nbsp;1).</span></dd></dl>

<dl><dt><a name="NumbaBackendNeuralNetwork-tune_hyperparameters"><strong>tune_hyperparameters</strong></a>(self, X_train, y_train, X_val, y_val, param_grid, layer_configs=None, optimizer_types=None, lr_range=(0.0001, 0.01, 5), epochs=30, batch_size=32)</dt><dd><span class="code">Performs&nbsp;hyperparameter&nbsp;tuning&nbsp;using&nbsp;grid&nbsp;search.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;X_train:&nbsp;(np.ndarray)&nbsp;-&nbsp;Training&nbsp;feature&nbsp;data.<br>
&nbsp;&nbsp;&nbsp;&nbsp;y_train:&nbsp;(np.ndarray)&nbsp;-&nbsp;Training&nbsp;target&nbsp;data.<br>
&nbsp;&nbsp;&nbsp;&nbsp;X_val:&nbsp;(np.ndarray)&nbsp;-&nbsp;Validation&nbsp;feature&nbsp;data.<br>
&nbsp;&nbsp;&nbsp;&nbsp;y_val:&nbsp;(np.ndarray)&nbsp;-&nbsp;Validation&nbsp;target&nbsp;data.<br>
&nbsp;&nbsp;&nbsp;&nbsp;param_grid:&nbsp;(dict)&nbsp;-&nbsp;Dictionary&nbsp;of&nbsp;parameters&nbsp;to&nbsp;try.<br>
&nbsp;&nbsp;&nbsp;&nbsp;layer_configs:&nbsp;(list),&nbsp;optional&nbsp;-&nbsp;List&nbsp;of&nbsp;layer&nbsp;configurations&nbsp;(default&nbsp;is&nbsp;None).<br>
&nbsp;&nbsp;&nbsp;&nbsp;optimizer_types:&nbsp;(list),&nbsp;optional&nbsp;-&nbsp;List&nbsp;of&nbsp;optimizer&nbsp;types&nbsp;(default&nbsp;is&nbsp;None).<br>
&nbsp;&nbsp;&nbsp;&nbsp;lr_range:&nbsp;(tuple),&nbsp;optional&nbsp;-&nbsp;(min_lr,&nbsp;max_lr,&nbsp;num_steps)&nbsp;for&nbsp;learning&nbsp;rates&nbsp;(default&nbsp;is&nbsp;(0.0001,&nbsp;0.01,&nbsp;5)).<br>
&nbsp;&nbsp;&nbsp;&nbsp;epochs:&nbsp;(int),&nbsp;optional&nbsp;-&nbsp;Max&nbsp;epochs&nbsp;for&nbsp;each&nbsp;trial&nbsp;(default&nbsp;is&nbsp;30).<br>
&nbsp;&nbsp;&nbsp;&nbsp;batch_size:&nbsp;(int),&nbsp;optional&nbsp;-&nbsp;Batch&nbsp;size&nbsp;for&nbsp;training&nbsp;(default&nbsp;is&nbsp;32).<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;best_params:&nbsp;(dict)&nbsp;-&nbsp;Best&nbsp;hyperparameters&nbsp;found.<br>
&nbsp;&nbsp;&nbsp;&nbsp;best_accuracy:&nbsp;(float)&nbsp;-&nbsp;Best&nbsp;validation&nbsp;accuracy.</span></dd></dl>

<hr>
Static methods defined here:<br>
<dl><dt><a name="NumbaBackendNeuralNetwork-is_not_instance_of_classes"><strong>is_not_instance_of_classes</strong></a>(obj, classes)</dt><dd><span class="code">Checks&nbsp;if&nbsp;an&nbsp;object&nbsp;is&nbsp;not&nbsp;an&nbsp;instance&nbsp;of&nbsp;any&nbsp;class&nbsp;in&nbsp;a&nbsp;list&nbsp;of&nbsp;classes.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;obj:&nbsp;The&nbsp;object&nbsp;to&nbsp;check.<br>
&nbsp;&nbsp;&nbsp;&nbsp;classes:&nbsp;A&nbsp;list&nbsp;of&nbsp;classes.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;bool:&nbsp;True&nbsp;if&nbsp;the&nbsp;object&nbsp;is&nbsp;not&nbsp;an&nbsp;instance&nbsp;of&nbsp;any&nbsp;class&nbsp;in&nbsp;the&nbsp;list&nbsp;of&nbsp;classes,&nbsp;False&nbsp;otherwise.</span></dd></dl>

<hr>
Methods inherited from <a href="sega_learn.neural_networks.neuralNetworkBase.html#NeuralNetworkBase">sega_learn.neural_networks.neuralNetworkBase.NeuralNetworkBase</a>:<br>
<dl><dt><a name="NumbaBackendNeuralNetwork-apply_dropout"><strong>apply_dropout</strong></a>(self, X)</dt><dd><span class="code">Applies&nbsp;dropout&nbsp;to&nbsp;the&nbsp;activation&nbsp;X.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;X:&nbsp;(ndarray)&nbsp;-&nbsp;Activation&nbsp;values.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;ndarray:&nbsp;Activation&nbsp;values&nbsp;after&nbsp;applying&nbsp;dropout.</span></dd></dl>

<dl><dt><a name="NumbaBackendNeuralNetwork-calculate_precision_recall_f1"><strong>calculate_precision_recall_f1</strong></a>(self, X, y)</dt><dd><span class="code">Calculates&nbsp;precision,&nbsp;recall,&nbsp;and&nbsp;F1&nbsp;score.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;X:&nbsp;(ndarray)&nbsp;-&nbsp;Input&nbsp;data<br>
&nbsp;&nbsp;&nbsp;&nbsp;y:&nbsp;(ndarray)&nbsp;-&nbsp;Target&nbsp;labels<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;precision:&nbsp;(float)&nbsp;-&nbsp;Precision&nbsp;score<br>
&nbsp;&nbsp;&nbsp;&nbsp;recall:&nbsp;(float)&nbsp;-&nbsp;Recall&nbsp;score<br>
&nbsp;&nbsp;&nbsp;&nbsp;f1:&nbsp;(float)&nbsp;-&nbsp;F1&nbsp;score</span></dd></dl>

<dl><dt><a name="NumbaBackendNeuralNetwork-compute_l2_reg"><strong>compute_l2_reg</strong></a>(self, weights)</dt><dd><span class="code">Computes&nbsp;the&nbsp;L2&nbsp;regularization&nbsp;term.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;weights:&nbsp;(list)&nbsp;-&nbsp;List&nbsp;of&nbsp;weight&nbsp;matrices.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;float:&nbsp;L2&nbsp;regularization&nbsp;term.</span></dd></dl>

<dl><dt><a name="NumbaBackendNeuralNetwork-create_scheduler"><strong>create_scheduler</strong></a>(self, scheduler_type, optimizer, **kwargs)</dt><dd><span class="code">Creates&nbsp;a&nbsp;learning&nbsp;rate&nbsp;scheduler.</span></dd></dl>

<dl><dt><a name="NumbaBackendNeuralNetwork-initialize_layers"><strong>initialize_layers</strong></a>(self)</dt><dd><span class="code">Initializes&nbsp;the&nbsp;weights&nbsp;and&nbsp;biases&nbsp;of&nbsp;the&nbsp;layers.</span></dd></dl>

<dl><dt><a name="NumbaBackendNeuralNetwork-plot_metrics"><strong>plot_metrics</strong></a>(self, save_dir=None)</dt><dd><span class="code">Plots&nbsp;the&nbsp;training&nbsp;and&nbsp;validation&nbsp;metrics.</span></dd></dl>

<hr>
Data descriptors inherited from <a href="sega_learn.neural_networks.neuralNetworkBase.html#NeuralNetworkBase">sega_learn.neural_networks.neuralNetworkBase.NeuralNetworkBase</a>:<br>
<dl><dt><strong>__dict__</strong></dt>
<dd><span class="code">dictionary&nbsp;for&nbsp;instance&nbsp;variables</span></dd>
</dl>
<dl><dt><strong>__weakref__</strong></dt>
<dd><span class="code">list&nbsp;of&nbsp;weak&nbsp;references&nbsp;to&nbsp;the&nbsp;object</span></dd>
</dl>
</td></tr></table></td></tr></table><p>
<table class="section">
<tr class="decor data-decor heading-text">
<td class="section-title" colspan=3>&nbsp;<br><strong class="bigsection">Data</strong></td></tr>

<tr><td class="decor data-decor"><span class="code">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span></td><td>&nbsp;</td>
<td class="singlecolumn"><strong>CACHE</strong> = False<br>
<strong>NUMBA_AVAILABLE</strong> = True<br>
<strong>TQDM_AVAILABLE</strong> = True<br>
<strong>conv_spec</strong> = [('in_channels', int32), ('out_channels', int32), ('kernel_size', int32), ('stride', int32), ('padding', int32), ('weights', Array(float64, 4, 'A', False, aligned=True)), ('biases', Array(float64, 2, 'A', False, aligned=True)), ('activation', unicode_type), ('weight_gradients', Array(float64, 4, 'A', False, aligned=True)), ('bias_gradients', Array(float64, 2, 'A', False, aligned=True)), ('input_cache', Array(float64, 4, 'A', False, aligned=True)), ('X_cols', Array(float64, 3, 'A', False, aligned=True)), ('X_padded', Array(float64, 4, 'A', False, aligned=True)), ('h_out', int32), ('w_out', int32), ('input_size', int32), ('output_size', int32)]<br>
<strong>flatten_spec</strong> = [('input_shape', UniTuple(int32, 3)), ('output_size', int32), ('input_cache', Array(float64, 4, 'A', False, aligned=True)), ('input_size', int32)]<br>
<strong>float64</strong> = float64<br>
<strong>int32</strong> = int32<br>
<strong>spec</strong> = [('weights', Array(float64, 2, 'C', False, aligned=True)), ('biases', Array(float64, 2, 'C', False, aligned=True)), ('activation', unicode_type), ('weight_gradients', Array(float64, 2, 'C', False, aligned=True)), ('bias_gradients', Array(float64, 2, 'C', False, aligned=True)), ('input_cache', Array(float64, 2, 'C', False, aligned=True)), ('output_cache', Array(float64, 2, 'C', False, aligned=True)), ('input_size', int32), ('output_size', int32)]<br>
<strong>spec_adadelta</strong> = [('learning_rate', float64), ('rho', float64), ('epsilon', float64), ('reg_lambda', float64), ('E_g2', Array(float64, 3, 'C', False, aligned=True)), ('E_delta_x2', Array(float64, 3, 'C', False, aligned=True))]<br>
<strong>spec_adam</strong> = [('learning_rate', float64), ('beta1', float64), ('beta2', float64), ('epsilon', float64), ('reg_lambda', float64), ('m', Array(float64, 3, 'C', False, aligned=True)), ('v', Array(float64, 3, 'C', False, aligned=True)), ('t', int32), ('dW', Array(float64, 2, 'A', False, aligned=True)), ('db', Array(float64, 2, 'A', False, aligned=True)), ('index', int32)]<br>
<strong>spec_sgd</strong> = [('learning_rate', float64), ('momentum', float64), ('reg_lambda', float64), ('velocity', Array(float64, 3, 'C', False, aligned=True))]</td></tr></table>
</body></html>
