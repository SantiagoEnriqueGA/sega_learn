<!DOCTYPE html>
<html lang="en">
<head>
<style>
body { background-color: #f0f0f8; }
table.heading tr { background-color: #7799ee; }
.decor { color: #ffffff; }
.title-decor { background-color: #ffc8d8; color: #000000; }
.pkg-content-decor { background-color: #aa55cc; }
.index-decor { background-color: #ee77aa; }
.functions-decor { background-color: #eeaa77; }
.data-decor { background-color: #55aa55; }
.author-decor { background-color: #7799ee; }
.credits-decor { background-color: #7799ee; }
.error-decor { background-color: #bb0000; }
.grey { color: #909090; }
.white { color: #ffffff; }
.repr { color: #c040c0; }
table.heading tr td.title, table.heading tr td.extra { vertical-align: bottom; }
table.heading tr td.extra { text-align: right; }
.heading-text { font-family: helvetica, arial; }
.bigsection { font-size: larger; }
.title { font-size: x-large; }
.code { font-family: monospace; }
table { width: 100%; border-spacing: 0; border-collapse: collapse; border: 0; }
td { padding: 2; }
td.section-title, td.multicolumn { vertical-align: bottom; }
td.multicolumn { width: 25%; }
td.singlecolumn { width: 100%; }
</style>
<meta charset="utf-8">
<title>Python: package sega_learn.neural_networks</title>
</head><body>

<table class="heading">
<tr class="heading-text decor">
<td class="title">&nbsp;<br><strong class="title"><a href="sega_learn.html" class="white">sega_learn</a>.neural_networks</strong></td>
</tr></table>
    <p></p>
<p>
<table class="section">
<tr class="decor pkg-content-decor heading-text">
<td class="section-title" colspan=3>&nbsp;<br><strong class="bigsection">Package Contents</strong></td></tr>
    
<tr><td class="decor pkg-content-decor"><span class="code">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span></td><td>&nbsp;</td>
<td class="singlecolumn"><table><tr><td class="multicolumn"><a href="sega_learn.neural_networks.activations.html">activations</a><br>
<a href="sega_learn.neural_networks.layers.html">layers</a><br>
<a href="sega_learn.neural_networks.layers_jit.html">layers_jit</a><br>
</td><td class="multicolumn"><a href="sega_learn.neural_networks.loss.html">loss</a><br>
<a href="sega_learn.neural_networks.loss_jit.html">loss_jit</a><br>
<a href="sega_learn.neural_networks.neuralNetworkBase.html">neuralNetworkBase</a><br>
</td><td class="multicolumn"><a href="sega_learn.neural_networks.neuralNetworkBaseBackend.html">neuralNetworkBaseBackend</a><br>
<a href="sega_learn.neural_networks.neuralNetworkNumbaBackend.html">neuralNetworkNumbaBackend</a><br>
<a href="sega_learn.neural_networks.numba_utils.html">numba_utils</a><br>
</td><td class="multicolumn"><a href="sega_learn.neural_networks.optimizers.html">optimizers</a><br>
<a href="sega_learn.neural_networks.optimizers_jit.html">optimizers_jit</a><br>
<a href="sega_learn.neural_networks.schedulers.html">schedulers</a><br>
</td></tr></table></td></tr></table><p>
<table class="section">
<tr class="decor index-decor heading-text">
<td class="section-title" colspan=3>&nbsp;<br><strong class="bigsection">Classes</strong></td></tr>
    
<tr><td class="decor index-decor"><span class="code">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span></td><td>&nbsp;</td>
<td class="singlecolumn"><dl>
<dt class="heading-text"><a href="builtins.html#object">builtins.object</a>
</dt><dd>
<dl>
<dt class="heading-text"><a href="sega_learn.neural_networks.activations.html#Activation">sega_learn.neural_networks.activations.Activation</a>
</dt><dt class="heading-text"><a href="sega_learn.neural_networks.layers.html#ConvLayer">sega_learn.neural_networks.layers.ConvLayer</a>
</dt><dt class="heading-text"><a href="sega_learn.neural_networks.layers.html#DenseLayer">sega_learn.neural_networks.layers.DenseLayer</a>
</dt><dt class="heading-text"><a href="sega_learn.neural_networks.layers.html#FlattenLayer">sega_learn.neural_networks.layers.FlattenLayer</a>
</dt><dt class="heading-text"><a href="sega_learn.neural_networks.layers.html#RNNLayer">sega_learn.neural_networks.layers.RNNLayer</a>
</dt><dt class="heading-text"><a href="sega_learn.neural_networks.layers_jit.html#JITRNNLayer">sega_learn.neural_networks.layers_jit.JITRNNLayer</a>
</dt><dt class="heading-text"><a href="sega_learn.neural_networks.loss.html#BCEWithLogitsLoss">sega_learn.neural_networks.loss.BCEWithLogitsLoss</a>
</dt><dt class="heading-text"><a href="sega_learn.neural_networks.loss.html#CrossEntropyLoss">sega_learn.neural_networks.loss.CrossEntropyLoss</a>
</dt><dt class="heading-text"><a href="sega_learn.neural_networks.loss_jit.html#JITBCEWithLogitsLoss">sega_learn.neural_networks.loss_jit.JITBCEWithLogitsLoss</a>
</dt><dt class="heading-text"><a href="sega_learn.neural_networks.loss_jit.html#JITCrossEntropyLoss">sega_learn.neural_networks.loss_jit.JITCrossEntropyLoss</a>
</dt><dt class="heading-text"><a href="sega_learn.neural_networks.neuralNetworkBase.html#NeuralNetworkBase">sega_learn.neural_networks.neuralNetworkBase.NeuralNetworkBase</a>
</dt><dd>
<dl>
<dt class="heading-text"><a href="sega_learn.neural_networks.neuralNetworkBaseBackend.html#BaseBackendNeuralNetwork">sega_learn.neural_networks.neuralNetworkBaseBackend.BaseBackendNeuralNetwork</a>
</dt><dt class="heading-text"><a href="sega_learn.neural_networks.neuralNetworkNumbaBackend.html#NumbaBackendNeuralNetwork">sega_learn.neural_networks.neuralNetworkNumbaBackend.NumbaBackendNeuralNetwork</a>
</dt></dl>
</dd>
<dt class="heading-text"><a href="sega_learn.neural_networks.optimizers.html#AdadeltaOptimizer">sega_learn.neural_networks.optimizers.AdadeltaOptimizer</a>
</dt><dt class="heading-text"><a href="sega_learn.neural_networks.optimizers.html#AdamOptimizer">sega_learn.neural_networks.optimizers.AdamOptimizer</a>
</dt><dt class="heading-text"><a href="sega_learn.neural_networks.optimizers.html#SGDOptimizer">sega_learn.neural_networks.optimizers.SGDOptimizer</a>
</dt><dt class="heading-text"><a href="sega_learn.neural_networks.schedulers.html#lr_scheduler_exp">sega_learn.neural_networks.schedulers.lr_scheduler_exp</a>
</dt><dt class="heading-text"><a href="sega_learn.neural_networks.schedulers.html#lr_scheduler_plateau">sega_learn.neural_networks.schedulers.lr_scheduler_plateau</a>
</dt><dt class="heading-text"><a href="sega_learn.neural_networks.schedulers.html#lr_scheduler_step">sega_learn.neural_networks.schedulers.lr_scheduler_step</a>
</dt></dl>
</dd>
<dt class="heading-text">sega_learn.neural_networks.layers_jit.JITConvLayer(<a href="builtins.html#object">builtins.object</a>)
</dt><dd>
<dl>
<dt class="heading-text"><a href="sega_learn.neural_networks.layers_jit.html#JITConvLayer">sega_learn.neural_networks.layers_jit.JITConvLayer</a>
</dt></dl>
</dd>
<dt class="heading-text">sega_learn.neural_networks.layers_jit.JITDenseLayer(<a href="builtins.html#object">builtins.object</a>)
</dt><dd>
<dl>
<dt class="heading-text"><a href="sega_learn.neural_networks.layers_jit.html#JITDenseLayer">sega_learn.neural_networks.layers_jit.JITDenseLayer</a>
</dt></dl>
</dd>
<dt class="heading-text">sega_learn.neural_networks.layers_jit.JITFlattenLayer(<a href="builtins.html#object">builtins.object</a>)
</dt><dd>
<dl>
<dt class="heading-text"><a href="sega_learn.neural_networks.layers_jit.html#JITFlattenLayer">sega_learn.neural_networks.layers_jit.JITFlattenLayer</a>
</dt></dl>
</dd>
<dt class="heading-text">sega_learn.neural_networks.optimizers_jit.JITAdadeltaOptimizer(<a href="builtins.html#object">builtins.object</a>)
</dt><dd>
<dl>
<dt class="heading-text"><a href="sega_learn.neural_networks.optimizers_jit.html#JITAdadeltaOptimizer">sega_learn.neural_networks.optimizers_jit.JITAdadeltaOptimizer</a>
</dt></dl>
</dd>
<dt class="heading-text">sega_learn.neural_networks.optimizers_jit.JITAdamOptimizer(<a href="builtins.html#object">builtins.object</a>)
</dt><dd>
<dl>
<dt class="heading-text"><a href="sega_learn.neural_networks.optimizers_jit.html#JITAdamOptimizer">sega_learn.neural_networks.optimizers_jit.JITAdamOptimizer</a>
</dt></dl>
</dd>
<dt class="heading-text">sega_learn.neural_networks.optimizers_jit.JITSGDOptimizer(<a href="builtins.html#object">builtins.object</a>)
</dt><dd>
<dl>
<dt class="heading-text"><a href="sega_learn.neural_networks.optimizers_jit.html#JITSGDOptimizer">sega_learn.neural_networks.optimizers_jit.JITSGDOptimizer</a>
</dt></dl>
</dd>
</dl>
 <p>
<table class="section">
<tr class="decor title-decor heading-text">
<td class="section-title" colspan=3>&nbsp;<br><a name="Activation">class <strong>Activation</strong></a>(<a href="builtins.html#object">builtins.object</a>)</td></tr>
    
<tr><td class="decor title-decor" rowspan=2><span class="code">&nbsp;&nbsp;&nbsp;</span></td>
<td class="decor title-decor" colspan=2><span class="code">This&nbsp;class&nbsp;contains&nbsp;various&nbsp;activation&nbsp;functions&nbsp;and&nbsp;their&nbsp;corresponding&nbsp;derivatives&nbsp;for&nbsp;use&nbsp;in&nbsp;neural&nbsp;networks.<br>
relu:&nbsp;Rectified&nbsp;Linear&nbsp;Unit&nbsp;activation&nbsp;function.&nbsp;Returns&nbsp;the&nbsp;input&nbsp;directly&nbsp;if&nbsp;it's&nbsp;positive,&nbsp;otherwise&nbsp;returns&nbsp;0.<br>
leaky_relu:&nbsp;Leaky&nbsp;ReLU&nbsp;activation&nbsp;function.&nbsp;A&nbsp;variant&nbsp;of&nbsp;ReLU&nbsp;that&nbsp;allows&nbsp;a&nbsp;small&nbsp;gradient&nbsp;when&nbsp;the&nbsp;input&nbsp;is&nbsp;negative.&nbsp;<br>
tanh:&nbsp;Hyperbolic&nbsp;tangent&nbsp;activation&nbsp;function.&nbsp;Maps&nbsp;input&nbsp;to&nbsp;range&nbsp;[-1,&nbsp;1].&nbsp;Commonly&nbsp;used&nbsp;for&nbsp;normalized&nbsp;input.<br>
sigmoid:&nbsp;Sigmoid&nbsp;activation&nbsp;function.&nbsp;Maps&nbsp;input&nbsp;to&nbsp;range&nbsp;[0,&nbsp;1].&nbsp;Commonly&nbsp;used&nbsp;for&nbsp;binary&nbsp;classification.<br>
softmax:&nbsp;Softmax&nbsp;activation&nbsp;function.&nbsp;Maps&nbsp;input&nbsp;into&nbsp;a&nbsp;probability&nbsp;distribution&nbsp;over&nbsp;multiple&nbsp;classes.<br>&nbsp;</span></td></tr>
<tr><td>&nbsp;</td>
<td class="singlecolumn">Static methods defined here:<br>
<dl><dt><a name="Activation-leaky_relu"><strong>leaky_relu</strong></a>(z, alpha=0.01)</dt><dd><span class="code">Leaky&nbsp;ReLU&nbsp;activation&nbsp;function:&nbsp;f(z)&nbsp;=&nbsp;z&nbsp;if&nbsp;z&nbsp;&gt;&nbsp;0,&nbsp;else&nbsp;alpha&nbsp;*&nbsp;z<br>
Allows&nbsp;a&nbsp;small,&nbsp;non-zero&nbsp;gradient&nbsp;when&nbsp;the&nbsp;input&nbsp;is&nbsp;negative&nbsp;to&nbsp;address&nbsp;the&nbsp;dying&nbsp;ReLU&nbsp;problem.</span></dd></dl>

<dl><dt><a name="Activation-leaky_relu_derivative"><strong>leaky_relu_derivative</strong></a>(z, alpha=0.01)</dt><dd><span class="code">Derivative&nbsp;of&nbsp;the&nbsp;Leaky&nbsp;ReLU&nbsp;function:&nbsp;f'(z)&nbsp;=&nbsp;1&nbsp;if&nbsp;z&nbsp;&gt;&nbsp;0,&nbsp;else&nbsp;alpha<br>
Returns&nbsp;1&nbsp;for&nbsp;positive&nbsp;input,&nbsp;and&nbsp;alpha&nbsp;for&nbsp;negative&nbsp;input.</span></dd></dl>

<dl><dt><a name="Activation-relu"><strong>relu</strong></a>(z)</dt><dd><span class="code">ReLU&nbsp;(Rectified&nbsp;Linear&nbsp;Unit)&nbsp;activation&nbsp;function:&nbsp;f(z)&nbsp;=&nbsp;max(0,&nbsp;z)<br>
Returns&nbsp;the&nbsp;input&nbsp;directly&nbsp;if&nbsp;it's&nbsp;positive,&nbsp;otherwise&nbsp;returns&nbsp;0.</span></dd></dl>

<dl><dt><a name="Activation-relu_derivative"><strong>relu_derivative</strong></a>(z)</dt><dd><span class="code">Derivative&nbsp;of&nbsp;the&nbsp;ReLU&nbsp;function:&nbsp;f'(z)&nbsp;=&nbsp;1&nbsp;if&nbsp;z&nbsp;&gt;&nbsp;0,&nbsp;else&nbsp;0<br>
Returns&nbsp;1&nbsp;for&nbsp;positive&nbsp;input,&nbsp;and&nbsp;0&nbsp;for&nbsp;negative&nbsp;input.</span></dd></dl>

<dl><dt><a name="Activation-sigmoid"><strong>sigmoid</strong></a>(z)</dt><dd><span class="code">Sigmoid&nbsp;activation&nbsp;function:&nbsp;f(z)&nbsp;=&nbsp;1&nbsp;/&nbsp;(1&nbsp;+&nbsp;exp(-z))<br>
Maps&nbsp;input&nbsp;to&nbsp;the&nbsp;range&nbsp;[0,&nbsp;1],&nbsp;commonly&nbsp;used&nbsp;for&nbsp;binary&nbsp;classification.</span></dd></dl>

<dl><dt><a name="Activation-sigmoid_derivative"><strong>sigmoid_derivative</strong></a>(z)</dt><dd><span class="code">Derivative&nbsp;of&nbsp;the&nbsp;sigmoid&nbsp;function:&nbsp;f'(z)&nbsp;=&nbsp;<a href="#Activation-sigmoid">sigmoid</a>(z)&nbsp;*&nbsp;(1&nbsp;-&nbsp;<a href="#Activation-sigmoid">sigmoid</a>(z))<br>
Used&nbsp;for&nbsp;backpropagation&nbsp;through&nbsp;the&nbsp;sigmoid&nbsp;activation.</span></dd></dl>

<dl><dt><a name="Activation-softmax"><strong>softmax</strong></a>(z)</dt><dd><span class="code">Softmax&nbsp;activation&nbsp;function:&nbsp;f(z)_i&nbsp;=&nbsp;exp(z_i)&nbsp;/&nbsp;sum(exp(z_j))&nbsp;for&nbsp;all&nbsp;j<br>
Maps&nbsp;input&nbsp;into&nbsp;a&nbsp;probability&nbsp;distribution&nbsp;over&nbsp;multiple&nbsp;classes.&nbsp;Used&nbsp;for&nbsp;multiclass&nbsp;classification.</span></dd></dl>

<dl><dt><a name="Activation-tanh"><strong>tanh</strong></a>(z)</dt><dd><span class="code">Hyperbolic&nbsp;tangent&nbsp;(tanh)&nbsp;activation&nbsp;function:&nbsp;f(z)&nbsp;=&nbsp;(exp(z)&nbsp;-&nbsp;exp(-z))&nbsp;/&nbsp;(exp(z)&nbsp;+&nbsp;exp(-z))<br>
Maps&nbsp;input&nbsp;to&nbsp;the&nbsp;range&nbsp;[-1,&nbsp;1],&nbsp;typically&nbsp;used&nbsp;for&nbsp;normalized&nbsp;input.</span></dd></dl>

<dl><dt><a name="Activation-tanh_derivative"><strong>tanh_derivative</strong></a>(z)</dt><dd><span class="code">Derivative&nbsp;of&nbsp;the&nbsp;tanh&nbsp;function:&nbsp;f'(z)&nbsp;=&nbsp;1&nbsp;-&nbsp;<a href="#Activation-tanh">tanh</a>(z)^2<br>
Used&nbsp;for&nbsp;backpropagation&nbsp;through&nbsp;the&nbsp;tanh&nbsp;activation.</span></dd></dl>

<hr>
Data descriptors defined here:<br>
<dl><dt><strong>__dict__</strong></dt>
<dd><span class="code">dictionary&nbsp;for&nbsp;instance&nbsp;variables</span></dd>
</dl>
<dl><dt><strong>__weakref__</strong></dt>
<dd><span class="code">list&nbsp;of&nbsp;weak&nbsp;references&nbsp;to&nbsp;the&nbsp;object</span></dd>
</dl>
</td></tr></table> <p>
<table class="section">
<tr class="decor title-decor heading-text">
<td class="section-title" colspan=3>&nbsp;<br><a name="AdadeltaOptimizer">class <strong>AdadeltaOptimizer</strong></a>(<a href="builtins.html#object">builtins.object</a>)</td></tr>
    
<tr><td class="decor title-decor" rowspan=2><span class="code">&nbsp;&nbsp;&nbsp;</span></td>
<td class="decor title-decor" colspan=2><span class="code"><a href="#AdadeltaOptimizer">AdadeltaOptimizer</a>(learning_rate=1.0,&nbsp;rho=0.95,&nbsp;epsilon=1e-06,&nbsp;reg_lambda=0.0)<br>
&nbsp;<br>
Adadelta&nbsp;optimizer&nbsp;class&nbsp;for&nbsp;training&nbsp;neural&nbsp;networks.<br>
Formula:&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;E[g^2]_t&nbsp;=&nbsp;rho&nbsp;*&nbsp;E[g^2]_{t-1}&nbsp;+&nbsp;(1&nbsp;-&nbsp;rho)&nbsp;*&nbsp;g^2<br>
&nbsp;&nbsp;&nbsp;&nbsp;Delta_x&nbsp;=&nbsp;-&nbsp;(sqrt(E[delta_x^2]_{t-1}&nbsp;+&nbsp;epsilon)&nbsp;/&nbsp;sqrt(E[g^2]_t&nbsp;+&nbsp;epsilon))&nbsp;*&nbsp;g<br>
&nbsp;&nbsp;&nbsp;&nbsp;E[delta_x^2]_t&nbsp;=&nbsp;rho&nbsp;*&nbsp;E[delta_x^2]_{t-1}&nbsp;+&nbsp;(1&nbsp;-&nbsp;rho)&nbsp;*&nbsp;Delta_x^2<br>
Derived&nbsp;from:&nbsp;<a href="https://arxiv.org/abs/1212.5701">https://arxiv.org/abs/1212.5701</a><br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;learning_rate&nbsp;(float,&nbsp;optional):&nbsp;The&nbsp;learning&nbsp;rate&nbsp;for&nbsp;the&nbsp;optimizer.&nbsp;Defaults&nbsp;to&nbsp;1.0.<br>
&nbsp;&nbsp;&nbsp;&nbsp;rho&nbsp;(float,&nbsp;optional):&nbsp;The&nbsp;decay&nbsp;rate.&nbsp;Defaults&nbsp;to&nbsp;0.95.<br>
&nbsp;&nbsp;&nbsp;&nbsp;epsilon&nbsp;(float,&nbsp;optional):&nbsp;A&nbsp;small&nbsp;value&nbsp;to&nbsp;prevent&nbsp;division&nbsp;by&nbsp;zero.&nbsp;Defaults&nbsp;to&nbsp;1e-6.<br>
&nbsp;&nbsp;&nbsp;&nbsp;reg_lambda&nbsp;(float,&nbsp;optional):&nbsp;The&nbsp;regularization&nbsp;parameter.&nbsp;Defaults&nbsp;to&nbsp;0.0.<br>&nbsp;</span></td></tr>
<tr><td>&nbsp;</td>
<td class="singlecolumn">Methods defined here:<br>
<dl><dt><a name="AdadeltaOptimizer-__init__"><strong>__init__</strong></a>(self, learning_rate=1.0, rho=0.95, epsilon=1e-06, reg_lambda=0.0)</dt><dd><span class="code">Initialize&nbsp;self.&nbsp;&nbsp;See&nbsp;help(type(self))&nbsp;for&nbsp;accurate&nbsp;signature.</span></dd></dl>

<dl><dt><a name="AdadeltaOptimizer-initialize"><strong>initialize</strong></a>(self, layers)</dt><dd><span class="code">Initializes&nbsp;the&nbsp;running&nbsp;averages&nbsp;for&nbsp;each&nbsp;layer's&nbsp;weights.<br>
Args:&nbsp;layers&nbsp;(list):&nbsp;List&nbsp;of&nbsp;layers&nbsp;in&nbsp;the&nbsp;neural&nbsp;network.<br>
Returns:&nbsp;None</span></dd></dl>

<dl><dt><a name="AdadeltaOptimizer-update"><strong>update</strong></a>(self, layer, dW, db, index)</dt><dd><span class="code">Updates&nbsp;the&nbsp;weights&nbsp;and&nbsp;biases&nbsp;of&nbsp;a&nbsp;layer&nbsp;using&nbsp;the&nbsp;Adadelta&nbsp;optimization&nbsp;algorithm.<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;layer&nbsp;(Layer):&nbsp;The&nbsp;layer&nbsp;to&nbsp;update.<br>
&nbsp;&nbsp;&nbsp;&nbsp;dW&nbsp;(ndarray):&nbsp;The&nbsp;gradient&nbsp;of&nbsp;the&nbsp;weights.<br>
&nbsp;&nbsp;&nbsp;&nbsp;db&nbsp;(ndarray):&nbsp;The&nbsp;gradient&nbsp;of&nbsp;the&nbsp;biases.<br>
&nbsp;&nbsp;&nbsp;&nbsp;index&nbsp;(int):&nbsp;The&nbsp;index&nbsp;of&nbsp;the&nbsp;layer.<br>
Returns:&nbsp;None</span></dd></dl>

<hr>
Data descriptors defined here:<br>
<dl><dt><strong>__dict__</strong></dt>
<dd><span class="code">dictionary&nbsp;for&nbsp;instance&nbsp;variables</span></dd>
</dl>
<dl><dt><strong>__weakref__</strong></dt>
<dd><span class="code">list&nbsp;of&nbsp;weak&nbsp;references&nbsp;to&nbsp;the&nbsp;object</span></dd>
</dl>
</td></tr></table> <p>
<table class="section">
<tr class="decor title-decor heading-text">
<td class="section-title" colspan=3>&nbsp;<br><a name="AdamOptimizer">class <strong>AdamOptimizer</strong></a>(<a href="builtins.html#object">builtins.object</a>)</td></tr>
    
<tr><td class="decor title-decor" rowspan=2><span class="code">&nbsp;&nbsp;&nbsp;</span></td>
<td class="decor title-decor" colspan=2><span class="code"><a href="#AdamOptimizer">AdamOptimizer</a>(learning_rate=0.001,&nbsp;beta1=0.9,&nbsp;beta2=0.999,&nbsp;epsilon=1e-08,&nbsp;reg_lambda=0.01)<br>
&nbsp;<br>
Adam&nbsp;optimizer&nbsp;class&nbsp;for&nbsp;training&nbsp;neural&nbsp;networks.<br>
Formula:&nbsp;w&nbsp;=&nbsp;w&nbsp;-&nbsp;alpha&nbsp;*&nbsp;m_hat&nbsp;/&nbsp;(sqrt(v_hat)&nbsp;+&nbsp;epsilon)&nbsp;-&nbsp;lambda&nbsp;*&nbsp;w&nbsp;<br>
Derived&nbsp;from:&nbsp;<a href="https://arxiv.org/abs/1412.6980">https://arxiv.org/abs/1412.6980</a><br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;learning_rate&nbsp;(float,&nbsp;optional):&nbsp;The&nbsp;learning&nbsp;rate&nbsp;for&nbsp;the&nbsp;optimizer.&nbsp;Defaults&nbsp;to&nbsp;0.001.<br>
&nbsp;&nbsp;&nbsp;&nbsp;beta1&nbsp;(float,&nbsp;optional):&nbsp;The&nbsp;exponential&nbsp;decay&nbsp;rate&nbsp;for&nbsp;the&nbsp;first&nbsp;moment&nbsp;estimates.&nbsp;Defaults&nbsp;to&nbsp;0.9.<br>
&nbsp;&nbsp;&nbsp;&nbsp;beta2&nbsp;(float,&nbsp;optional):&nbsp;The&nbsp;exponential&nbsp;decay&nbsp;rate&nbsp;for&nbsp;the&nbsp;second&nbsp;moment&nbsp;estimates.&nbsp;Defaults&nbsp;to&nbsp;0.999.<br>
&nbsp;&nbsp;&nbsp;&nbsp;epsilon&nbsp;(float,&nbsp;optional):&nbsp;A&nbsp;small&nbsp;value&nbsp;to&nbsp;prevent&nbsp;division&nbsp;by&nbsp;zero.&nbsp;Defaults&nbsp;to&nbsp;1e-8.<br>
&nbsp;&nbsp;&nbsp;&nbsp;reg_lambda&nbsp;(float,&nbsp;optional):&nbsp;The&nbsp;regularization&nbsp;parameter.&nbsp;Defaults&nbsp;to&nbsp;0.01.<br>&nbsp;</span></td></tr>
<tr><td>&nbsp;</td>
<td class="singlecolumn">Methods defined here:<br>
<dl><dt><a name="AdamOptimizer-__init__"><strong>__init__</strong></a>(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, reg_lambda=0.01)</dt><dd><span class="code">Initialize&nbsp;self.&nbsp;&nbsp;See&nbsp;help(type(self))&nbsp;for&nbsp;accurate&nbsp;signature.</span></dd></dl>

<dl><dt><a name="AdamOptimizer-initialize"><strong>initialize</strong></a>(self, layers)</dt><dd><span class="code">Initializes&nbsp;the&nbsp;first&nbsp;and&nbsp;second&nbsp;moment&nbsp;estimates&nbsp;for&nbsp;each&nbsp;layer's&nbsp;weights.<br>
Args:&nbsp;layers&nbsp;(list):&nbsp;List&nbsp;of&nbsp;layers&nbsp;in&nbsp;the&nbsp;neural&nbsp;network.<br>
Returns:&nbsp;None</span></dd></dl>

<dl><dt><a name="AdamOptimizer-update"><strong>update</strong></a>(self, layer, dW, db, index)</dt><dd><span class="code">Updates&nbsp;the&nbsp;weights&nbsp;and&nbsp;biases&nbsp;of&nbsp;a&nbsp;layer&nbsp;using&nbsp;the&nbsp;Adam&nbsp;optimization&nbsp;algorithm.<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;layer&nbsp;(Layer):&nbsp;The&nbsp;layer&nbsp;to&nbsp;update.<br>
&nbsp;&nbsp;&nbsp;&nbsp;dW&nbsp;(ndarray):&nbsp;The&nbsp;gradient&nbsp;of&nbsp;the&nbsp;weights.<br>
&nbsp;&nbsp;&nbsp;&nbsp;db&nbsp;(ndarray):&nbsp;The&nbsp;gradient&nbsp;of&nbsp;the&nbsp;biases.<br>
&nbsp;&nbsp;&nbsp;&nbsp;index&nbsp;(int):&nbsp;The&nbsp;index&nbsp;of&nbsp;the&nbsp;layer.<br>
Returns:&nbsp;None</span></dd></dl>

<hr>
Data descriptors defined here:<br>
<dl><dt><strong>__dict__</strong></dt>
<dd><span class="code">dictionary&nbsp;for&nbsp;instance&nbsp;variables</span></dd>
</dl>
<dl><dt><strong>__weakref__</strong></dt>
<dd><span class="code">list&nbsp;of&nbsp;weak&nbsp;references&nbsp;to&nbsp;the&nbsp;object</span></dd>
</dl>
</td></tr></table> <p>
<table class="section">
<tr class="decor title-decor heading-text">
<td class="section-title" colspan=3>&nbsp;<br><a name="BCEWithLogitsLoss">class <strong>BCEWithLogitsLoss</strong></a>(<a href="builtins.html#object">builtins.object</a>)</td></tr>
    
<tr><td class="decor title-decor" rowspan=2><span class="code">&nbsp;&nbsp;&nbsp;</span></td>
<td class="decor title-decor" colspan=2><span class="code">Custom&nbsp;binary&nbsp;cross&nbsp;entropy&nbsp;loss&nbsp;with&nbsp;logits&nbsp;implementation&nbsp;using&nbsp;numpy.<br>
Formula:&nbsp;-mean(y&nbsp;*&nbsp;log(p)&nbsp;+&nbsp;(1&nbsp;-&nbsp;y)&nbsp;*&nbsp;log(1&nbsp;-&nbsp;p))<br>
Methods:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#BCEWithLogitsLoss-__call__">__call__</a>(self,&nbsp;logits,&nbsp;targets):&nbsp;Calculate&nbsp;the&nbsp;binary&nbsp;cross&nbsp;entropy&nbsp;loss.<br>&nbsp;</span></td></tr>
<tr><td>&nbsp;</td>
<td class="singlecolumn">Methods defined here:<br>
<dl><dt><a name="BCEWithLogitsLoss-__call__"><strong>__call__</strong></a>(self, logits, targets)</dt><dd><span class="code">Calculate&nbsp;the&nbsp;binary&nbsp;cross&nbsp;entropy&nbsp;loss.<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;logits&nbsp;(np.ndarray):&nbsp;The&nbsp;logits&nbsp;(predicted&nbsp;values)&nbsp;of&nbsp;shape&nbsp;(num_samples,).<br>
&nbsp;&nbsp;&nbsp;&nbsp;targets&nbsp;(np.ndarray):&nbsp;The&nbsp;target&nbsp;labels&nbsp;of&nbsp;shape&nbsp;(num_samples,).<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;float:&nbsp;The&nbsp;binary&nbsp;cross&nbsp;entropy&nbsp;loss.</span></dd></dl>

<hr>
Data descriptors defined here:<br>
<dl><dt><strong>__dict__</strong></dt>
<dd><span class="code">dictionary&nbsp;for&nbsp;instance&nbsp;variables</span></dd>
</dl>
<dl><dt><strong>__weakref__</strong></dt>
<dd><span class="code">list&nbsp;of&nbsp;weak&nbsp;references&nbsp;to&nbsp;the&nbsp;object</span></dd>
</dl>
</td></tr></table> <p>
<table class="section">
<tr class="decor title-decor heading-text">
<td class="section-title" colspan=3>&nbsp;<br><a name="BaseBackendNeuralNetwork">class <strong>BaseBackendNeuralNetwork</strong></a>(<a href="sega_learn.neural_networks.neuralNetworkBase.html#NeuralNetworkBase">sega_learn.neural_networks.neuralNetworkBase.NeuralNetworkBase</a>)</td></tr>
    
<tr><td class="decor title-decor" rowspan=2><span class="code">&nbsp;&nbsp;&nbsp;</span></td>
<td class="decor title-decor" colspan=2><span class="code"><a href="#BaseBackendNeuralNetwork">BaseBackendNeuralNetwork</a>(layers,&nbsp;dropout_rate=0.2,&nbsp;reg_lambda=0.01,&nbsp;activations=None)<br>
&nbsp;<br>
<br>&nbsp;</span></td></tr>
<tr><td>&nbsp;</td>
<td class="singlecolumn"><dl><dt>Method resolution order:</dt>
<dd><a href="sega_learn.neural_networks.neuralNetworkBaseBackend.html#BaseBackendNeuralNetwork">BaseBackendNeuralNetwork</a></dd>
<dd><a href="sega_learn.neural_networks.neuralNetworkBase.html#NeuralNetworkBase">sega_learn.neural_networks.neuralNetworkBase.NeuralNetworkBase</a></dd>
<dd><a href="builtins.html#object">builtins.object</a></dd>
</dl>
<hr>
Methods defined here:<br>
<dl><dt><a name="BaseBackendNeuralNetwork-__init__"><strong>__init__</strong></a>(self, layers, dropout_rate=0.2, reg_lambda=0.01, activations=None)</dt><dd><span class="code">Initializes&nbsp;the&nbsp;Numba&nbsp;backend&nbsp;neural&nbsp;network.<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;layers&nbsp;(list):&nbsp;List&nbsp;of&nbsp;layer&nbsp;sizes&nbsp;or&nbsp;Layer&nbsp;objects.<br>
&nbsp;&nbsp;&nbsp;&nbsp;dropout_rate&nbsp;(float):&nbsp;Dropout&nbsp;rate&nbsp;for&nbsp;regularization.<br>
&nbsp;&nbsp;&nbsp;&nbsp;reg_lambda&nbsp;(float):&nbsp;L2&nbsp;regularization&nbsp;parameter.<br>
&nbsp;&nbsp;&nbsp;&nbsp;activations&nbsp;(list):&nbsp;List&nbsp;of&nbsp;activation&nbsp;functions&nbsp;for&nbsp;each&nbsp;layer.</span></dd></dl>

<dl><dt><a name="BaseBackendNeuralNetwork-backward"><strong>backward</strong></a>(self, y)</dt><dd><span class="code">Performs&nbsp;backward&nbsp;propagation&nbsp;to&nbsp;calculate&nbsp;the&nbsp;gradients.<br>
Parameters:&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;y&nbsp;(ndarray):&nbsp;Target&nbsp;labels&nbsp;of&nbsp;shape&nbsp;(m,&nbsp;output_size).</span></dd></dl>

<dl><dt><a name="BaseBackendNeuralNetwork-calculate_loss"><strong>calculate_loss</strong></a>(self, X, y)</dt><dd><span class="code">Calculates&nbsp;the&nbsp;loss&nbsp;with&nbsp;L2&nbsp;regularization.<br>
Parameters:<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;X&nbsp;(ndarray):&nbsp;Input&nbsp;data<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;y&nbsp;(ndarray):&nbsp;Target&nbsp;labels<br>
Returns:&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;float:&nbsp;The&nbsp;calculated&nbsp;loss&nbsp;value</span></dd></dl>

<dl><dt><a name="BaseBackendNeuralNetwork-evaluate"><strong>evaluate</strong></a>(self, X, y)</dt></dl>

<dl><dt><a name="BaseBackendNeuralNetwork-forward"><strong>forward</strong></a>(self, X, training=True)</dt><dd><span class="code">Performs&nbsp;forward&nbsp;propagation&nbsp;through&nbsp;the&nbsp;neural&nbsp;network.<br>
Args:&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;X&nbsp;(ndarray):&nbsp;Input&nbsp;data&nbsp;of&nbsp;shape&nbsp;(batch_size,&nbsp;input_size).<br>
&nbsp;&nbsp;&nbsp;&nbsp;training&nbsp;(bool):&nbsp;Whether&nbsp;the&nbsp;network&nbsp;is&nbsp;in&nbsp;training&nbsp;mode&nbsp;(applies&nbsp;dropout).<br>
Returns:&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;ndarray:&nbsp;Output&nbsp;predictions&nbsp;of&nbsp;shape&nbsp;(batch_size,&nbsp;output_size).</span></dd></dl>

<dl><dt><a name="BaseBackendNeuralNetwork-initialize_new_layers"><strong>initialize_new_layers</strong></a>(self)</dt><dd><span class="code">Initializes&nbsp;the&nbsp;layers&nbsp;of&nbsp;the&nbsp;neural&nbsp;network.<br>
Each&nbsp;layer&nbsp;is&nbsp;created&nbsp;with&nbsp;the&nbsp;specified&nbsp;number&nbsp;of&nbsp;neurons&nbsp;and&nbsp;activation&nbsp;function.</span></dd></dl>

<dl><dt><a name="BaseBackendNeuralNetwork-predict"><strong>predict</strong></a>(self, X)</dt></dl>

<dl><dt><a name="BaseBackendNeuralNetwork-train"><strong>train</strong></a>(self, X_train, y_train, X_val=None, y_val=None, optimizer=None, epochs=100, batch_size=32, early_stopping_threshold=10, lr_scheduler=None, p=True, use_tqdm=True, n_jobs=1, track_metrics=False, track_adv_metrics=False)</dt><dd><span class="code">Trains&nbsp;the&nbsp;neural&nbsp;network&nbsp;model.<br>
Parameters:<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;X_train&nbsp;(ndarray):&nbsp;Training&nbsp;data&nbsp;features.<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;y_train&nbsp;(ndarray):&nbsp;Training&nbsp;data&nbsp;labels.<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;X_val&nbsp;(ndarray):&nbsp;Validation&nbsp;data&nbsp;features,&nbsp;optional.<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;y_val&nbsp;(ndarray):&nbsp;Validation&nbsp;data&nbsp;labels,&nbsp;optional.<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;optimizer&nbsp;(Optimizer):&nbsp;Optimizer&nbsp;for&nbsp;updating&nbsp;parameters&nbsp;(default:&nbsp;Adam,&nbsp;lr=0.0001).<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;epochs&nbsp;(int):&nbsp;Number&nbsp;of&nbsp;training&nbsp;epochs&nbsp;(default:&nbsp;100).<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;batch_size&nbsp;(int):&nbsp;Batch&nbsp;size&nbsp;for&nbsp;mini-batch&nbsp;gradient&nbsp;descent&nbsp;(default:&nbsp;32).<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;early_stopping_threshold&nbsp;(int):&nbsp;Patience&nbsp;for&nbsp;early&nbsp;stopping&nbsp;(default:&nbsp;10).<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;lr_scheduler&nbsp;(Scheduler):&nbsp;Learning&nbsp;rate&nbsp;scheduler&nbsp;(default:&nbsp;None).<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;p&nbsp;(bool):&nbsp;Whether&nbsp;to&nbsp;print&nbsp;training&nbsp;progress&nbsp;(default:&nbsp;True).<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;use_tqdm&nbsp;(bool):&nbsp;Whether&nbsp;to&nbsp;use&nbsp;tqdm&nbsp;for&nbsp;progress&nbsp;bar&nbsp;(default:&nbsp;True).<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;n_jobs&nbsp;(int):&nbsp;Number&nbsp;of&nbsp;jobs&nbsp;for&nbsp;parallel&nbsp;processing&nbsp;(default:&nbsp;1).<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;track_metrics&nbsp;(bool):&nbsp;Whether&nbsp;to&nbsp;track&nbsp;training&nbsp;metrics&nbsp;(default:&nbsp;False).<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;track_adv_metrics&nbsp;(bool):&nbsp;Whether&nbsp;to&nbsp;track&nbsp;advanced&nbsp;metrics&nbsp;(default:&nbsp;False).</span></dd></dl>

<dl><dt><a name="BaseBackendNeuralNetwork-tune_hyperparameters"><strong>tune_hyperparameters</strong></a>(self, X_train, y_train, X_val, y_val, param_grid, layer_configs=None, optimizer_types=None, lr_range=(0.0001, 0.01, 5), epochs=30, batch_size=32)</dt><dd><span class="code">Performs&nbsp;hyperparameter&nbsp;tuning&nbsp;using&nbsp;grid&nbsp;search.<br>
Parameters:<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;X_train,&nbsp;y_train:&nbsp;Training&nbsp;data<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;X_val,&nbsp;y_val:&nbsp;Validation&nbsp;data<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;param_grid:&nbsp;Dict&nbsp;of&nbsp;parameters&nbsp;to&nbsp;try<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;layer_configs:&nbsp;List&nbsp;of&nbsp;layer&nbsp;configurations<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;optimizer_types:&nbsp;List&nbsp;of&nbsp;optimizer&nbsp;types<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;lr_range:&nbsp;(min_lr,&nbsp;max_lr,&nbsp;num_steps)&nbsp;for&nbsp;learning&nbsp;rates<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;epochs:&nbsp;Max&nbsp;epochs&nbsp;for&nbsp;each&nbsp;trial<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;batch_size:&nbsp;Batch&nbsp;size&nbsp;for&nbsp;training<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;best_params:&nbsp;Best&nbsp;hyperparameters&nbsp;found<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;best_accuracy:&nbsp;Best&nbsp;validation&nbsp;accuracy</span></dd></dl>

<hr>
Methods inherited from <a href="sega_learn.neural_networks.neuralNetworkBase.html#NeuralNetworkBase">sega_learn.neural_networks.neuralNetworkBase.NeuralNetworkBase</a>:<br>
<dl><dt><a name="BaseBackendNeuralNetwork-apply_dropout"><strong>apply_dropout</strong></a>(self, X)</dt><dd><span class="code">Applies&nbsp;dropout&nbsp;to&nbsp;the&nbsp;activation&nbsp;X.<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;X&nbsp;(ndarray):&nbsp;<a href="#Activation">Activation</a>&nbsp;values.<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;ndarray:&nbsp;<a href="#Activation">Activation</a>&nbsp;values&nbsp;after&nbsp;applying&nbsp;dropout.</span></dd></dl>

<dl><dt><a name="BaseBackendNeuralNetwork-calculate_precision_recall_f1"><strong>calculate_precision_recall_f1</strong></a>(self, X, y)</dt><dd><span class="code">Calculates&nbsp;precision,&nbsp;recall,&nbsp;and&nbsp;F1&nbsp;score.<br>
Parameters:<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;X&nbsp;(ndarray):&nbsp;Input&nbsp;data<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;y&nbsp;(ndarray):&nbsp;Target&nbsp;labels<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;precision&nbsp;(float):&nbsp;Precision&nbsp;score<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;recall&nbsp;(float):&nbsp;Recall&nbsp;score<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;f1&nbsp;(float):&nbsp;F1&nbsp;score</span></dd></dl>

<dl><dt><a name="BaseBackendNeuralNetwork-compute_l2_reg"><strong>compute_l2_reg</strong></a>(self, weights)</dt><dd><span class="code">Computes&nbsp;the&nbsp;L2&nbsp;regularization&nbsp;term.<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;weights&nbsp;(list):&nbsp;List&nbsp;of&nbsp;weight&nbsp;matrices.<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;float:&nbsp;L2&nbsp;regularization&nbsp;term.</span></dd></dl>

<dl><dt><a name="BaseBackendNeuralNetwork-create_scheduler"><strong>create_scheduler</strong></a>(self, scheduler_type, optimizer, **kwargs)</dt><dd><span class="code">Creates&nbsp;a&nbsp;learning&nbsp;rate&nbsp;scheduler.</span></dd></dl>

<dl><dt><a name="BaseBackendNeuralNetwork-initialize_layers"><strong>initialize_layers</strong></a>(self)</dt></dl>

<dl><dt><a name="BaseBackendNeuralNetwork-plot_metrics"><strong>plot_metrics</strong></a>(self, save_dir=None)</dt><dd><span class="code">Plots&nbsp;the&nbsp;training&nbsp;and&nbsp;validation&nbsp;metrics.</span></dd></dl>

<hr>
Data descriptors inherited from <a href="sega_learn.neural_networks.neuralNetworkBase.html#NeuralNetworkBase">sega_learn.neural_networks.neuralNetworkBase.NeuralNetworkBase</a>:<br>
<dl><dt><strong>__dict__</strong></dt>
<dd><span class="code">dictionary&nbsp;for&nbsp;instance&nbsp;variables</span></dd>
</dl>
<dl><dt><strong>__weakref__</strong></dt>
<dd><span class="code">list&nbsp;of&nbsp;weak&nbsp;references&nbsp;to&nbsp;the&nbsp;object</span></dd>
</dl>
</td></tr></table> <p>
<table class="section">
<tr class="decor title-decor heading-text">
<td class="section-title" colspan=3>&nbsp;<br><a name="ConvLayer">class <strong>ConvLayer</strong></a>(<a href="builtins.html#object">builtins.object</a>)</td></tr>
    
<tr><td class="decor title-decor" rowspan=2><span class="code">&nbsp;&nbsp;&nbsp;</span></td>
<td class="decor title-decor" colspan=2><span class="code"><a href="#ConvLayer">ConvLayer</a>(in_channels,&nbsp;out_channels,&nbsp;kernel_size,&nbsp;stride=1,&nbsp;padding=0,&nbsp;activation='relu')<br>
&nbsp;<br>
A&nbsp;convolutional&nbsp;layer&nbsp;implementation&nbsp;for&nbsp;neural&nbsp;networks.&nbsp;&nbsp;<br>
&nbsp;<br>
This&nbsp;layer&nbsp;performs&nbsp;2D&nbsp;convolution&nbsp;operations,&nbsp;commonly&nbsp;used&nbsp;in&nbsp;convolutional&nbsp;neural&nbsp;networks&nbsp;(CNNs).<br>
The&nbsp;implementation&nbsp;uses&nbsp;the&nbsp;im2col&nbsp;technique&nbsp;for&nbsp;efficient&nbsp;computation,&nbsp;transforming&nbsp;the&nbsp;convolution&nbsp;operation&nbsp;into&nbsp;matrix&nbsp;multiplication.<br>
An&nbsp;optional&nbsp;activation&nbsp;function&nbsp;is&nbsp;applied&nbsp;element-wise&nbsp;to&nbsp;the&nbsp;output.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;in_channels&nbsp;(int):&nbsp;Number&nbsp;of&nbsp;input&nbsp;channels&nbsp;(depth&nbsp;of&nbsp;input&nbsp;volume).<br>
&nbsp;&nbsp;&nbsp;&nbsp;out_channels&nbsp;(int):&nbsp;Number&nbsp;of&nbsp;output&nbsp;channels&nbsp;(number&nbsp;of&nbsp;filters).<br>
&nbsp;&nbsp;&nbsp;&nbsp;kernel_size&nbsp;(int):&nbsp;Size&nbsp;of&nbsp;the&nbsp;convolutional&nbsp;kernel&nbsp;(square&nbsp;kernel&nbsp;assumed).<br>
&nbsp;&nbsp;&nbsp;&nbsp;stride&nbsp;(int,&nbsp;optional):&nbsp;Stride&nbsp;of&nbsp;the&nbsp;convolution.&nbsp;Default:&nbsp;1.<br>
&nbsp;&nbsp;&nbsp;&nbsp;padding&nbsp;(int,&nbsp;optional):&nbsp;Zero-padding&nbsp;added&nbsp;to&nbsp;both&nbsp;sides&nbsp;of&nbsp;the&nbsp;input.&nbsp;Default:&nbsp;0.<br>
&nbsp;&nbsp;&nbsp;&nbsp;activation&nbsp;(str,&nbsp;optional):&nbsp;<a href="#Activation">Activation</a>&nbsp;function&nbsp;to&nbsp;use.&nbsp;Options&nbsp;are&nbsp;"relu",&nbsp;"sigmoid",&nbsp;"tanh",&nbsp;or&nbsp;None.&nbsp;Default:&nbsp;"relu".<br>
&nbsp;<br>
Attributes:<br>
&nbsp;&nbsp;&nbsp;&nbsp;in_channels&nbsp;(int):&nbsp;Number&nbsp;of&nbsp;input&nbsp;channels.<br>
&nbsp;&nbsp;&nbsp;&nbsp;out_channels&nbsp;(int):&nbsp;Number&nbsp;of&nbsp;output&nbsp;channels.<br>
&nbsp;&nbsp;&nbsp;&nbsp;kernel_size&nbsp;(int):&nbsp;Size&nbsp;of&nbsp;the&nbsp;square&nbsp;convolutional&nbsp;kernel.<br>
&nbsp;&nbsp;&nbsp;&nbsp;stride&nbsp;(int):&nbsp;Stride&nbsp;of&nbsp;the&nbsp;convolution.<br>
&nbsp;&nbsp;&nbsp;&nbsp;padding&nbsp;(int):&nbsp;Zero-padding&nbsp;added&nbsp;to&nbsp;both&nbsp;sides&nbsp;of&nbsp;the&nbsp;input.<br>
&nbsp;&nbsp;&nbsp;&nbsp;weights&nbsp;(numpy.ndarray):&nbsp;Learnable&nbsp;weights&nbsp;of&nbsp;shape&nbsp;(out_channels,&nbsp;in_channels,&nbsp;kernel_size,&nbsp;kernel_size).<br>
&nbsp;&nbsp;&nbsp;&nbsp;biases&nbsp;(numpy.ndarray):&nbsp;Learnable&nbsp;biases&nbsp;of&nbsp;shape&nbsp;(out_channels,&nbsp;1).<br>
&nbsp;&nbsp;&nbsp;&nbsp;activation&nbsp;(str):&nbsp;Type&nbsp;of&nbsp;activation&nbsp;function.<br>
&nbsp;&nbsp;&nbsp;&nbsp;weight_gradients&nbsp;(numpy.ndarray):&nbsp;Gradients&nbsp;with&nbsp;respect&nbsp;to&nbsp;weights.<br>
&nbsp;&nbsp;&nbsp;&nbsp;bias_gradients&nbsp;(numpy.ndarray):&nbsp;Gradients&nbsp;with&nbsp;respect&nbsp;to&nbsp;biases.<br>
&nbsp;&nbsp;&nbsp;&nbsp;input_cache&nbsp;(numpy.ndarray):&nbsp;Cached&nbsp;input&nbsp;for&nbsp;use&nbsp;in&nbsp;backward&nbsp;pass.<br>
&nbsp;&nbsp;&nbsp;&nbsp;X_cols&nbsp;(numpy.ndarray):&nbsp;Cached&nbsp;column-transformed&nbsp;input.<br>
&nbsp;&nbsp;&nbsp;&nbsp;X_padded&nbsp;(numpy.ndarray):&nbsp;Cached&nbsp;padded&nbsp;input.<br>
&nbsp;&nbsp;&nbsp;&nbsp;h_out&nbsp;(int):&nbsp;Height&nbsp;of&nbsp;output&nbsp;feature&nbsp;maps.<br>
&nbsp;&nbsp;&nbsp;&nbsp;w_out&nbsp;(int):&nbsp;Width&nbsp;of&nbsp;output&nbsp;feature&nbsp;maps.<br>
&nbsp;&nbsp;&nbsp;&nbsp;input_size&nbsp;(int):&nbsp;Size&nbsp;of&nbsp;input&nbsp;(same&nbsp;as&nbsp;in_channels).<br>
&nbsp;&nbsp;&nbsp;&nbsp;output_size&nbsp;(int):&nbsp;Size&nbsp;of&nbsp;output&nbsp;(same&nbsp;as&nbsp;out_channels).<br>
&nbsp;<br>
Methods:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#ConvLayer-zero_grad">zero_grad</a>():&nbsp;Reset&nbsp;gradients&nbsp;to&nbsp;zero.<br>
&nbsp;&nbsp;&nbsp;&nbsp;_im2col(x,&nbsp;h_out,&nbsp;w_out):&nbsp;Convert&nbsp;image&nbsp;regions&nbsp;to&nbsp;columns&nbsp;for&nbsp;efficient&nbsp;convolution.<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#ConvLayer-forward">forward</a>(X):&nbsp;Perform&nbsp;forward&nbsp;pass&nbsp;of&nbsp;the&nbsp;convolutional&nbsp;layer.<br>
&nbsp;&nbsp;&nbsp;&nbsp;_col2im(dcol,&nbsp;x_shape):&nbsp;Convert&nbsp;column&nbsp;back&nbsp;to&nbsp;image&nbsp;format&nbsp;for&nbsp;the&nbsp;backward&nbsp;pass.<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#ConvLayer-backward">backward</a>(d_out,&nbsp;reg_lambda=0):&nbsp;Perform&nbsp;backward&nbsp;pass&nbsp;of&nbsp;the&nbsp;convolutional&nbsp;layer.<br>&nbsp;</span></td></tr>
<tr><td>&nbsp;</td>
<td class="singlecolumn">Methods defined here:<br>
<dl><dt><a name="ConvLayer-__init__"><strong>__init__</strong></a>(self, in_channels, out_channels, kernel_size, stride=1, padding=0, activation='relu')</dt><dd><span class="code">Initialize&nbsp;self.&nbsp;&nbsp;See&nbsp;help(type(self))&nbsp;for&nbsp;accurate&nbsp;signature.</span></dd></dl>

<dl><dt><a name="ConvLayer-activate"><strong>activate</strong></a>(self, Z)</dt><dd><span class="code">Apply&nbsp;activation&nbsp;function.</span></dd></dl>

<dl><dt><a name="ConvLayer-backward"><strong>backward</strong></a>(self, d_out, reg_lambda=0)</dt><dd><span class="code">Optimized&nbsp;backward&nbsp;pass&nbsp;using&nbsp;im2col&nbsp;technique.<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;d_out&nbsp;(np.ndarray):&nbsp;Gradient&nbsp;of&nbsp;the&nbsp;loss&nbsp;with&nbsp;respect&nbsp;to&nbsp;the&nbsp;layer&nbsp;output,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;shape&nbsp;(batch_size,&nbsp;out_channels,&nbsp;h_out,&nbsp;w_out)<br>
&nbsp;&nbsp;&nbsp;&nbsp;reg_lambda&nbsp;(float,&nbsp;optional):&nbsp;Regularization&nbsp;parameter.<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;dX:&nbsp;Gradient&nbsp;with&nbsp;respect&nbsp;to&nbsp;the&nbsp;input&nbsp;X.</span></dd></dl>

<dl><dt><a name="ConvLayer-forward"><strong>forward</strong></a>(self, X)</dt><dd><span class="code">X:&nbsp;numpy&nbsp;array&nbsp;with&nbsp;shape&nbsp;(batch_size,&nbsp;in_channels,&nbsp;height,&nbsp;width)<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;Output&nbsp;feature&nbsp;maps&nbsp;after&nbsp;convolution&nbsp;and&nbsp;activation.</span></dd></dl>

<dl><dt><a name="ConvLayer-zero_grad"><strong>zero_grad</strong></a>(self)</dt><dd><span class="code">Reset&nbsp;the&nbsp;gradients&nbsp;of&nbsp;the&nbsp;weights&nbsp;and&nbsp;biases&nbsp;to&nbsp;zero.</span></dd></dl>

<hr>
Data descriptors defined here:<br>
<dl><dt><strong>__dict__</strong></dt>
<dd><span class="code">dictionary&nbsp;for&nbsp;instance&nbsp;variables</span></dd>
</dl>
<dl><dt><strong>__weakref__</strong></dt>
<dd><span class="code">list&nbsp;of&nbsp;weak&nbsp;references&nbsp;to&nbsp;the&nbsp;object</span></dd>
</dl>
</td></tr></table> <p>
<table class="section">
<tr class="decor title-decor heading-text">
<td class="section-title" colspan=3>&nbsp;<br><a name="CrossEntropyLoss">class <strong>CrossEntropyLoss</strong></a>(<a href="builtins.html#object">builtins.object</a>)</td></tr>
    
<tr><td class="decor title-decor" rowspan=2><span class="code">&nbsp;&nbsp;&nbsp;</span></td>
<td class="decor title-decor" colspan=2><span class="code">Custom&nbsp;cross&nbsp;entropy&nbsp;loss&nbsp;implementation&nbsp;using&nbsp;numpy&nbsp;for&nbsp;multi-class&nbsp;classification.<br>
Formula:&nbsp;-sum(y&nbsp;*&nbsp;log(p)&nbsp;+&nbsp;(1&nbsp;-&nbsp;y)&nbsp;*&nbsp;log(1&nbsp;-&nbsp;p))&nbsp;/&nbsp;m<br>
Methods:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#CrossEntropyLoss-__call__">__call__</a>(self,&nbsp;logits,&nbsp;targets):&nbsp;Calculate&nbsp;the&nbsp;cross&nbsp;entropy&nbsp;loss.<br>&nbsp;</span></td></tr>
<tr><td>&nbsp;</td>
<td class="singlecolumn">Methods defined here:<br>
<dl><dt><a name="CrossEntropyLoss-__call__"><strong>__call__</strong></a>(self, logits, targets)</dt><dd><span class="code">Calculate&nbsp;the&nbsp;cross&nbsp;entropy&nbsp;loss.<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;logits&nbsp;(np.ndarray):&nbsp;The&nbsp;logits&nbsp;(predicted&nbsp;values)&nbsp;of&nbsp;shape&nbsp;(num_samples,&nbsp;num_classes).<br>
&nbsp;&nbsp;&nbsp;&nbsp;targets&nbsp;(np.ndarray):&nbsp;The&nbsp;target&nbsp;labels&nbsp;of&nbsp;shape&nbsp;(num_samples,).<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;float:&nbsp;The&nbsp;cross&nbsp;entropy&nbsp;loss.</span></dd></dl>

<hr>
Data descriptors defined here:<br>
<dl><dt><strong>__dict__</strong></dt>
<dd><span class="code">dictionary&nbsp;for&nbsp;instance&nbsp;variables</span></dd>
</dl>
<dl><dt><strong>__weakref__</strong></dt>
<dd><span class="code">list&nbsp;of&nbsp;weak&nbsp;references&nbsp;to&nbsp;the&nbsp;object</span></dd>
</dl>
</td></tr></table> <p>
<table class="section">
<tr class="decor title-decor heading-text">
<td class="section-title" colspan=3>&nbsp;<br><a name="DenseLayer">class <strong>DenseLayer</strong></a>(<a href="builtins.html#object">builtins.object</a>)</td></tr>
    
<tr><td class="decor title-decor" rowspan=2><span class="code">&nbsp;&nbsp;&nbsp;</span></td>
<td class="decor title-decor" colspan=2><span class="code"><a href="#DenseLayer">DenseLayer</a>(input_size,&nbsp;output_size,&nbsp;activation='relu')<br>
&nbsp;<br>
Initializes&nbsp;a&nbsp;fully&nbsp;connected&nbsp;layer&nbsp;<a href="builtins.html#object">object</a>,&nbsp;where&nbsp;each&nbsp;neuron&nbsp;is&nbsp;connected&nbsp;to&nbsp;all&nbsp;neurons&nbsp;in&nbsp;the&nbsp;previous&nbsp;layer.<br>
Each&nbsp;layer&nbsp;consists&nbsp;of&nbsp;weights,&nbsp;biases,&nbsp;and&nbsp;an&nbsp;activation&nbsp;function.<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;input_size&nbsp;(int):&nbsp;The&nbsp;size&nbsp;of&nbsp;the&nbsp;input&nbsp;to&nbsp;the&nbsp;layer.<br>
&nbsp;&nbsp;&nbsp;&nbsp;output_size&nbsp;(int):&nbsp;The&nbsp;size&nbsp;of&nbsp;the&nbsp;output&nbsp;from&nbsp;the&nbsp;layer.<br>
&nbsp;&nbsp;&nbsp;&nbsp;activation&nbsp;(str):&nbsp;The&nbsp;activation&nbsp;function&nbsp;to&nbsp;be&nbsp;used&nbsp;in&nbsp;the&nbsp;layer.<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
Attributes:<br>
&nbsp;&nbsp;&nbsp;&nbsp;weights&nbsp;(np.ndarray):&nbsp;Weights&nbsp;of&nbsp;the&nbsp;layer.<br>
&nbsp;&nbsp;&nbsp;&nbsp;biases&nbsp;(np.ndarray):&nbsp;Biases&nbsp;of&nbsp;the&nbsp;layer.<br>
&nbsp;&nbsp;&nbsp;&nbsp;activation&nbsp;(str):&nbsp;<a href="#Activation">Activation</a>&nbsp;function&nbsp;name.<br>
&nbsp;&nbsp;&nbsp;&nbsp;weight_gradients&nbsp;(np.ndarray):&nbsp;Gradients&nbsp;of&nbsp;the&nbsp;weights.<br>
&nbsp;&nbsp;&nbsp;&nbsp;bias_gradients&nbsp;(np.ndarray):&nbsp;Gradients&nbsp;of&nbsp;the&nbsp;biases.<br>
&nbsp;&nbsp;&nbsp;&nbsp;input_cache&nbsp;(np.ndarray):&nbsp;Cached&nbsp;input&nbsp;for&nbsp;backpropagation.<br>
&nbsp;&nbsp;&nbsp;&nbsp;output_cache&nbsp;(np.ndarray):&nbsp;Cached&nbsp;output&nbsp;for&nbsp;backpropagation.<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
Methods:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#DenseLayer-zero_grad">zero_grad</a>():&nbsp;Resets&nbsp;the&nbsp;gradients&nbsp;of&nbsp;the&nbsp;weights&nbsp;and&nbsp;biases&nbsp;to&nbsp;zero.<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#DenseLayer-forward">forward</a>(X):&nbsp;Performs&nbsp;the&nbsp;forward&nbsp;pass&nbsp;of&nbsp;the&nbsp;layer.<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#DenseLayer-backward">backward</a>(dA,&nbsp;reg_lambda):&nbsp;Performs&nbsp;the&nbsp;backward&nbsp;pass&nbsp;of&nbsp;the&nbsp;layer.<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#DenseLayer-activate">activate</a>(Z):&nbsp;Applies&nbsp;the&nbsp;activation&nbsp;function.<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#DenseLayer-activation_derivative">activation_derivative</a>(Z):&nbsp;Applies&nbsp;the&nbsp;derivative&nbsp;of&nbsp;the&nbsp;activation&nbsp;function.<br>&nbsp;</span></td></tr>
<tr><td>&nbsp;</td>
<td class="singlecolumn">Methods defined here:<br>
<dl><dt><a name="DenseLayer-__init__"><strong>__init__</strong></a>(self, input_size, output_size, activation='relu')</dt><dd><span class="code">Initialize&nbsp;self.&nbsp;&nbsp;See&nbsp;help(type(self))&nbsp;for&nbsp;accurate&nbsp;signature.</span></dd></dl>

<dl><dt><a name="DenseLayer-activate"><strong>activate</strong></a>(self, Z)</dt><dd><span class="code">Apply&nbsp;activation&nbsp;function.</span></dd></dl>

<dl><dt><a name="DenseLayer-activation_derivative"><strong>activation_derivative</strong></a>(self, Z)</dt><dd><span class="code">Apply&nbsp;activation&nbsp;derivative.</span></dd></dl>

<dl><dt><a name="DenseLayer-backward"><strong>backward</strong></a>(self, dA, reg_lambda)</dt></dl>

<dl><dt><a name="DenseLayer-forward"><strong>forward</strong></a>(self, X)</dt></dl>

<dl><dt><a name="DenseLayer-zero_grad"><strong>zero_grad</strong></a>(self)</dt><dd><span class="code">Reset&nbsp;the&nbsp;gradients&nbsp;of&nbsp;the&nbsp;weights&nbsp;and&nbsp;biases&nbsp;to&nbsp;zero.</span></dd></dl>

<hr>
Data descriptors defined here:<br>
<dl><dt><strong>__dict__</strong></dt>
<dd><span class="code">dictionary&nbsp;for&nbsp;instance&nbsp;variables</span></dd>
</dl>
<dl><dt><strong>__weakref__</strong></dt>
<dd><span class="code">list&nbsp;of&nbsp;weak&nbsp;references&nbsp;to&nbsp;the&nbsp;object</span></dd>
</dl>
</td></tr></table> <p>
<table class="section">
<tr class="decor title-decor heading-text">
<td class="section-title" colspan=3>&nbsp;<br><a name="FlattenLayer">class <strong>FlattenLayer</strong></a>(<a href="builtins.html#object">builtins.object</a>)</td></tr>
    
<tr><td class="decor title-decor" rowspan=2><span class="code">&nbsp;&nbsp;&nbsp;</span></td>
<td class="decor title-decor" colspan=2><span class="code">A&nbsp;layer&nbsp;that&nbsp;flattens&nbsp;multi-dimensional&nbsp;input&nbsp;into&nbsp;a&nbsp;2D&nbsp;array&nbsp;(batch_size,&nbsp;flattened_size).<br>
Useful&nbsp;for&nbsp;transitioning&nbsp;from&nbsp;convolutional&nbsp;layers&nbsp;to&nbsp;dense&nbsp;layers.<br>
&nbsp;<br>
Attributes:<br>
&nbsp;&nbsp;&nbsp;&nbsp;input_shape&nbsp;(tuple):&nbsp;Shape&nbsp;of&nbsp;the&nbsp;input&nbsp;data&nbsp;(excluding&nbsp;batch&nbsp;size).<br>
&nbsp;&nbsp;&nbsp;&nbsp;output_size&nbsp;(int):&nbsp;Size&nbsp;of&nbsp;the&nbsp;flattened&nbsp;output&nbsp;vector.<br>
&nbsp;&nbsp;&nbsp;&nbsp;input_cache&nbsp;(np.ndarray):&nbsp;Cached&nbsp;input&nbsp;for&nbsp;backpropagation.<br>
&nbsp;&nbsp;&nbsp;&nbsp;input_size&nbsp;(int):&nbsp;Size&nbsp;of&nbsp;the&nbsp;input&nbsp;(same&nbsp;as&nbsp;input_shape).<br>
&nbsp;&nbsp;&nbsp;&nbsp;output_size&nbsp;(int):&nbsp;Size&nbsp;of&nbsp;the&nbsp;output&nbsp;(same&nbsp;as&nbsp;output_size).<br>&nbsp;</span></td></tr>
<tr><td>&nbsp;</td>
<td class="singlecolumn">Methods defined here:<br>
<dl><dt><a name="FlattenLayer-__init__"><strong>__init__</strong></a>(self)</dt><dd><span class="code">Initialize&nbsp;self.&nbsp;&nbsp;See&nbsp;help(type(self))&nbsp;for&nbsp;accurate&nbsp;signature.</span></dd></dl>

<dl><dt><a name="FlattenLayer-backward"><strong>backward</strong></a>(self, dA, reg_lambda=0)</dt><dd><span class="code">Reshapes&nbsp;the&nbsp;gradient&nbsp;back&nbsp;to&nbsp;the&nbsp;original&nbsp;input&nbsp;shape.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;dA&nbsp;(np.ndarray):&nbsp;Gradient&nbsp;of&nbsp;the&nbsp;loss&nbsp;with&nbsp;respect&nbsp;to&nbsp;the&nbsp;layer's&nbsp;output,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;shape&nbsp;(batch_size,&nbsp;flattened_size)<br>
&nbsp;&nbsp;&nbsp;&nbsp;reg_lambda&nbsp;(float):&nbsp;Regularization&nbsp;parameter&nbsp;(unused&nbsp;in&nbsp;<a href="#FlattenLayer">FlattenLayer</a>).<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;np.ndarray:&nbsp;Gradient&nbsp;with&nbsp;respect&nbsp;to&nbsp;the&nbsp;input,&nbsp;reshaped&nbsp;to&nbsp;original&nbsp;input&nbsp;shape.</span></dd></dl>

<dl><dt><a name="FlattenLayer-forward"><strong>forward</strong></a>(self, X)</dt><dd><span class="code">Flattens&nbsp;the&nbsp;input&nbsp;tensor.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;X&nbsp;(np.ndarray):&nbsp;Input&nbsp;data&nbsp;of&nbsp;shape&nbsp;(batch_size,&nbsp;channels,&nbsp;height,&nbsp;width)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;or&nbsp;any&nbsp;multi-dimensional&nbsp;shape&nbsp;after&nbsp;batch&nbsp;dimension.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;np.ndarray:&nbsp;Flattened&nbsp;output&nbsp;of&nbsp;shape&nbsp;(batch_size,&nbsp;flattened_size)</span></dd></dl>

<hr>
Data descriptors defined here:<br>
<dl><dt><strong>__dict__</strong></dt>
<dd><span class="code">dictionary&nbsp;for&nbsp;instance&nbsp;variables</span></dd>
</dl>
<dl><dt><strong>__weakref__</strong></dt>
<dd><span class="code">list&nbsp;of&nbsp;weak&nbsp;references&nbsp;to&nbsp;the&nbsp;object</span></dd>
</dl>
</td></tr></table> <p>
<table class="section">
<tr class="decor title-decor heading-text">
<td class="section-title" colspan=3>&nbsp;<br><a name="JITAdadeltaOptimizer">class <strong>JITAdadeltaOptimizer</strong></a>(JITAdadeltaOptimizer)</td></tr>
    
<tr><td class="decor title-decor" rowspan=2><span class="code">&nbsp;&nbsp;&nbsp;</span></td>
<td class="decor title-decor" colspan=2><span class="code"><a href="#JITAdadeltaOptimizer">JITAdadeltaOptimizer</a>(*args,&nbsp;**kwargs)<br>
&nbsp;<br>
Adadelta&nbsp;optimizer&nbsp;class&nbsp;for&nbsp;training&nbsp;neural&nbsp;networks.<br>
Formula:&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;E[g^2]_t&nbsp;=&nbsp;rho&nbsp;*&nbsp;E[g^2]_{t-1}&nbsp;+&nbsp;(1&nbsp;-&nbsp;rho)&nbsp;*&nbsp;g^2<br>
&nbsp;&nbsp;&nbsp;&nbsp;Delta_x&nbsp;=&nbsp;-&nbsp;(sqrt(E[delta_x^2]_{t-1}&nbsp;+&nbsp;epsilon)&nbsp;/&nbsp;sqrt(E[g^2]_t&nbsp;+&nbsp;epsilon))&nbsp;*&nbsp;g<br>
&nbsp;&nbsp;&nbsp;&nbsp;E[delta_x^2]_t&nbsp;=&nbsp;rho&nbsp;*&nbsp;E[delta_x^2]_{t-1}&nbsp;+&nbsp;(1&nbsp;-&nbsp;rho)&nbsp;*&nbsp;Delta_x^2<br>
Derived&nbsp;from:&nbsp;<a href="https://arxiv.org/abs/1212.5701">https://arxiv.org/abs/1212.5701</a><br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;learning_rate&nbsp;(float,&nbsp;optional):&nbsp;The&nbsp;learning&nbsp;rate&nbsp;for&nbsp;the&nbsp;optimizer.&nbsp;Defaults&nbsp;to&nbsp;1.0.<br>
&nbsp;&nbsp;&nbsp;&nbsp;rho&nbsp;(float,&nbsp;optional):&nbsp;The&nbsp;decay&nbsp;rate.&nbsp;Defaults&nbsp;to&nbsp;0.95.<br>
&nbsp;&nbsp;&nbsp;&nbsp;epsilon&nbsp;(float,&nbsp;optional):&nbsp;A&nbsp;small&nbsp;value&nbsp;to&nbsp;prevent&nbsp;division&nbsp;by&nbsp;zero.&nbsp;Defaults&nbsp;to&nbsp;1e-6.<br>
&nbsp;&nbsp;&nbsp;&nbsp;reg_lambda&nbsp;(float,&nbsp;optional):&nbsp;The&nbsp;regularization&nbsp;parameter.&nbsp;Defaults&nbsp;to&nbsp;0.0.<br>&nbsp;</span></td></tr>
<tr><td>&nbsp;</td>
<td class="singlecolumn"><dl><dt>Method resolution order:</dt>
<dd><a href="sega_learn.neural_networks.optimizers_jit.html#JITAdadeltaOptimizer">JITAdadeltaOptimizer</a></dd>
<dd>JITAdadeltaOptimizer</dd>
<dd><a href="builtins.html#object">builtins.object</a></dd>
</dl>
<hr>
Data and other attributes defined here:<br>
<dl><dt><strong>class_type</strong> = jitclass.JITAdadeltaOptimizer#2df18038a50&lt;learni...float64, 3d, C),E_delta_x2:array(float64, 3d, C)&gt;</dl>

<hr>
Methods inherited from JITAdadeltaOptimizer:<br>
<dl><dt><a name="JITAdadeltaOptimizer-__init__"><strong>__init__</strong></a>(self, learning_rate=1.0, rho=0.95, epsilon=1e-06, reg_lambda=0.0)</dt><dd><span class="code">Initialize&nbsp;self.&nbsp;&nbsp;See&nbsp;help(type(self))&nbsp;for&nbsp;accurate&nbsp;signature.</span></dd></dl>

<dl><dt><a name="JITAdadeltaOptimizer-initialize"><strong>initialize</strong></a>(self, layers)</dt><dd><span class="code">Initializes&nbsp;the&nbsp;running&nbsp;averages&nbsp;for&nbsp;each&nbsp;layer's&nbsp;weights.<br>
Args:&nbsp;layers&nbsp;(list):&nbsp;List&nbsp;of&nbsp;layers&nbsp;in&nbsp;the&nbsp;neural&nbsp;network.<br>
Returns:&nbsp;None</span></dd></dl>

<dl><dt><a name="JITAdadeltaOptimizer-update"><strong>update</strong></a>(self, layer, dW, db, index)</dt><dd><span class="code">Updates&nbsp;the&nbsp;weights&nbsp;and&nbsp;biases&nbsp;of&nbsp;a&nbsp;layer&nbsp;using&nbsp;the&nbsp;Adadelta&nbsp;optimization&nbsp;algorithm.<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;layer&nbsp;(Layer):&nbsp;The&nbsp;layer&nbsp;to&nbsp;update.<br>
&nbsp;&nbsp;&nbsp;&nbsp;dW&nbsp;(ndarray):&nbsp;The&nbsp;gradient&nbsp;of&nbsp;the&nbsp;weights.<br>
&nbsp;&nbsp;&nbsp;&nbsp;db&nbsp;(ndarray):&nbsp;The&nbsp;gradient&nbsp;of&nbsp;the&nbsp;biases.<br>
&nbsp;&nbsp;&nbsp;&nbsp;index&nbsp;(int):&nbsp;The&nbsp;index&nbsp;of&nbsp;the&nbsp;layer.<br>
Returns:&nbsp;None</span></dd></dl>

<dl><dt><a name="JITAdadeltaOptimizer-update_layers"><strong>update_layers</strong></a>(self, layers, dWs, dbs)</dt></dl>

<hr>
Data descriptors inherited from JITAdadeltaOptimizer:<br>
<dl><dt><strong>__dict__</strong></dt>
<dd><span class="code">dictionary&nbsp;for&nbsp;instance&nbsp;variables</span></dd>
</dl>
<dl><dt><strong>__weakref__</strong></dt>
<dd><span class="code">list&nbsp;of&nbsp;weak&nbsp;references&nbsp;to&nbsp;the&nbsp;object</span></dd>
</dl>
</td></tr></table> <p>
<table class="section">
<tr class="decor title-decor heading-text">
<td class="section-title" colspan=3>&nbsp;<br><a name="JITAdamOptimizer">class <strong>JITAdamOptimizer</strong></a>(JITAdamOptimizer)</td></tr>
    
<tr><td class="decor title-decor" rowspan=2><span class="code">&nbsp;&nbsp;&nbsp;</span></td>
<td class="decor title-decor" colspan=2><span class="code"><a href="#JITAdamOptimizer">JITAdamOptimizer</a>(*args,&nbsp;**kwargs)<br>
&nbsp;<br>
Adam&nbsp;optimizer&nbsp;class&nbsp;for&nbsp;training&nbsp;neural&nbsp;networks.<br>
Formula:&nbsp;w&nbsp;=&nbsp;w&nbsp;-&nbsp;alpha&nbsp;*&nbsp;m_hat&nbsp;/&nbsp;(sqrt(v_hat)&nbsp;+&nbsp;epsilon)&nbsp;-&nbsp;lambda&nbsp;*&nbsp;w&nbsp;<br>
Derived&nbsp;from:&nbsp;<a href="https://arxiv.org/abs/1412.6980">https://arxiv.org/abs/1412.6980</a><br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;learning_rate&nbsp;(float,&nbsp;optional):&nbsp;The&nbsp;learning&nbsp;rate&nbsp;for&nbsp;the&nbsp;optimizer.&nbsp;Defaults&nbsp;to&nbsp;0.001.<br>
&nbsp;&nbsp;&nbsp;&nbsp;beta1&nbsp;(float,&nbsp;optional):&nbsp;The&nbsp;exponential&nbsp;decay&nbsp;rate&nbsp;for&nbsp;the&nbsp;first&nbsp;moment&nbsp;estimates.&nbsp;Defaults&nbsp;to&nbsp;0.9.<br>
&nbsp;&nbsp;&nbsp;&nbsp;beta2&nbsp;(float,&nbsp;optional):&nbsp;The&nbsp;exponential&nbsp;decay&nbsp;rate&nbsp;for&nbsp;the&nbsp;second&nbsp;moment&nbsp;estimates.&nbsp;Defaults&nbsp;to&nbsp;0.999.<br>
&nbsp;&nbsp;&nbsp;&nbsp;epsilon&nbsp;(float,&nbsp;optional):&nbsp;A&nbsp;small&nbsp;value&nbsp;to&nbsp;prevent&nbsp;division&nbsp;by&nbsp;zero.&nbsp;Defaults&nbsp;to&nbsp;1e-8.<br>
&nbsp;&nbsp;&nbsp;&nbsp;reg_lambda&nbsp;(float,&nbsp;optional):&nbsp;The&nbsp;regularization&nbsp;parameter.&nbsp;Defaults&nbsp;to&nbsp;0.01.<br>&nbsp;</span></td></tr>
<tr><td>&nbsp;</td>
<td class="singlecolumn"><dl><dt>Method resolution order:</dt>
<dd><a href="sega_learn.neural_networks.optimizers_jit.html#JITAdamOptimizer">JITAdamOptimizer</a></dd>
<dd>JITAdamOptimizer</dd>
<dd><a href="builtins.html#object">builtins.object</a></dd>
</dl>
<hr>
Data and other attributes defined here:<br>
<dl><dt><strong>class_type</strong> = jitclass.JITAdamOptimizer#2df18028450&lt;learning_r...t64, 2d, A),db:array(float64, 2d, A),index:int32&gt;</dl>

<hr>
Methods inherited from JITAdamOptimizer:<br>
<dl><dt><a name="JITAdamOptimizer-__init__"><strong>__init__</strong></a>(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, reg_lambda=0.01)</dt><dd><span class="code">Initialize&nbsp;self.&nbsp;&nbsp;See&nbsp;help(type(self))&nbsp;for&nbsp;accurate&nbsp;signature.</span></dd></dl>

<dl><dt><a name="JITAdamOptimizer-initialize"><strong>initialize</strong></a>(self, layers)</dt><dd><span class="code">Initializes&nbsp;the&nbsp;first&nbsp;and&nbsp;second&nbsp;moment&nbsp;estimates&nbsp;for&nbsp;each&nbsp;layer's&nbsp;weights.<br>
Args:&nbsp;layers&nbsp;(list):&nbsp;List&nbsp;of&nbsp;layers&nbsp;in&nbsp;the&nbsp;neural&nbsp;network.<br>
Returns:&nbsp;None</span></dd></dl>

<dl><dt><a name="JITAdamOptimizer-update"><strong>update</strong></a>(self, layer, dW, db, index)</dt><dd><span class="code">Updates&nbsp;the&nbsp;weights&nbsp;and&nbsp;biases&nbsp;of&nbsp;a&nbsp;layer&nbsp;using&nbsp;the&nbsp;Adam&nbsp;optimization&nbsp;algorithm.<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;layer&nbsp;(Layer):&nbsp;The&nbsp;layer&nbsp;to&nbsp;update.<br>
&nbsp;&nbsp;&nbsp;&nbsp;dW&nbsp;(ndarray):&nbsp;The&nbsp;gradient&nbsp;of&nbsp;the&nbsp;weights.<br>
&nbsp;&nbsp;&nbsp;&nbsp;db&nbsp;(ndarray):&nbsp;The&nbsp;gradient&nbsp;of&nbsp;the&nbsp;biases.<br>
&nbsp;&nbsp;&nbsp;&nbsp;index&nbsp;(int):&nbsp;The&nbsp;index&nbsp;of&nbsp;the&nbsp;layer.<br>
Returns:&nbsp;None</span></dd></dl>

<dl><dt><a name="JITAdamOptimizer-update_layers"><strong>update_layers</strong></a>(self, layers, dWs, dbs)</dt></dl>

<hr>
Data descriptors inherited from JITAdamOptimizer:<br>
<dl><dt><strong>__dict__</strong></dt>
<dd><span class="code">dictionary&nbsp;for&nbsp;instance&nbsp;variables</span></dd>
</dl>
<dl><dt><strong>__weakref__</strong></dt>
<dd><span class="code">list&nbsp;of&nbsp;weak&nbsp;references&nbsp;to&nbsp;the&nbsp;object</span></dd>
</dl>
</td></tr></table> <p>
<table class="section">
<tr class="decor title-decor heading-text">
<td class="section-title" colspan=3>&nbsp;<br><a name="JITBCEWithLogitsLoss">class <strong>JITBCEWithLogitsLoss</strong></a>(<a href="builtins.html#object">builtins.object</a>)</td></tr>
    
<tr><td class="decor title-decor"><span class="code">&nbsp;&nbsp;&nbsp;</span></td><td>&nbsp;</td>
<td class="singlecolumn">Methods defined here:<br>
<dl><dt><a name="JITBCEWithLogitsLoss-__init__"><strong>__init__</strong></a>(self)</dt><dd><span class="code">Initialize&nbsp;self.&nbsp;&nbsp;See&nbsp;help(type(self))&nbsp;for&nbsp;accurate&nbsp;signature.</span></dd></dl>

<dl><dt><a name="JITBCEWithLogitsLoss-calculate_loss"><strong>calculate_loss</strong></a>(self, logits, targets)</dt></dl>

<hr>
Data descriptors defined here:<br>
<dl><dt><strong>__dict__</strong></dt>
<dd><span class="code">dictionary&nbsp;for&nbsp;instance&nbsp;variables</span></dd>
</dl>
<dl><dt><strong>__weakref__</strong></dt>
<dd><span class="code">list&nbsp;of&nbsp;weak&nbsp;references&nbsp;to&nbsp;the&nbsp;object</span></dd>
</dl>
</td></tr></table> <p>
<table class="section">
<tr class="decor title-decor heading-text">
<td class="section-title" colspan=3>&nbsp;<br><a name="JITConvLayer">class <strong>JITConvLayer</strong></a>(JITConvLayer)</td></tr>
    
<tr><td class="decor title-decor" rowspan=2><span class="code">&nbsp;&nbsp;&nbsp;</span></td>
<td class="decor title-decor" colspan=2><span class="code"><a href="#JITConvLayer">JITConvLayer</a>(*args,&nbsp;**kwargs)<br>
&nbsp;<br>
A&nbsp;convolutional&nbsp;layer&nbsp;implementation&nbsp;for&nbsp;neural&nbsp;networks&nbsp;using&nbsp;Numba&nbsp;JIT&nbsp;compilation.<br>&nbsp;</span></td></tr>
<tr><td>&nbsp;</td>
<td class="singlecolumn"><dl><dt>Method resolution order:</dt>
<dd><a href="sega_learn.neural_networks.layers_jit.html#JITConvLayer">JITConvLayer</a></dd>
<dd>JITConvLayer</dd>
<dd><a href="builtins.html#object">builtins.object</a></dd>
</dl>
<hr>
Data and other attributes defined here:<br>
<dl><dt><strong>class_type</strong> = jitclass.JITConvLayer#2df178e5010&lt;in_channels:in...2,w_out:int32,input_size:int32,output_size:int32&gt;</dl>

<hr>
Methods inherited from JITConvLayer:<br>
<dl><dt><a name="JITConvLayer-__init__"><strong>__init__</strong></a>(self, in_channels, out_channels, kernel_size, stride=1, padding=0, activation='relu')</dt><dd><span class="code">Initialize&nbsp;self.&nbsp;&nbsp;See&nbsp;help(type(self))&nbsp;for&nbsp;accurate&nbsp;signature.</span></dd></dl>

<dl><dt><a name="JITConvLayer-activate"><strong>activate</strong></a>(self, Z)</dt><dd><span class="code">Apply&nbsp;activation&nbsp;function.</span></dd></dl>

<dl><dt><a name="JITConvLayer-activation_derivative"><strong>activation_derivative</strong></a>(self, Z)</dt><dd><span class="code">Apply&nbsp;activation&nbsp;derivative.</span></dd></dl>

<dl><dt><a name="JITConvLayer-backward"><strong>backward</strong></a>(self, d_out, reg_lambda=0)</dt><dd><span class="code">Backward&nbsp;pass&nbsp;for&nbsp;convolutional&nbsp;layer.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;d_out&nbsp;(np.ndarray):&nbsp;Gradient&nbsp;of&nbsp;the&nbsp;loss&nbsp;with&nbsp;respect&nbsp;to&nbsp;the&nbsp;layer&nbsp;output<br>
&nbsp;&nbsp;&nbsp;&nbsp;reg_lambda&nbsp;(float,&nbsp;optional):&nbsp;Regularization&nbsp;parameter<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;dX:&nbsp;Gradient&nbsp;with&nbsp;respect&nbsp;to&nbsp;the&nbsp;input&nbsp;X</span></dd></dl>

<dl><dt><a name="JITConvLayer-forward"><strong>forward</strong></a>(self, X)</dt><dd><span class="code">Forward&nbsp;pass&nbsp;for&nbsp;convolutional&nbsp;layer.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;X:&nbsp;numpy&nbsp;array&nbsp;with&nbsp;shape&nbsp;(batch_size,&nbsp;in_channels,&nbsp;height,&nbsp;width)<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;Output&nbsp;feature&nbsp;maps&nbsp;after&nbsp;convolution&nbsp;and&nbsp;activation.</span></dd></dl>

<dl><dt><a name="JITConvLayer-zero_grad"><strong>zero_grad</strong></a>(self)</dt><dd><span class="code">Reset&nbsp;the&nbsp;gradients&nbsp;of&nbsp;the&nbsp;weights&nbsp;and&nbsp;biases&nbsp;to&nbsp;zero.</span></dd></dl>

<hr>
Data descriptors inherited from JITConvLayer:<br>
<dl><dt><strong>__dict__</strong></dt>
<dd><span class="code">dictionary&nbsp;for&nbsp;instance&nbsp;variables</span></dd>
</dl>
<dl><dt><strong>__weakref__</strong></dt>
<dd><span class="code">list&nbsp;of&nbsp;weak&nbsp;references&nbsp;to&nbsp;the&nbsp;object</span></dd>
</dl>
</td></tr></table> <p>
<table class="section">
<tr class="decor title-decor heading-text">
<td class="section-title" colspan=3>&nbsp;<br><a name="JITCrossEntropyLoss">class <strong>JITCrossEntropyLoss</strong></a>(<a href="builtins.html#object">builtins.object</a>)</td></tr>
    
<tr><td class="decor title-decor"><span class="code">&nbsp;&nbsp;&nbsp;</span></td><td>&nbsp;</td>
<td class="singlecolumn">Methods defined here:<br>
<dl><dt><a name="JITCrossEntropyLoss-__init__"><strong>__init__</strong></a>(self)</dt><dd><span class="code">Initialize&nbsp;self.&nbsp;&nbsp;See&nbsp;help(type(self))&nbsp;for&nbsp;accurate&nbsp;signature.</span></dd></dl>

<dl><dt><a name="JITCrossEntropyLoss-calculate_loss"><strong>calculate_loss</strong></a>(self, logits, targets)</dt></dl>

<hr>
Data descriptors defined here:<br>
<dl><dt><strong>__dict__</strong></dt>
<dd><span class="code">dictionary&nbsp;for&nbsp;instance&nbsp;variables</span></dd>
</dl>
<dl><dt><strong>__weakref__</strong></dt>
<dd><span class="code">list&nbsp;of&nbsp;weak&nbsp;references&nbsp;to&nbsp;the&nbsp;object</span></dd>
</dl>
</td></tr></table> <p>
<table class="section">
<tr class="decor title-decor heading-text">
<td class="section-title" colspan=3>&nbsp;<br><a name="JITDenseLayer">class <strong>JITDenseLayer</strong></a>(JITDenseLayer)</td></tr>
    
<tr><td class="decor title-decor" rowspan=2><span class="code">&nbsp;&nbsp;&nbsp;</span></td>
<td class="decor title-decor" colspan=2><span class="code"><a href="#JITDenseLayer">JITDenseLayer</a>(*args,&nbsp;**kwargs)<br>
&nbsp;<br>
Initializes&nbsp;a&nbsp;fully&nbsp;connected&nbsp;layer&nbsp;<a href="builtins.html#object">object</a>,&nbsp;where&nbsp;each&nbsp;neuron&nbsp;is&nbsp;connected&nbsp;to&nbsp;all&nbsp;neurons&nbsp;in&nbsp;the&nbsp;previous&nbsp;layer.<br>
Each&nbsp;layer&nbsp;consists&nbsp;of&nbsp;weights,&nbsp;biases,&nbsp;and&nbsp;an&nbsp;activation&nbsp;function.<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;input_size&nbsp;(int):&nbsp;The&nbsp;size&nbsp;of&nbsp;the&nbsp;input&nbsp;to&nbsp;the&nbsp;layer.<br>
&nbsp;&nbsp;&nbsp;&nbsp;output_size&nbsp;(int):&nbsp;The&nbsp;size&nbsp;of&nbsp;the&nbsp;output&nbsp;from&nbsp;the&nbsp;layer.<br>
&nbsp;&nbsp;&nbsp;&nbsp;activation&nbsp;(str):&nbsp;The&nbsp;activation&nbsp;function&nbsp;to&nbsp;be&nbsp;used&nbsp;in&nbsp;the&nbsp;layer.<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
Attributes:<br>
&nbsp;&nbsp;&nbsp;&nbsp;weights&nbsp;(np.ndarray):&nbsp;Weights&nbsp;of&nbsp;the&nbsp;layer.<br>
&nbsp;&nbsp;&nbsp;&nbsp;biases&nbsp;(np.ndarray):&nbsp;Biases&nbsp;of&nbsp;the&nbsp;layer.<br>
&nbsp;&nbsp;&nbsp;&nbsp;activation&nbsp;(str):&nbsp;<a href="#Activation">Activation</a>&nbsp;function&nbsp;name.<br>
&nbsp;&nbsp;&nbsp;&nbsp;weight_gradients&nbsp;(np.ndarray):&nbsp;Gradients&nbsp;of&nbsp;the&nbsp;weights.<br>
&nbsp;&nbsp;&nbsp;&nbsp;bias_gradients&nbsp;(np.ndarray):&nbsp;Gradients&nbsp;of&nbsp;the&nbsp;biases.<br>
&nbsp;&nbsp;&nbsp;&nbsp;input_cache&nbsp;(np.ndarray):&nbsp;Cached&nbsp;input&nbsp;for&nbsp;backpropagation.<br>
&nbsp;&nbsp;&nbsp;&nbsp;output_cache&nbsp;(np.ndarray):&nbsp;Cached&nbsp;output&nbsp;for&nbsp;backpropagation.<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
Methods:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#JITDenseLayer-zero_grad">zero_grad</a>():&nbsp;Resets&nbsp;the&nbsp;gradients&nbsp;of&nbsp;the&nbsp;weights&nbsp;and&nbsp;biases&nbsp;to&nbsp;zero.<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#JITDenseLayer-forward">forward</a>(X):&nbsp;Performs&nbsp;the&nbsp;forward&nbsp;pass&nbsp;of&nbsp;the&nbsp;layer.<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#JITDenseLayer-backward">backward</a>(dA,&nbsp;reg_lambda):&nbsp;Performs&nbsp;the&nbsp;backward&nbsp;pass&nbsp;of&nbsp;the&nbsp;layer.<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#JITDenseLayer-activate">activate</a>(Z):&nbsp;Applies&nbsp;the&nbsp;activation&nbsp;function.<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#JITDenseLayer-activation_derivative">activation_derivative</a>(Z):&nbsp;Applies&nbsp;the&nbsp;derivative&nbsp;of&nbsp;the&nbsp;activation&nbsp;function.<br>&nbsp;</span></td></tr>
<tr><td>&nbsp;</td>
<td class="singlecolumn"><dl><dt>Method resolution order:</dt>
<dd><a href="sega_learn.neural_networks.layers_jit.html#JITDenseLayer">JITDenseLayer</a></dd>
<dd>JITDenseLayer</dd>
<dd><a href="builtins.html#object">builtins.object</a></dd>
</dl>
<hr>
Data and other attributes defined here:<br>
<dl><dt><strong>class_type</strong> = jitclass.JITDenseLayer#2df178cc9d0&lt;weights:array...oat64, 2d, C),input_size:int32,output_size:int32&gt;</dl>

<hr>
Methods inherited from JITDenseLayer:<br>
<dl><dt><a name="JITDenseLayer-__init__"><strong>__init__</strong></a>(self, input_size, output_size, activation='relu')</dt><dd><span class="code">Initialize&nbsp;self.&nbsp;&nbsp;See&nbsp;help(type(self))&nbsp;for&nbsp;accurate&nbsp;signature.</span></dd></dl>

<dl><dt><a name="JITDenseLayer-activate"><strong>activate</strong></a>(self, Z)</dt><dd><span class="code">Apply&nbsp;activation&nbsp;function.</span></dd></dl>

<dl><dt><a name="JITDenseLayer-activation_derivative"><strong>activation_derivative</strong></a>(self, Z)</dt><dd><span class="code">Apply&nbsp;activation&nbsp;derivative.</span></dd></dl>

<dl><dt><a name="JITDenseLayer-backward"><strong>backward</strong></a>(self, dA, reg_lambda)</dt></dl>

<dl><dt><a name="JITDenseLayer-forward"><strong>forward</strong></a>(self, X)</dt></dl>

<dl><dt><a name="JITDenseLayer-zero_grad"><strong>zero_grad</strong></a>(self)</dt><dd><span class="code">Reset&nbsp;the&nbsp;gradients&nbsp;of&nbsp;the&nbsp;weights&nbsp;and&nbsp;biases&nbsp;to&nbsp;zero.</span></dd></dl>

<hr>
Data descriptors inherited from JITDenseLayer:<br>
<dl><dt><strong>__dict__</strong></dt>
<dd><span class="code">dictionary&nbsp;for&nbsp;instance&nbsp;variables</span></dd>
</dl>
<dl><dt><strong>__weakref__</strong></dt>
<dd><span class="code">list&nbsp;of&nbsp;weak&nbsp;references&nbsp;to&nbsp;the&nbsp;object</span></dd>
</dl>
</td></tr></table> <p>
<table class="section">
<tr class="decor title-decor heading-text">
<td class="section-title" colspan=3>&nbsp;<br><a name="JITFlattenLayer">class <strong>JITFlattenLayer</strong></a>(JITFlattenLayer)</td></tr>
    
<tr><td class="decor title-decor" rowspan=2><span class="code">&nbsp;&nbsp;&nbsp;</span></td>
<td class="decor title-decor" colspan=2><span class="code"><a href="#JITFlattenLayer">JITFlattenLayer</a>(*args,&nbsp;**kwargs)<br>
&nbsp;<br>
A&nbsp;layer&nbsp;that&nbsp;flattens&nbsp;multi-dimensional&nbsp;input&nbsp;into&nbsp;a&nbsp;2D&nbsp;array&nbsp;(batch_size,&nbsp;flattened_size).<br>
Useful&nbsp;for&nbsp;transitioning&nbsp;from&nbsp;convolutional&nbsp;layers&nbsp;to&nbsp;dense&nbsp;layers.<br>&nbsp;</span></td></tr>
<tr><td>&nbsp;</td>
<td class="singlecolumn"><dl><dt>Method resolution order:</dt>
<dd><a href="sega_learn.neural_networks.layers_jit.html#JITFlattenLayer">JITFlattenLayer</a></dd>
<dd>JITFlattenLayer</dd>
<dd><a href="builtins.html#object">builtins.object</a></dd>
</dl>
<hr>
Data and other attributes defined here:<br>
<dl><dt><strong>class_type</strong> = jitclass.JITFlattenLayer#2df178ce490&lt;input_shape...put_cache:array(float64, 4d, A),input_size:int32&gt;</dl>

<hr>
Methods inherited from JITFlattenLayer:<br>
<dl><dt><a name="JITFlattenLayer-__init__"><strong>__init__</strong></a>(self)</dt><dd><span class="code">Initialize&nbsp;self.&nbsp;&nbsp;See&nbsp;help(type(self))&nbsp;for&nbsp;accurate&nbsp;signature.</span></dd></dl>

<dl><dt><a name="JITFlattenLayer-backward"><strong>backward</strong></a>(self, dA, reg_lambda=0)</dt><dd><span class="code">Reshapes&nbsp;the&nbsp;gradient&nbsp;back&nbsp;to&nbsp;the&nbsp;original&nbsp;input&nbsp;shape.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;dA&nbsp;(np.ndarray):&nbsp;Gradient&nbsp;of&nbsp;the&nbsp;loss&nbsp;with&nbsp;respect&nbsp;to&nbsp;the&nbsp;layer's&nbsp;output,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;shape&nbsp;(batch_size,&nbsp;flattened_size)<br>
&nbsp;&nbsp;&nbsp;&nbsp;reg_lambda&nbsp;(float):&nbsp;Regularization&nbsp;parameter&nbsp;(unused&nbsp;in&nbsp;<a href="#FlattenLayer">FlattenLayer</a>).<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;np.ndarray:&nbsp;Gradient&nbsp;with&nbsp;respect&nbsp;to&nbsp;the&nbsp;input,&nbsp;reshaped&nbsp;to&nbsp;original&nbsp;input&nbsp;shape.</span></dd></dl>

<dl><dt><a name="JITFlattenLayer-forward"><strong>forward</strong></a>(self, X)</dt><dd><span class="code">Flattens&nbsp;the&nbsp;input&nbsp;tensor.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;X&nbsp;(np.ndarray):&nbsp;Input&nbsp;data&nbsp;of&nbsp;shape&nbsp;(batch_size,&nbsp;channels,&nbsp;height,&nbsp;width)<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;np.ndarray:&nbsp;Flattened&nbsp;output&nbsp;of&nbsp;shape&nbsp;(batch_size,&nbsp;flattened_size)</span></dd></dl>

<hr>
Data descriptors inherited from JITFlattenLayer:<br>
<dl><dt><strong>__dict__</strong></dt>
<dd><span class="code">dictionary&nbsp;for&nbsp;instance&nbsp;variables</span></dd>
</dl>
<dl><dt><strong>__weakref__</strong></dt>
<dd><span class="code">list&nbsp;of&nbsp;weak&nbsp;references&nbsp;to&nbsp;the&nbsp;object</span></dd>
</dl>
</td></tr></table> <p>
<table class="section">
<tr class="decor title-decor heading-text">
<td class="section-title" colspan=3>&nbsp;<br><a name="JITRNNLayer">class <strong>JITRNNLayer</strong></a>(<a href="builtins.html#object">builtins.object</a>)</td></tr>
    
<tr><td class="decor title-decor" rowspan=2><span class="code">&nbsp;&nbsp;&nbsp;</span></td>
<td class="decor title-decor" colspan=2><span class="code"><a href="#JITRNNLayer">JITRNNLayer</a>(input_size,&nbsp;hidden_size,&nbsp;activation='tanh')<br>
&nbsp;<br>
<br>&nbsp;</span></td></tr>
<tr><td>&nbsp;</td>
<td class="singlecolumn">Methods defined here:<br>
<dl><dt><a name="JITRNNLayer-__init__"><strong>__init__</strong></a>(self, input_size, hidden_size, activation='tanh')</dt><dd><span class="code">Initialize&nbsp;self.&nbsp;&nbsp;See&nbsp;help(type(self))&nbsp;for&nbsp;accurate&nbsp;signature.</span></dd></dl>

<hr>
Data descriptors defined here:<br>
<dl><dt><strong>__dict__</strong></dt>
<dd><span class="code">dictionary&nbsp;for&nbsp;instance&nbsp;variables</span></dd>
</dl>
<dl><dt><strong>__weakref__</strong></dt>
<dd><span class="code">list&nbsp;of&nbsp;weak&nbsp;references&nbsp;to&nbsp;the&nbsp;object</span></dd>
</dl>
</td></tr></table> <p>
<table class="section">
<tr class="decor title-decor heading-text">
<td class="section-title" colspan=3>&nbsp;<br><a name="JITSGDOptimizer">class <strong>JITSGDOptimizer</strong></a>(JITSGDOptimizer)</td></tr>
    
<tr><td class="decor title-decor" rowspan=2><span class="code">&nbsp;&nbsp;&nbsp;</span></td>
<td class="decor title-decor" colspan=2><span class="code"><a href="#JITSGDOptimizer">JITSGDOptimizer</a>(*args,&nbsp;**kwargs)<br>
&nbsp;<br>
Stochastic&nbsp;Gradient&nbsp;Descent&nbsp;(SGD)&nbsp;optimizer&nbsp;class&nbsp;for&nbsp;training&nbsp;neural&nbsp;networks.<br>
Formula:&nbsp;w&nbsp;=&nbsp;w&nbsp;-&nbsp;learning_rate&nbsp;*&nbsp;dW,&nbsp;b&nbsp;=&nbsp;b&nbsp;-&nbsp;learning_rate&nbsp;*&nbsp;db<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;learning_rate&nbsp;(float,&nbsp;optional):&nbsp;The&nbsp;learning&nbsp;rate&nbsp;for&nbsp;the&nbsp;optimizer.&nbsp;Defaults&nbsp;to&nbsp;0.001.<br>
&nbsp;&nbsp;&nbsp;&nbsp;momentum&nbsp;(float,&nbsp;optional):&nbsp;The&nbsp;momentum&nbsp;factor.&nbsp;Defaults&nbsp;to&nbsp;0.0.<br>
&nbsp;&nbsp;&nbsp;&nbsp;reg_lambda&nbsp;(float,&nbsp;optional):&nbsp;The&nbsp;regularization&nbsp;parameter.&nbsp;Defaults&nbsp;to&nbsp;0.0.<br>&nbsp;</span></td></tr>
<tr><td>&nbsp;</td>
<td class="singlecolumn"><dl><dt>Method resolution order:</dt>
<dd><a href="sega_learn.neural_networks.optimizers_jit.html#JITSGDOptimizer">JITSGDOptimizer</a></dd>
<dd>JITSGDOptimizer</dd>
<dd><a href="builtins.html#object">builtins.object</a></dd>
</dl>
<hr>
Data and other attributes defined here:<br>
<dl><dt><strong>class_type</strong> = jitclass.JITSGDOptimizer#2df1802a950&lt;learning_ra...eg_lambda:float64,velocity:array(float64, 3d, C)&gt;</dl>

<hr>
Methods inherited from JITSGDOptimizer:<br>
<dl><dt><a name="JITSGDOptimizer-__init__"><strong>__init__</strong></a>(self, learning_rate=0.001, momentum=0.0, reg_lambda=0.0)</dt><dd><span class="code">Initialize&nbsp;self.&nbsp;&nbsp;See&nbsp;help(type(self))&nbsp;for&nbsp;accurate&nbsp;signature.</span></dd></dl>

<dl><dt><a name="JITSGDOptimizer-initialize"><strong>initialize</strong></a>(self, layers)</dt><dd><span class="code">Initializes&nbsp;the&nbsp;velocity&nbsp;for&nbsp;each&nbsp;layer's&nbsp;weights.<br>
Args:&nbsp;layers&nbsp;(list):&nbsp;List&nbsp;of&nbsp;layers&nbsp;in&nbsp;the&nbsp;neural&nbsp;network.<br>
Returns:&nbsp;None</span></dd></dl>

<dl><dt><a name="JITSGDOptimizer-update"><strong>update</strong></a>(self, layer, dW, db, index)</dt><dd><span class="code">Updates&nbsp;the&nbsp;weights&nbsp;and&nbsp;biases&nbsp;of&nbsp;a&nbsp;layer&nbsp;using&nbsp;the&nbsp;SGD&nbsp;optimization&nbsp;algorithm.<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;layer&nbsp;(Layer):&nbsp;The&nbsp;layer&nbsp;to&nbsp;update.<br>
&nbsp;&nbsp;&nbsp;&nbsp;dW&nbsp;(ndarray):&nbsp;The&nbsp;gradient&nbsp;of&nbsp;the&nbsp;weights.<br>
&nbsp;&nbsp;&nbsp;&nbsp;db&nbsp;(ndarray):&nbsp;The&nbsp;gradient&nbsp;of&nbsp;the&nbsp;biases.<br>
&nbsp;&nbsp;&nbsp;&nbsp;index&nbsp;(int):&nbsp;The&nbsp;index&nbsp;of&nbsp;the&nbsp;layer.<br>
Returns:&nbsp;None</span></dd></dl>

<dl><dt><a name="JITSGDOptimizer-update_layers"><strong>update_layers</strong></a>(self, layers, dWs, dbs)</dt></dl>

<hr>
Data descriptors inherited from JITSGDOptimizer:<br>
<dl><dt><strong>__dict__</strong></dt>
<dd><span class="code">dictionary&nbsp;for&nbsp;instance&nbsp;variables</span></dd>
</dl>
<dl><dt><strong>__weakref__</strong></dt>
<dd><span class="code">list&nbsp;of&nbsp;weak&nbsp;references&nbsp;to&nbsp;the&nbsp;object</span></dd>
</dl>
</td></tr></table> <p>
<table class="section">
<tr class="decor title-decor heading-text">
<td class="section-title" colspan=3>&nbsp;<br><a name="NeuralNetworkBase">class <strong>NeuralNetworkBase</strong></a>(<a href="builtins.html#object">builtins.object</a>)</td></tr>
    
<tr><td class="decor title-decor" rowspan=2><span class="code">&nbsp;&nbsp;&nbsp;</span></td>
<td class="decor title-decor" colspan=2><span class="code"><a href="#NeuralNetworkBase">NeuralNetworkBase</a>(layers,&nbsp;dropout_rate=0.0,&nbsp;reg_lambda=0.0,&nbsp;activations=None)<br>
&nbsp;<br>
<br>&nbsp;</span></td></tr>
<tr><td>&nbsp;</td>
<td class="singlecolumn">Methods defined here:<br>
<dl><dt><a name="NeuralNetworkBase-__init__"><strong>__init__</strong></a>(self, layers, dropout_rate=0.0, reg_lambda=0.0, activations=None)</dt><dd><span class="code">Initialize&nbsp;self.&nbsp;&nbsp;See&nbsp;help(type(self))&nbsp;for&nbsp;accurate&nbsp;signature.</span></dd></dl>

<dl><dt><a name="NeuralNetworkBase-apply_dropout"><strong>apply_dropout</strong></a>(self, X)</dt><dd><span class="code">Applies&nbsp;dropout&nbsp;to&nbsp;the&nbsp;activation&nbsp;X.<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;X&nbsp;(ndarray):&nbsp;<a href="#Activation">Activation</a>&nbsp;values.<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;ndarray:&nbsp;<a href="#Activation">Activation</a>&nbsp;values&nbsp;after&nbsp;applying&nbsp;dropout.</span></dd></dl>

<dl><dt><a name="NeuralNetworkBase-backward"><strong>backward</strong></a>(self, y)</dt></dl>

<dl><dt><a name="NeuralNetworkBase-calculate_loss"><strong>calculate_loss</strong></a>(self, X, y)</dt></dl>

<dl><dt><a name="NeuralNetworkBase-calculate_precision_recall_f1"><strong>calculate_precision_recall_f1</strong></a>(self, X, y)</dt><dd><span class="code">Calculates&nbsp;precision,&nbsp;recall,&nbsp;and&nbsp;F1&nbsp;score.<br>
Parameters:<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;X&nbsp;(ndarray):&nbsp;Input&nbsp;data<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;y&nbsp;(ndarray):&nbsp;Target&nbsp;labels<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;precision&nbsp;(float):&nbsp;Precision&nbsp;score<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;recall&nbsp;(float):&nbsp;Recall&nbsp;score<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;f1&nbsp;(float):&nbsp;F1&nbsp;score</span></dd></dl>

<dl><dt><a name="NeuralNetworkBase-compute_l2_reg"><strong>compute_l2_reg</strong></a>(self, weights)</dt><dd><span class="code">Computes&nbsp;the&nbsp;L2&nbsp;regularization&nbsp;term.<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;weights&nbsp;(list):&nbsp;List&nbsp;of&nbsp;weight&nbsp;matrices.<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;float:&nbsp;L2&nbsp;regularization&nbsp;term.</span></dd></dl>

<dl><dt><a name="NeuralNetworkBase-create_scheduler"><strong>create_scheduler</strong></a>(self, scheduler_type, optimizer, **kwargs)</dt><dd><span class="code">Creates&nbsp;a&nbsp;learning&nbsp;rate&nbsp;scheduler.</span></dd></dl>

<dl><dt><a name="NeuralNetworkBase-evaluate"><strong>evaluate</strong></a>(self, X, y)</dt></dl>

<dl><dt><a name="NeuralNetworkBase-forward"><strong>forward</strong></a>(self, X, training=True)</dt></dl>

<dl><dt><a name="NeuralNetworkBase-initialize_layers"><strong>initialize_layers</strong></a>(self)</dt></dl>

<dl><dt><a name="NeuralNetworkBase-plot_metrics"><strong>plot_metrics</strong></a>(self, save_dir=None)</dt><dd><span class="code">Plots&nbsp;the&nbsp;training&nbsp;and&nbsp;validation&nbsp;metrics.</span></dd></dl>

<dl><dt><a name="NeuralNetworkBase-predict"><strong>predict</strong></a>(self, X)</dt></dl>

<dl><dt><a name="NeuralNetworkBase-train"><strong>train</strong></a>(self, X_train, y_train, X_val=None, y_val=None, optimizer=None, epochs=100, batch_size=32, early_stopping_threshold=10, lr_scheduler=None, p=True, use_tqdm=True, n_jobs=1, track_metrics=False, track_adv_metrics=False)</dt></dl>

<hr>
Data descriptors defined here:<br>
<dl><dt><strong>__dict__</strong></dt>
<dd><span class="code">dictionary&nbsp;for&nbsp;instance&nbsp;variables</span></dd>
</dl>
<dl><dt><strong>__weakref__</strong></dt>
<dd><span class="code">list&nbsp;of&nbsp;weak&nbsp;references&nbsp;to&nbsp;the&nbsp;object</span></dd>
</dl>
</td></tr></table> <p>
<table class="section">
<tr class="decor title-decor heading-text">
<td class="section-title" colspan=3>&nbsp;<br><a name="NumbaBackendNeuralNetwork">class <strong>NumbaBackendNeuralNetwork</strong></a>(<a href="sega_learn.neural_networks.neuralNetworkBase.html#NeuralNetworkBase">sega_learn.neural_networks.neuralNetworkBase.NeuralNetworkBase</a>)</td></tr>
    
<tr><td class="decor title-decor" rowspan=2><span class="code">&nbsp;&nbsp;&nbsp;</span></td>
<td class="decor title-decor" colspan=2><span class="code"><a href="#NumbaBackendNeuralNetwork">NumbaBackendNeuralNetwork</a>(layers,&nbsp;dropout_rate=0.2,&nbsp;reg_lambda=0.01,&nbsp;activations=None,&nbsp;compile_numba=True,&nbsp;progress_bar=True)<br>
&nbsp;<br>
<br>&nbsp;</span></td></tr>
<tr><td>&nbsp;</td>
<td class="singlecolumn"><dl><dt>Method resolution order:</dt>
<dd><a href="sega_learn.neural_networks.neuralNetworkNumbaBackend.html#NumbaBackendNeuralNetwork">NumbaBackendNeuralNetwork</a></dd>
<dd><a href="sega_learn.neural_networks.neuralNetworkBase.html#NeuralNetworkBase">sega_learn.neural_networks.neuralNetworkBase.NeuralNetworkBase</a></dd>
<dd><a href="builtins.html#object">builtins.object</a></dd>
</dl>
<hr>
Methods defined here:<br>
<dl><dt><a name="NumbaBackendNeuralNetwork-__init__"><strong>__init__</strong></a>(self, layers, dropout_rate=0.2, reg_lambda=0.01, activations=None, compile_numba=True, progress_bar=True)</dt><dd><span class="code">Initializes&nbsp;the&nbsp;Numba&nbsp;backend&nbsp;neural&nbsp;network.<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;layers&nbsp;(list):&nbsp;List&nbsp;of&nbsp;layer&nbsp;sizes&nbsp;or&nbsp;Layer&nbsp;objects.<br>
&nbsp;&nbsp;&nbsp;&nbsp;dropout_rate&nbsp;(float):&nbsp;Dropout&nbsp;rate&nbsp;for&nbsp;regularization.<br>
&nbsp;&nbsp;&nbsp;&nbsp;reg_lambda&nbsp;(float):&nbsp;L2&nbsp;regularization&nbsp;parameter.<br>
&nbsp;&nbsp;&nbsp;&nbsp;activations&nbsp;(list):&nbsp;List&nbsp;of&nbsp;activation&nbsp;functions&nbsp;for&nbsp;each&nbsp;layer.<br>
&nbsp;&nbsp;&nbsp;&nbsp;compile_numba&nbsp;(bool):&nbsp;Whether&nbsp;to&nbsp;compile&nbsp;Numba&nbsp;functions.<br>
&nbsp;&nbsp;&nbsp;&nbsp;progress_bar&nbsp;(bool):&nbsp;Whether&nbsp;to&nbsp;display&nbsp;a&nbsp;progress&nbsp;bar.</span></dd></dl>

<dl><dt><a name="NumbaBackendNeuralNetwork-backward"><strong>backward</strong></a>(self, y)</dt><dd><span class="code">Performs&nbsp;backward&nbsp;propagation&nbsp;to&nbsp;calculate&nbsp;the&nbsp;gradients.<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;y&nbsp;(ndarray):&nbsp;Target&nbsp;labels&nbsp;of&nbsp;shape&nbsp;(m,&nbsp;output_size).</span></dd></dl>

<dl><dt><a name="NumbaBackendNeuralNetwork-calculate_loss"><strong>calculate_loss</strong></a>(self, X, y)</dt><dd><span class="code">Calculates&nbsp;the&nbsp;loss&nbsp;with&nbsp;L2&nbsp;regularization.<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;X&nbsp;(ndarray):&nbsp;Input&nbsp;data.<br>
&nbsp;&nbsp;&nbsp;&nbsp;y&nbsp;(ndarray):&nbsp;Target&nbsp;labels.<br>
Returns:&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;float:&nbsp;The&nbsp;calculated&nbsp;loss&nbsp;value.</span></dd></dl>

<dl><dt><a name="NumbaBackendNeuralNetwork-compile_numba_functions"><strong>compile_numba_functions</strong></a>(self, progress_bar=True)</dt><dd><span class="code">Compiles&nbsp;all&nbsp;Numba&nbsp;JIT&nbsp;functions&nbsp;to&nbsp;improve&nbsp;performance.<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;progress_bar&nbsp;(bool):&nbsp;Whether&nbsp;to&nbsp;display&nbsp;a&nbsp;progress&nbsp;bar.</span></dd></dl>

<dl><dt><a name="NumbaBackendNeuralNetwork-evaluate"><strong>evaluate</strong></a>(self, X, y)</dt><dd><span class="code">Evaluates&nbsp;the&nbsp;neural&nbsp;network&nbsp;on&nbsp;the&nbsp;given&nbsp;data.<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;X&nbsp;(ndarray):&nbsp;Input&nbsp;data.<br>
&nbsp;&nbsp;&nbsp;&nbsp;y&nbsp;(ndarray):&nbsp;Target&nbsp;labels.<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tuple:&nbsp;Accuracy&nbsp;and&nbsp;predicted&nbsp;labels.</span></dd></dl>

<dl><dt><a name="NumbaBackendNeuralNetwork-forward"><strong>forward</strong></a>(self, X, training=True)</dt><dd><span class="code">Performs&nbsp;forward&nbsp;propagation&nbsp;through&nbsp;the&nbsp;neural&nbsp;network.<br>
Args:&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;X&nbsp;(ndarray):&nbsp;Input&nbsp;data&nbsp;of&nbsp;shape&nbsp;(batch_size,&nbsp;input_size).<br>
&nbsp;&nbsp;&nbsp;&nbsp;training&nbsp;(bool):&nbsp;Whether&nbsp;the&nbsp;network&nbsp;is&nbsp;in&nbsp;training&nbsp;mode&nbsp;(applies&nbsp;dropout).<br>
Returns:&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;ndarray:&nbsp;Output&nbsp;predictions&nbsp;of&nbsp;shape&nbsp;(batch_size,&nbsp;output_size).</span></dd></dl>

<dl><dt><a name="NumbaBackendNeuralNetwork-initialize_new_layers"><strong>initialize_new_layers</strong></a>(self)</dt><dd><span class="code">Initializes&nbsp;the&nbsp;layers&nbsp;of&nbsp;the&nbsp;neural&nbsp;network.<br>
Each&nbsp;layer&nbsp;is&nbsp;created&nbsp;with&nbsp;the&nbsp;specified&nbsp;number&nbsp;of&nbsp;neurons&nbsp;and&nbsp;activation&nbsp;function.</span></dd></dl>

<dl><dt><a name="NumbaBackendNeuralNetwork-predict"><strong>predict</strong></a>(self, X)</dt><dd><span class="code">Predicts&nbsp;the&nbsp;output&nbsp;for&nbsp;the&nbsp;given&nbsp;input&nbsp;data.<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;X&nbsp;(ndarray):&nbsp;Input&nbsp;data.<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;ndarray:&nbsp;Predicted&nbsp;outputs.</span></dd></dl>

<dl><dt><a name="NumbaBackendNeuralNetwork-restore_layers"><strong>restore_layers</strong></a>(self)</dt><dd><span class="code">Restores&nbsp;the&nbsp;layers&nbsp;after&nbsp;initialization.</span></dd></dl>

<dl><dt><a name="NumbaBackendNeuralNetwork-store_init_layers"><strong>store_init_layers</strong></a>(self)</dt><dd><span class="code">Stores&nbsp;the&nbsp;layers&nbsp;to&nbsp;restore&nbsp;after&nbsp;initialization.</span></dd></dl>

<dl><dt><a name="NumbaBackendNeuralNetwork-train"><strong>train</strong></a>(self, X_train, y_train, X_val=None, y_val=None, optimizer=None, epochs=100, batch_size=32, early_stopping_threshold=10, lr_scheduler=None, p=True, use_tqdm=True, n_jobs=1, track_metrics=False, track_adv_metrics=False)</dt><dd><span class="code">Trains&nbsp;the&nbsp;neural&nbsp;network&nbsp;model.<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;X_train&nbsp;(ndarray):&nbsp;Training&nbsp;data&nbsp;features.<br>
&nbsp;&nbsp;&nbsp;&nbsp;y_train&nbsp;(ndarray):&nbsp;Training&nbsp;data&nbsp;labels.<br>
&nbsp;&nbsp;&nbsp;&nbsp;X_val&nbsp;(ndarray):&nbsp;Validation&nbsp;data&nbsp;features,&nbsp;optional.<br>
&nbsp;&nbsp;&nbsp;&nbsp;y_val&nbsp;(ndarray):&nbsp;Validation&nbsp;data&nbsp;labels,&nbsp;optional.<br>
&nbsp;&nbsp;&nbsp;&nbsp;optimizer&nbsp;(Optimizer):&nbsp;Optimizer&nbsp;for&nbsp;updating&nbsp;parameters&nbsp;(default:&nbsp;JITAdam,&nbsp;lr=0.0001).<br>
&nbsp;&nbsp;&nbsp;&nbsp;epochs&nbsp;(int):&nbsp;Number&nbsp;of&nbsp;training&nbsp;epochs&nbsp;(default:&nbsp;100).<br>
&nbsp;&nbsp;&nbsp;&nbsp;batch_size&nbsp;(int):&nbsp;Batch&nbsp;size&nbsp;for&nbsp;mini-batch&nbsp;gradient&nbsp;descent&nbsp;(default:&nbsp;32).<br>
&nbsp;&nbsp;&nbsp;&nbsp;early_stopping_threshold&nbsp;(int):&nbsp;Patience&nbsp;for&nbsp;early&nbsp;stopping&nbsp;(default:&nbsp;10).<br>
&nbsp;&nbsp;&nbsp;&nbsp;lr_scheduler&nbsp;(Scheduler):&nbsp;Learning&nbsp;rate&nbsp;scheduler&nbsp;(default:&nbsp;None).<br>
&nbsp;&nbsp;&nbsp;&nbsp;p&nbsp;(bool):&nbsp;Whether&nbsp;to&nbsp;print&nbsp;training&nbsp;progress&nbsp;(default:&nbsp;True).<br>
&nbsp;&nbsp;&nbsp;&nbsp;use_tqdm&nbsp;(bool):&nbsp;Whether&nbsp;to&nbsp;use&nbsp;tqdm&nbsp;for&nbsp;progress&nbsp;bar&nbsp;(default:&nbsp;True).<br>
&nbsp;&nbsp;&nbsp;&nbsp;n_jobs&nbsp;(int):&nbsp;Number&nbsp;of&nbsp;jobs&nbsp;for&nbsp;parallel&nbsp;processing&nbsp;(default:&nbsp;1).<br>
&nbsp;&nbsp;&nbsp;&nbsp;track_metrics&nbsp;(bool):&nbsp;Whether&nbsp;to&nbsp;track&nbsp;training&nbsp;metrics&nbsp;(default:&nbsp;False).<br>
&nbsp;&nbsp;&nbsp;&nbsp;track_adv_metrics&nbsp;(bool):&nbsp;Whether&nbsp;to&nbsp;track&nbsp;advanced&nbsp;metrics&nbsp;(default:&nbsp;False).</span></dd></dl>

<dl><dt><a name="NumbaBackendNeuralNetwork-tune_hyperparameters"><strong>tune_hyperparameters</strong></a>(self, X_train, y_train, X_val, y_val, param_grid, layer_configs=None, optimizer_types=None, lr_range=(0.0001, 0.01, 5), epochs=30, batch_size=32)</dt><dd><span class="code">Performs&nbsp;hyperparameter&nbsp;tuning&nbsp;using&nbsp;grid&nbsp;search.<br>
Parameters:<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;X_train,&nbsp;y_train:&nbsp;Training&nbsp;data<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;X_val,&nbsp;y_val:&nbsp;Validation&nbsp;data<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;param_grid:&nbsp;Dict&nbsp;of&nbsp;parameters&nbsp;to&nbsp;try<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;layer_configs:&nbsp;List&nbsp;of&nbsp;layer&nbsp;configurations<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;optimizer_types:&nbsp;List&nbsp;of&nbsp;optimizer&nbsp;types<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;lr_range:&nbsp;(min_lr,&nbsp;max_lr,&nbsp;num_steps)&nbsp;for&nbsp;learning&nbsp;rates<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;epochs:&nbsp;Max&nbsp;epochs&nbsp;for&nbsp;each&nbsp;trial<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;batch_size:&nbsp;Batch&nbsp;size&nbsp;for&nbsp;training<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;best_params:&nbsp;Best&nbsp;hyperparameters&nbsp;found<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;best_accuracy:&nbsp;Best&nbsp;validation&nbsp;accuracy</span></dd></dl>

<hr>
Static methods defined here:<br>
<dl><dt><a name="NumbaBackendNeuralNetwork-is_not_instance_of_classes"><strong>is_not_instance_of_classes</strong></a>(obj, classes)</dt><dd><span class="code">Checks&nbsp;if&nbsp;an&nbsp;<a href="builtins.html#object">object</a>&nbsp;is&nbsp;not&nbsp;an&nbsp;instance&nbsp;of&nbsp;any&nbsp;class&nbsp;in&nbsp;a&nbsp;list&nbsp;of&nbsp;classes.<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;obj:&nbsp;The&nbsp;<a href="builtins.html#object">object</a>&nbsp;to&nbsp;check.<br>
&nbsp;&nbsp;&nbsp;&nbsp;classes:&nbsp;A&nbsp;list&nbsp;of&nbsp;classes.<br>
Returns:&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;bool:&nbsp;True&nbsp;if&nbsp;the&nbsp;<a href="builtins.html#object">object</a>&nbsp;is&nbsp;not&nbsp;an&nbsp;instance&nbsp;of&nbsp;any&nbsp;class&nbsp;in&nbsp;the&nbsp;list&nbsp;of&nbsp;classes,&nbsp;False&nbsp;otherwise.</span></dd></dl>

<hr>
Methods inherited from <a href="sega_learn.neural_networks.neuralNetworkBase.html#NeuralNetworkBase">sega_learn.neural_networks.neuralNetworkBase.NeuralNetworkBase</a>:<br>
<dl><dt><a name="NumbaBackendNeuralNetwork-apply_dropout"><strong>apply_dropout</strong></a>(self, X)</dt><dd><span class="code">Applies&nbsp;dropout&nbsp;to&nbsp;the&nbsp;activation&nbsp;X.<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;X&nbsp;(ndarray):&nbsp;<a href="#Activation">Activation</a>&nbsp;values.<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;ndarray:&nbsp;<a href="#Activation">Activation</a>&nbsp;values&nbsp;after&nbsp;applying&nbsp;dropout.</span></dd></dl>

<dl><dt><a name="NumbaBackendNeuralNetwork-calculate_precision_recall_f1"><strong>calculate_precision_recall_f1</strong></a>(self, X, y)</dt><dd><span class="code">Calculates&nbsp;precision,&nbsp;recall,&nbsp;and&nbsp;F1&nbsp;score.<br>
Parameters:<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;X&nbsp;(ndarray):&nbsp;Input&nbsp;data<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;y&nbsp;(ndarray):&nbsp;Target&nbsp;labels<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;precision&nbsp;(float):&nbsp;Precision&nbsp;score<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;recall&nbsp;(float):&nbsp;Recall&nbsp;score<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;f1&nbsp;(float):&nbsp;F1&nbsp;score</span></dd></dl>

<dl><dt><a name="NumbaBackendNeuralNetwork-compute_l2_reg"><strong>compute_l2_reg</strong></a>(self, weights)</dt><dd><span class="code">Computes&nbsp;the&nbsp;L2&nbsp;regularization&nbsp;term.<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;weights&nbsp;(list):&nbsp;List&nbsp;of&nbsp;weight&nbsp;matrices.<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;float:&nbsp;L2&nbsp;regularization&nbsp;term.</span></dd></dl>

<dl><dt><a name="NumbaBackendNeuralNetwork-create_scheduler"><strong>create_scheduler</strong></a>(self, scheduler_type, optimizer, **kwargs)</dt><dd><span class="code">Creates&nbsp;a&nbsp;learning&nbsp;rate&nbsp;scheduler.</span></dd></dl>

<dl><dt><a name="NumbaBackendNeuralNetwork-initialize_layers"><strong>initialize_layers</strong></a>(self)</dt></dl>

<dl><dt><a name="NumbaBackendNeuralNetwork-plot_metrics"><strong>plot_metrics</strong></a>(self, save_dir=None)</dt><dd><span class="code">Plots&nbsp;the&nbsp;training&nbsp;and&nbsp;validation&nbsp;metrics.</span></dd></dl>

<hr>
Data descriptors inherited from <a href="sega_learn.neural_networks.neuralNetworkBase.html#NeuralNetworkBase">sega_learn.neural_networks.neuralNetworkBase.NeuralNetworkBase</a>:<br>
<dl><dt><strong>__dict__</strong></dt>
<dd><span class="code">dictionary&nbsp;for&nbsp;instance&nbsp;variables</span></dd>
</dl>
<dl><dt><strong>__weakref__</strong></dt>
<dd><span class="code">list&nbsp;of&nbsp;weak&nbsp;references&nbsp;to&nbsp;the&nbsp;object</span></dd>
</dl>
</td></tr></table> <p>
<table class="section">
<tr class="decor title-decor heading-text">
<td class="section-title" colspan=3>&nbsp;<br><a name="RNNLayer">class <strong>RNNLayer</strong></a>(<a href="builtins.html#object">builtins.object</a>)</td></tr>
    
<tr><td class="decor title-decor" rowspan=2><span class="code">&nbsp;&nbsp;&nbsp;</span></td>
<td class="decor title-decor" colspan=2><span class="code"><a href="#RNNLayer">RNNLayer</a>(input_size,&nbsp;hidden_size,&nbsp;activation='tanh')<br>
&nbsp;<br>
<br>&nbsp;</span></td></tr>
<tr><td>&nbsp;</td>
<td class="singlecolumn">Methods defined here:<br>
<dl><dt><a name="RNNLayer-__init__"><strong>__init__</strong></a>(self, input_size, hidden_size, activation='tanh')</dt><dd><span class="code">Initialize&nbsp;self.&nbsp;&nbsp;See&nbsp;help(type(self))&nbsp;for&nbsp;accurate&nbsp;signature.</span></dd></dl>

<hr>
Data descriptors defined here:<br>
<dl><dt><strong>__dict__</strong></dt>
<dd><span class="code">dictionary&nbsp;for&nbsp;instance&nbsp;variables</span></dd>
</dl>
<dl><dt><strong>__weakref__</strong></dt>
<dd><span class="code">list&nbsp;of&nbsp;weak&nbsp;references&nbsp;to&nbsp;the&nbsp;object</span></dd>
</dl>
</td></tr></table> <p>
<table class="section">
<tr class="decor title-decor heading-text">
<td class="section-title" colspan=3>&nbsp;<br><a name="SGDOptimizer">class <strong>SGDOptimizer</strong></a>(<a href="builtins.html#object">builtins.object</a>)</td></tr>
    
<tr><td class="decor title-decor" rowspan=2><span class="code">&nbsp;&nbsp;&nbsp;</span></td>
<td class="decor title-decor" colspan=2><span class="code"><a href="#SGDOptimizer">SGDOptimizer</a>(learning_rate=0.001,&nbsp;momentum=0.0,&nbsp;reg_lambda=0.0)<br>
&nbsp;<br>
Stochastic&nbsp;Gradient&nbsp;Descent&nbsp;(SGD)&nbsp;optimizer&nbsp;class&nbsp;for&nbsp;training&nbsp;neural&nbsp;networks.<br>
Formula:&nbsp;w&nbsp;=&nbsp;w&nbsp;-&nbsp;learning_rate&nbsp;*&nbsp;dW,&nbsp;b&nbsp;=&nbsp;b&nbsp;-&nbsp;learning_rate&nbsp;*&nbsp;db<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;learning_rate&nbsp;(float,&nbsp;optional):&nbsp;The&nbsp;learning&nbsp;rate&nbsp;for&nbsp;the&nbsp;optimizer.&nbsp;Defaults&nbsp;to&nbsp;0.001.<br>
&nbsp;&nbsp;&nbsp;&nbsp;momentum&nbsp;(float,&nbsp;optional):&nbsp;The&nbsp;momentum&nbsp;factor.&nbsp;Defaults&nbsp;to&nbsp;0.0.<br>
&nbsp;&nbsp;&nbsp;&nbsp;reg_lambda&nbsp;(float,&nbsp;optional):&nbsp;The&nbsp;regularization&nbsp;parameter.&nbsp;Defaults&nbsp;to&nbsp;0.0.<br>&nbsp;</span></td></tr>
<tr><td>&nbsp;</td>
<td class="singlecolumn">Methods defined here:<br>
<dl><dt><a name="SGDOptimizer-__init__"><strong>__init__</strong></a>(self, learning_rate=0.001, momentum=0.0, reg_lambda=0.0)</dt><dd><span class="code">Initialize&nbsp;self.&nbsp;&nbsp;See&nbsp;help(type(self))&nbsp;for&nbsp;accurate&nbsp;signature.</span></dd></dl>

<dl><dt><a name="SGDOptimizer-initialize"><strong>initialize</strong></a>(self, layers)</dt><dd><span class="code">Initializes&nbsp;the&nbsp;velocity&nbsp;for&nbsp;each&nbsp;layer's&nbsp;weights.<br>
Args:&nbsp;layers&nbsp;(list):&nbsp;List&nbsp;of&nbsp;layers&nbsp;in&nbsp;the&nbsp;neural&nbsp;network.<br>
Returns:&nbsp;None</span></dd></dl>

<dl><dt><a name="SGDOptimizer-update"><strong>update</strong></a>(self, layer, dW, db, index)</dt><dd><span class="code">Updates&nbsp;the&nbsp;weights&nbsp;and&nbsp;biases&nbsp;of&nbsp;a&nbsp;layer&nbsp;using&nbsp;the&nbsp;SGD&nbsp;optimization&nbsp;algorithm.<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;layer&nbsp;(Layer):&nbsp;The&nbsp;layer&nbsp;to&nbsp;update.<br>
&nbsp;&nbsp;&nbsp;&nbsp;dW&nbsp;(ndarray):&nbsp;The&nbsp;gradient&nbsp;of&nbsp;the&nbsp;weights.<br>
&nbsp;&nbsp;&nbsp;&nbsp;db&nbsp;(ndarray):&nbsp;The&nbsp;gradient&nbsp;of&nbsp;the&nbsp;biases.<br>
&nbsp;&nbsp;&nbsp;&nbsp;index&nbsp;(int):&nbsp;The&nbsp;index&nbsp;of&nbsp;the&nbsp;layer.<br>
Returns:&nbsp;None</span></dd></dl>

<hr>
Data descriptors defined here:<br>
<dl><dt><strong>__dict__</strong></dt>
<dd><span class="code">dictionary&nbsp;for&nbsp;instance&nbsp;variables</span></dd>
</dl>
<dl><dt><strong>__weakref__</strong></dt>
<dd><span class="code">list&nbsp;of&nbsp;weak&nbsp;references&nbsp;to&nbsp;the&nbsp;object</span></dd>
</dl>
</td></tr></table> <p>
<table class="section">
<tr class="decor title-decor heading-text">
<td class="section-title" colspan=3>&nbsp;<br><a name="lr_scheduler_exp">class <strong>lr_scheduler_exp</strong></a>(<a href="builtins.html#object">builtins.object</a>)</td></tr>
    
<tr><td class="decor title-decor" rowspan=2><span class="code">&nbsp;&nbsp;&nbsp;</span></td>
<td class="decor title-decor" colspan=2><span class="code"><a href="#lr_scheduler_exp">lr_scheduler_exp</a>(optimizer,&nbsp;lr_decay=0.1,&nbsp;lr_decay_epoch=10)<br>
&nbsp;<br>
Learning&nbsp;rate&nbsp;scheduler&nbsp;class&nbsp;for&nbsp;training&nbsp;neural&nbsp;networks.<br>
Reduces&nbsp;the&nbsp;learning&nbsp;rate&nbsp;exponentially&nbsp;by&nbsp;lr_decay&nbsp;every&nbsp;lr_decay_epoch&nbsp;epochs.<br>&nbsp;</span></td></tr>
<tr><td>&nbsp;</td>
<td class="singlecolumn">Methods defined here:<br>
<dl><dt><a name="lr_scheduler_exp-__init__"><strong>__init__</strong></a>(self, optimizer, lr_decay=0.1, lr_decay_epoch=10)</dt><dd><span class="code">Initialize&nbsp;self.&nbsp;&nbsp;See&nbsp;help(type(self))&nbsp;for&nbsp;accurate&nbsp;signature.</span></dd></dl>

<dl><dt><a name="lr_scheduler_exp-__repr__"><strong>__repr__</strong></a>(self)</dt><dd><span class="code">Return&nbsp;repr(self).</span></dd></dl>

<dl><dt><a name="lr_scheduler_exp-reduce"><strong>reduce</strong></a>(self)</dt></dl>

<dl><dt><a name="lr_scheduler_exp-step"><strong>step</strong></a>(self, epoch)</dt><dd><span class="code">Adjusts&nbsp;the&nbsp;learning&nbsp;rate&nbsp;based&nbsp;on&nbsp;the&nbsp;current&nbsp;epoch.&nbsp;Decays&nbsp;the&nbsp;learning&nbsp;rate&nbsp;by&nbsp;lr_decay&nbsp;every&nbsp;lr_decay_epoch&nbsp;epochs.<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;epoch&nbsp;(int):&nbsp;The&nbsp;current&nbsp;epoch&nbsp;number.<br>
Returns:&nbsp;None</span></dd></dl>

<hr>
Data descriptors defined here:<br>
<dl><dt><strong>__dict__</strong></dt>
<dd><span class="code">dictionary&nbsp;for&nbsp;instance&nbsp;variables</span></dd>
</dl>
<dl><dt><strong>__weakref__</strong></dt>
<dd><span class="code">list&nbsp;of&nbsp;weak&nbsp;references&nbsp;to&nbsp;the&nbsp;object</span></dd>
</dl>
</td></tr></table> <p>
<table class="section">
<tr class="decor title-decor heading-text">
<td class="section-title" colspan=3>&nbsp;<br><a name="lr_scheduler_plateau">class <strong>lr_scheduler_plateau</strong></a>(<a href="builtins.html#object">builtins.object</a>)</td></tr>
    
<tr><td class="decor title-decor" rowspan=2><span class="code">&nbsp;&nbsp;&nbsp;</span></td>
<td class="decor title-decor" colspan=2><span class="code"><a href="#lr_scheduler_plateau">lr_scheduler_plateau</a>(lr_scheduler,&nbsp;patience=5,&nbsp;threshold=0.01)<br>
&nbsp;<br>
A&nbsp;custom&nbsp;learning&nbsp;rate&nbsp;scheduler&nbsp;that&nbsp;adjusts&nbsp;the&nbsp;learning&nbsp;rate&nbsp;based&nbsp;on&nbsp;the&nbsp;plateau&nbsp;of&nbsp;the&nbsp;loss&nbsp;function.<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;lr_scheduler&nbsp;(<a href="builtins.html#object">object</a>):&nbsp;The&nbsp;learning&nbsp;rate&nbsp;scheduler&nbsp;<a href="builtins.html#object">object</a>.<br>
&nbsp;&nbsp;&nbsp;&nbsp;patience&nbsp;(int):&nbsp;The&nbsp;number&nbsp;of&nbsp;epochs&nbsp;to&nbsp;wait&nbsp;for&nbsp;improvement&nbsp;before&nbsp;reducing&nbsp;the&nbsp;learning&nbsp;rate.&nbsp;Default&nbsp;is&nbsp;5.<br>
&nbsp;&nbsp;&nbsp;&nbsp;threshold&nbsp;(float):&nbsp;The&nbsp;minimum&nbsp;improvement&nbsp;threshold&nbsp;required&nbsp;to&nbsp;update&nbsp;the&nbsp;best&nbsp;loss.&nbsp;Default&nbsp;is&nbsp;0.01.<br>
Methods:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#lr_scheduler_plateau-step">step</a>(loss):&nbsp;Updates&nbsp;the&nbsp;learning&nbsp;rate&nbsp;based&nbsp;on&nbsp;the&nbsp;loss&nbsp;value.<br>&nbsp;</span></td></tr>
<tr><td>&nbsp;</td>
<td class="singlecolumn">Methods defined here:<br>
<dl><dt><a name="lr_scheduler_plateau-__init__"><strong>__init__</strong></a>(self, lr_scheduler, patience=5, threshold=0.01)</dt><dd><span class="code">Initialize&nbsp;self.&nbsp;&nbsp;See&nbsp;help(type(self))&nbsp;for&nbsp;accurate&nbsp;signature.</span></dd></dl>

<dl><dt><a name="lr_scheduler_plateau-__repr__"><strong>__repr__</strong></a>(self)</dt><dd><span class="code">Return&nbsp;repr(self).</span></dd></dl>

<dl><dt><a name="lr_scheduler_plateau-step"><strong>step</strong></a>(self, epoch, loss)</dt><dd><span class="code">Updates&nbsp;the&nbsp;learning&nbsp;rate&nbsp;based&nbsp;on&nbsp;the&nbsp;loss&nbsp;value.<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;loss&nbsp;(float):&nbsp;The&nbsp;current&nbsp;loss&nbsp;value.</span></dd></dl>

<hr>
Data descriptors defined here:<br>
<dl><dt><strong>__dict__</strong></dt>
<dd><span class="code">dictionary&nbsp;for&nbsp;instance&nbsp;variables</span></dd>
</dl>
<dl><dt><strong>__weakref__</strong></dt>
<dd><span class="code">list&nbsp;of&nbsp;weak&nbsp;references&nbsp;to&nbsp;the&nbsp;object</span></dd>
</dl>
</td></tr></table> <p>
<table class="section">
<tr class="decor title-decor heading-text">
<td class="section-title" colspan=3>&nbsp;<br><a name="lr_scheduler_step">class <strong>lr_scheduler_step</strong></a>(<a href="builtins.html#object">builtins.object</a>)</td></tr>
    
<tr><td class="decor title-decor" rowspan=2><span class="code">&nbsp;&nbsp;&nbsp;</span></td>
<td class="decor title-decor" colspan=2><span class="code"><a href="#lr_scheduler_step">lr_scheduler_step</a>(optimizer,&nbsp;lr_decay=0.1,&nbsp;lr_decay_epoch=10)<br>
&nbsp;<br>
Learning&nbsp;rate&nbsp;scheduler&nbsp;class&nbsp;for&nbsp;training&nbsp;neural&nbsp;networks.<br>
Reduces&nbsp;the&nbsp;learning&nbsp;rate&nbsp;by&nbsp;a&nbsp;factor&nbsp;of&nbsp;lr_decay&nbsp;every&nbsp;lr_decay_epoch&nbsp;epochs.<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;optimizer&nbsp;(Optimizer):&nbsp;The&nbsp;optimizer&nbsp;to&nbsp;adjust&nbsp;the&nbsp;learning&nbsp;rate&nbsp;for.<br>
&nbsp;&nbsp;&nbsp;&nbsp;lr_decay&nbsp;(float,&nbsp;optional):&nbsp;The&nbsp;factor&nbsp;to&nbsp;reduce&nbsp;the&nbsp;learning&nbsp;rate&nbsp;by.&nbsp;Defaults&nbsp;to&nbsp;0.1.<br>
&nbsp;&nbsp;&nbsp;&nbsp;lr_decay_epoch&nbsp;(int,&nbsp;optional):&nbsp;The&nbsp;number&nbsp;of&nbsp;epochs&nbsp;to&nbsp;wait&nbsp;before&nbsp;decaying&nbsp;the&nbsp;learning&nbsp;rate.&nbsp;Defaults&nbsp;to&nbsp;10<br>&nbsp;</span></td></tr>
<tr><td>&nbsp;</td>
<td class="singlecolumn">Methods defined here:<br>
<dl><dt><a name="lr_scheduler_step-__init__"><strong>__init__</strong></a>(self, optimizer, lr_decay=0.1, lr_decay_epoch=10)</dt><dd><span class="code">Initialize&nbsp;self.&nbsp;&nbsp;See&nbsp;help(type(self))&nbsp;for&nbsp;accurate&nbsp;signature.</span></dd></dl>

<dl><dt><a name="lr_scheduler_step-__repr__"><strong>__repr__</strong></a>(self)</dt><dd><span class="code">Return&nbsp;repr(self).</span></dd></dl>

<dl><dt><a name="lr_scheduler_step-reduce"><strong>reduce</strong></a>(self)</dt></dl>

<dl><dt><a name="lr_scheduler_step-step"><strong>step</strong></a>(self, epoch)</dt><dd><span class="code">Adjusts&nbsp;the&nbsp;learning&nbsp;rate&nbsp;based&nbsp;on&nbsp;the&nbsp;current&nbsp;epoch.&nbsp;Decays&nbsp;the&nbsp;learning&nbsp;rate&nbsp;by&nbsp;lr_decay&nbsp;every&nbsp;lr_decay_epoch&nbsp;epochs.<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;epoch&nbsp;(int):&nbsp;The&nbsp;current&nbsp;epoch&nbsp;number.<br>
Returns:&nbsp;None</span></dd></dl>

<hr>
Data descriptors defined here:<br>
<dl><dt><strong>__dict__</strong></dt>
<dd><span class="code">dictionary&nbsp;for&nbsp;instance&nbsp;variables</span></dd>
</dl>
<dl><dt><strong>__weakref__</strong></dt>
<dd><span class="code">list&nbsp;of&nbsp;weak&nbsp;references&nbsp;to&nbsp;the&nbsp;object</span></dd>
</dl>
</td></tr></table></td></tr></table><p>
<table class="section">
<tr class="decor data-decor heading-text">
<td class="section-title" colspan=3>&nbsp;<br><strong class="bigsection">Data</strong></td></tr>
    
<tr><td class="decor data-decor"><span class="code">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span></td><td>&nbsp;</td>
<td class="singlecolumn"><strong>__all__</strong> = ['AdamOptimizer', 'SGDOptimizer', 'AdadeltaOptimizer', 'lr_scheduler_exp', 'lr_scheduler_plateau', 'lr_scheduler_step', 'CrossEntropyLoss', 'BCEWithLogitsLoss', 'DenseLayer', 'FlattenLayer', 'ConvLayer', 'RNNLayer', 'Activation', 'NeuralNetworkBase', 'BaseBackendNeuralNetwork', 'JITAdamOptimizer', 'JITSGDOptimizer', 'JITAdadeltaOptimizer', 'JITBCEWithLogitsLoss', 'JITCrossEntropyLoss', ...]</td></tr></table>
</body></html>
