{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from sega_learn.neural_networks import *\n",
    "from sega_learn.neural_networks.numba_utils import (\n",
    "    relu as relu_nb, \n",
    "    relu_derivative as relu_derivative_nb,\n",
    "    leaky_relu as leaky_relu_nb,\n",
    "    leaky_relu_derivative as leaky_relu_derivative_nb,\n",
    "    tanh as tanh_nb,\n",
    "    tanh_derivative as tanh_derivative_nb,\n",
    "    sigmoid as sigmoid_nb,\n",
    "    sigmoid_derivative as sigmoid_derivative_nb,\n",
    "    softmax as softmax_nb,\n",
    ")\n",
    "\n",
    "from sega_learn.neural_networks.numba_utils import sum_axis0, sum_reduce\n",
    "from sega_learn.neural_networks.numba_utils import apply_dropout_jit\n",
    "from sega_learn.neural_networks.numba_utils import compute_l2_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_outputs(func1, func2, *args):\n",
    "    output1 = func1(*args)\n",
    "    output2 = func2(*args)\n",
    "    \n",
    "    tolerance = 1e-7\n",
    "    if np.allclose(output1, output2, atol=tolerance):\n",
    "        # print(f\"{func1.__name__} and {func2.__name__} outputs within tolerance of {tolerance}.\")\n",
    "        pass\n",
    "    else:\n",
    "        fail = True\n",
    "        print(f\"\\n{func1.__name__} and {func2.__name__} outputs are not within tolerance of {tolerance}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare Activation and Activation Derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.random.randn(1000, 2)\n",
    "\n",
    "# ReLU and ReLU Derivative\n",
    "compare_outputs(Activation.relu, relu_nb, z)\n",
    "compare_outputs(Activation.relu_derivative, relu_derivative_nb, z)\n",
    "\n",
    "# Leaky ReLU and Leaky ReLU Derivative\n",
    "compare_outputs(Activation.leaky_relu, leaky_relu_nb, z)\n",
    "compare_outputs(Activation.leaky_relu_derivative, leaky_relu_derivative_nb, z)\n",
    "\n",
    "# Tanh and Tanh Derivative\n",
    "compare_outputs(Activation.tanh, tanh_nb, z)\n",
    "compare_outputs(Activation.tanh_derivative, tanh_derivative_nb, z)\n",
    "\n",
    "# Sigmoid and Sigmoid Derivative\n",
    "compare_outputs(Activation.sigmoid, sigmoid_nb, z)\n",
    "compare_outputs(Activation.sigmoid_derivative, sigmoid_derivative_nb, z)\n",
    "\n",
    "# Softmax\n",
    "compare_outputs(Activation.softmax, softmax_nb, z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare JIT utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results match!\n"
     ]
    }
   ],
   "source": [
    "# Generate random data\n",
    "X = np.random.randn(1000, 1000)\n",
    "\n",
    "# Result for np.sum\n",
    "np_sum_result = np.sum(X, axis=0, keepdims=True)\n",
    "\n",
    "# Result for sum_axis0\n",
    "sum_axis0_result = sum_axis0(X)\n",
    "\n",
    "# Verify that results are the same\n",
    "tolerance = 1e-6\n",
    "if np.allclose(np_sum_result, sum_axis0_result, atol=tolerance):\n",
    "    print(\"Results match!\")\n",
    "else:\n",
    "    print(\"Results do not match!\")\n",
    "    diff_index = np.where(np.abs(np_sum_result - sum_axis0_result) > tolerance)[0][0]\n",
    "    print(f\"Difference found at index {diff_index}: {np_sum_result[0, diff_index]} vs {sum_axis0_result[diff_index]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sega_learn.neural_networks.loss import CrossEntropyLoss, BCEWithLogitsLoss\n",
    "from sega_learn.neural_networks.loss_jit import JITCrossEntropyLoss, JITBCEWithLogitsLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Entropy Loss Comparison:\n",
      "---------------------------------------------------------------------------\n",
      "Losses are equal to within tolerance of 1e-07\n",
      "Base Loss     : 1.2011396407224744\n",
      "JIT Loss      : 1.201139640722479\n",
      "Difference    : 4.6629367034256575e-15\n"
     ]
    }
   ],
   "source": [
    "# Compare Cross Entropy Loss\n",
    "\n",
    "# Generate some dummy data for multi-class classification\n",
    "n_samples, n_classes = 5, 3\n",
    "\n",
    "logits_ce = np.random.randn(n_samples, n_classes)\n",
    "\n",
    "# Generate integer targets and convert to one-hot\n",
    "targets_int = np.random.randint(0, n_classes, size=n_samples)\n",
    "targets_onehot = np.eye(n_classes)[targets_int]\n",
    "\n",
    "# Instantiate loss function objects\n",
    "base_ce_loss = CrossEntropyLoss()\n",
    "jit_ce_loss = JITCrossEntropyLoss()\n",
    "\n",
    "# Calculate losses\n",
    "loss_base_ce = base_ce_loss(logits_ce, targets_onehot)\n",
    "loss_jit_ce = jit_ce_loss.calculate_loss(logits_ce, targets_onehot)\n",
    "\n",
    "tolerance = 1e-7\n",
    "print(\"Cross Entropy Loss Comparison:\")\n",
    "print(\"-\"*75)\n",
    "if np.allclose(loss_base_ce, loss_jit_ce, atol=tolerance):\n",
    "    print(\"Losses are equal to within tolerance of\", tolerance)\n",
    "print(\"Base Loss     :\", loss_base_ce)\n",
    "print(\"JIT Loss      :\", loss_jit_ce)\n",
    "print(\"Difference    :\", abs(loss_base_ce - loss_jit_ce))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BCE With Logits Loss Comparison:\n",
      "--------------------------------------------------\n",
      "Losses are equal to within tolerance of 1e-07\n",
      "Base Loss     : 0.6638989523377259\n",
      "JIT Loss      : 0.6638989523377259\n",
      "Difference    : 0.0\n"
     ]
    }
   ],
   "source": [
    "# Compare Binary Cross Entropy Loss\n",
    "\n",
    "# Generate some dummy data for binary classification\n",
    "n_samples_bce = 10\n",
    "logits_bce = np.random.randn(n_samples_bce)\n",
    "\n",
    "# Generate binary targets (0 or 1)\n",
    "targets_bce = np.random.randint(0, 2, size=n_samples_bce)\n",
    "\n",
    "# Instantiate loss function objects\n",
    "base_bce_loss = BCEWithLogitsLoss()\n",
    "jit_bce_loss = JITBCEWithLogitsLoss()\n",
    "\n",
    "# Calculate losses\n",
    "loss_base_bce = base_bce_loss(logits_bce, targets_bce)\n",
    "loss_jit_bce = jit_bce_loss.calculate_loss(logits_bce, targets_bce)\n",
    "\n",
    "\n",
    "tolerance = 1e-7\n",
    "print(\"\\nBCE With Logits Loss Comparison:\")\n",
    "print(\"-\"*50)\n",
    "if np.allclose(loss_base_bce, loss_jit_bce, atol=tolerance):\n",
    "    print(\"Losses are equal to within tolerance of\", tolerance)\n",
    "\n",
    "print(\"Base Loss     :\", loss_base_bce)\n",
    "print(\"JIT Loss      :\", loss_jit_bce)\n",
    "print(\"Difference    :\", abs(loss_base_bce - loss_jit_bce))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Apply Dropout Comparison: 10,000 trials\n",
      "--------------------------------------------------\n",
      "Base Count : 2499.86 ± 35.43\n",
      "JIT Count  : 2500.28 ± 35.30\n"
     ]
    }
   ],
   "source": [
    "# Compare apply_dropout to apply_dropout_jit\n",
    "\n",
    "layer_sizes = [10, 5, 3, 2]\n",
    "dropout_rate = 0.5\n",
    "reg_lambda = 0\n",
    "activations = ['relu', 'relu', 'softmax']\n",
    "# Initialize neural networks\n",
    "nn = BaseBackendNeuralNetwork(layer_sizes, dropout_rate, reg_lambda, activations)\n",
    "\n",
    "counts_base = []\n",
    "counts_jit = []\n",
    "n_trials = 10_000\n",
    "for i in range(n_trials):\n",
    "    # Generate random data\n",
    "    X = np.random.randn(1000, 5)\n",
    "\n",
    "    # Apply dropout\n",
    "    X_dropout = nn.apply_dropout(X)\n",
    "    X_dropout_jit = apply_dropout_jit(X, dropout_rate)\n",
    "\n",
    "    # Count the number of non-zero elements in each array\n",
    "    count_base = np.count_nonzero(X_dropout)\n",
    "    count_jit = np.count_nonzero(X_dropout_jit)\n",
    "\n",
    "    counts_base.append(count_base)\n",
    "    counts_jit.append(count_jit)\n",
    "\n",
    "avg_count_base = np.mean(counts_base)\n",
    "avg_count_jit = np.mean(counts_jit)\n",
    "std_dev_base = np.std(counts_base)\n",
    "std_dev_jit = np.std(counts_jit)\n",
    "\n",
    "print(f\"\\nApply Dropout Comparison: {n_trials:,} trials\")\n",
    "print(\"-\"*50)\n",
    "print(f\"Base Count : {avg_count_base:.2f} ± {std_dev_base:.2f}\")\n",
    "print(f\"JIT Count  : {avg_count_jit:.2f} ± {std_dev_jit:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare L2 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "L2 Regularization Comparison:\n",
      "-----------------------------------\n",
      "Base L2 : 236.52\n",
      "JIT L2  : 236.52\n"
     ]
    }
   ],
   "source": [
    "weights = [np.random.randn(5, 5) for _ in range(10)]\n",
    "biases = [np.random.randn(5) for _ in range(10)]\n",
    "activations = ['relu' for _ in range(10)]\n",
    "layer_sizes = [5] * 10\n",
    "dropout_rate = 0.5\n",
    "reg_lambda = 0\n",
    "nn = BaseBackendNeuralNetwork(layer_sizes, dropout_rate, reg_lambda, activations)\n",
    "\n",
    "\n",
    "l2_base = nn.compute_l2_reg(weights)\n",
    "l2_jit = compute_l2_reg(weights) \n",
    "\n",
    "print(f\"\\nL2 Regularization Comparison:\")\n",
    "print(\"-\"*35)\n",
    "print(f\"Base L2 : {l2_base:.2f}\")\n",
    "print(f\"JIT L2  : {l2_jit:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sega_learn.neural_networks.optimizers import AdamOptimizer, SGDOptimizer, AdadeltaOptimizer\n",
    "from sega_learn.neural_networks.optimizers_jit import JITAdamOptimizer, JITSGDOptimizer, JITAdadeltaOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 5\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 weights and biases are within tolerance: 1e-07\n",
      "Layer 1 weights and biases are within tolerance: 1e-07\n",
      "Layer 2 weights and biases are within tolerance: 1e-07\n",
      "Layer 3 weights and biases are within tolerance: 1e-07\n",
      "Layer 4 weights and biases are within tolerance: 1e-07\n"
     ]
    }
   ],
   "source": [
    "# Compare Adam Optimizer\n",
    "lr = 0.01\n",
    "beta1 = 0.5\n",
    "beta2 = 0.9\n",
    "epsilon = 1e-5\n",
    "reg_lambda = 0.01\n",
    "activations = ['relu' for _ in range(num_layers)]\n",
    "\n",
    "base_layers = []\n",
    "jit_layers = []\n",
    "for i in range(num_layers):\n",
    "    base_layers.append(Layer(3,3,activations[i]))\n",
    "    jit_layers.append(JITLayer(3,3,activations[i]))\n",
    "\n",
    "# Initialize optimizer objects\n",
    "adam_base = AdamOptimizer(lr, beta1, beta2, epsilon, reg_lambda)\n",
    "adam_jit = JITAdamOptimizer(lr, beta1, beta2, epsilon, reg_lambda)\n",
    "\n",
    "adam_base.initialize(base_layers)\n",
    "adam_jit.initialize(jit_layers)\n",
    "\n",
    "# Assert that the optimizer states are the same (m, v, t)\n",
    "for i in range(num_layers):\n",
    "    assert np.allclose(adam_base.m, adam_jit.m)\n",
    "    assert np.allclose(adam_base.v, adam_jit.v)\n",
    "    assert adam_base.t == adam_jit.t\n",
    "\n",
    "# Set layer weights to the same values (initialized randomly)\n",
    "for i in range(num_layers):\n",
    "    jit_layers[i].weights = base_layers[i].weights\n",
    "    \n",
    "\n",
    "# Update optimizer\n",
    "dW = [np.random.randn(3,3) for _ in range(num_layers)]\n",
    "db = [np.random.randn(3) for _ in range(num_layers)]\n",
    "for i in range(num_layers):\n",
    "    adam_base.update(base_layers[i], dW[i], db[i], i)\n",
    "    # adam_jit.update(jit_layers[i], dW[i], db[i], i)\n",
    "adam_jit.update_layers(jit_layers, dW, db)\n",
    "\n",
    "tolerance = 1e-7\n",
    "for i in range(num_layers):\n",
    "    assert np.allclose(adam_base.m, adam_jit.m)\n",
    "    assert np.allclose(adam_base.v, adam_jit.v)\n",
    "    assert adam_base.t == adam_jit.t\n",
    "\n",
    "    weights_close = np.allclose(base_layers[i].weights, jit_layers[i].weights, atol=tolerance)\n",
    "    if not weights_close:\n",
    "        print(f\"\\nLayer {i} weights are not close.\")\n",
    "        # Find the index of the first element that is not close\n",
    "        diff_index = np.where(np.abs(base_layers[i].weights - jit_layers[i].weights) > 1e-7)[0][0]\n",
    "        print(f\"Difference found at index {diff_index}: \\n\\t{base_layers[i].weights[:, diff_index]} vs \\n\\t{jit_layers[i].weights[:, diff_index]}\")\n",
    "    \n",
    "    biases_close = np.allclose(base_layers[i].biases, jit_layers[i].biases, atol=tolerance)\n",
    "    if not biases_close:\n",
    "        print(f\"\\nLayer {i} biases are not close.\")\n",
    "        # Find the index of the first element that is not close\n",
    "        diff_index = np.where(np.abs(base_layers[i].biases - jit_layers[i].biases) > 1e-7)[0][0]\n",
    "        print(f\"Difference found at index {diff_index}: \\n\\t{base_layers[i].biases[diff_index]} vs \\n\\t{jit_layers[i].biases[diff_index]}\")\n",
    "        \n",
    "    assert weights_close\n",
    "    assert biases_close\n",
    "    \n",
    "    print(f\"Layer {i} weights and biases are within tolerance: {tolerance}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 weights and biases are within tolerance: 1e-07\n",
      "Layer 1 weights and biases are within tolerance: 1e-07\n",
      "Layer 2 weights and biases are within tolerance: 1e-07\n",
      "Layer 3 weights and biases are within tolerance: 1e-07\n",
      "Layer 4 weights and biases are within tolerance: 1e-07\n"
     ]
    }
   ],
   "source": [
    "# Compare SGD Optimizer\n",
    "lr = 0.01\n",
    "momentum = 0.9\n",
    "reg_lambda = 0.01\n",
    "activations = ['relu' for _ in range(num_layers)]\n",
    "\n",
    "base_layers = []\n",
    "jit_layers = []\n",
    "for i in range(num_layers):\n",
    "    base_layers.append(Layer(3, 3, activations[i]))\n",
    "    jit_layers.append(JITLayer(3, 3, activations[i]))\n",
    "\n",
    "# Initialize optimizer objects\n",
    "sgd_base = SGDOptimizer(lr, momentum, reg_lambda)\n",
    "sgd_jit = JITSGDOptimizer(lr, momentum, reg_lambda)\n",
    "\n",
    "sgd_base.initialize(base_layers)\n",
    "sgd_jit.initialize(jit_layers)\n",
    "\n",
    "# Assert that the optimizer states are the same (velocity)\n",
    "for i in range(num_layers):\n",
    "    assert np.allclose(sgd_base.velocity, sgd_jit.velocity)\n",
    "\n",
    "# Set layer weights to the same values (initialized randomly)\n",
    "for i in range(num_layers):\n",
    "    jit_layers[i].weights = base_layers[i].weights\n",
    "\n",
    "# Update optimizer\n",
    "dW = [np.random.randn(3, 3) for _ in range(num_layers)]\n",
    "db = [np.random.randn(3) for _ in range(num_layers)]\n",
    "for i in range(num_layers):\n",
    "    sgd_base.update(base_layers[i], dW[i], db[i], i)\n",
    "sgd_jit.update_layers(jit_layers, dW, db)\n",
    "\n",
    "tolerance = 1e-7\n",
    "for i in range(num_layers):\n",
    "    assert np.allclose(sgd_base.velocity, sgd_jit.velocity)\n",
    "\n",
    "    weights_close = np.allclose(base_layers[i].weights, jit_layers[i].weights, atol=tolerance)\n",
    "    if not weights_close:\n",
    "        print(f\"\\nLayer {i} weights are not close.\")\n",
    "        diff_index = np.where(np.abs(base_layers[i].weights - jit_layers[i].weights) > 1e-7)[0][0]\n",
    "        print(f\"Difference found at index {diff_index}: \\n\\t{base_layers[i].weights[:, diff_index]} vs \\n\\t{jit_layers[i].weights[:, diff_index]}\")\n",
    "\n",
    "    biases_close = np.allclose(base_layers[i].biases, jit_layers[i].biases, atol=tolerance)\n",
    "    if not biases_close:\n",
    "        print(f\"\\nLayer {i} biases are not close.\")\n",
    "        diff_index = np.where(np.abs(base_layers[i].biases - jit_layers[i].biases) > 1e-7)[0][0]\n",
    "        print(f\"Difference found at index {diff_index}: \\n\\t{base_layers[i].biases[diff_index]} vs \\n\\t{jit_layers[i].biases[diff_index]}\")\n",
    "\n",
    "    assert weights_close\n",
    "    assert biases_close\n",
    "\n",
    "    print(f\"Layer {i} weights and biases are within tolerance: {tolerance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 weights and biases are within tolerance: 1e-07\n",
      "Layer 1 weights and biases are within tolerance: 1e-07\n",
      "Layer 2 weights and biases are within tolerance: 1e-07\n",
      "Layer 3 weights and biases are within tolerance: 1e-07\n",
      "Layer 4 weights and biases are within tolerance: 1e-07\n"
     ]
    }
   ],
   "source": [
    "# Compare Adadelta Optimizer\n",
    "lr = 1.0\n",
    "rho = 0.95\n",
    "epsilon = 1e-6\n",
    "reg_lambda = 0.01\n",
    "activations = ['relu' for _ in range(num_layers)]\n",
    "\n",
    "base_layers = []\n",
    "jit_layers = []\n",
    "for i in range(num_layers):\n",
    "    base_layers.append(Layer(3, 3, activations[i]))\n",
    "    jit_layers.append(JITLayer(3, 3, activations[i]))\n",
    "\n",
    "# Initialize optimizer objects\n",
    "adadelta_base = AdadeltaOptimizer(lr, rho, epsilon, reg_lambda)\n",
    "adadelta_jit = JITAdadeltaOptimizer(lr, rho, epsilon, reg_lambda)\n",
    "\n",
    "adadelta_base.initialize(base_layers)\n",
    "adadelta_jit.initialize(jit_layers)\n",
    "\n",
    "# Assert that the optimizer states are the same (E_g2, E_delta_x2)\n",
    "for i in range(num_layers):\n",
    "    assert np.allclose(adadelta_base.E_g2, adadelta_jit.E_g2)\n",
    "    assert np.allclose(adadelta_base.E_delta_x2, adadelta_jit.E_delta_x2)\n",
    "\n",
    "# Set layer weights to the same values (initialized randomly)\n",
    "for i in range(num_layers):\n",
    "    jit_layers[i].weights = base_layers[i].weights\n",
    "\n",
    "# Update optimizer\n",
    "dW = [np.random.randn(3, 3) for _ in range(num_layers)]\n",
    "db = [np.random.randn(3) for _ in range(num_layers)]\n",
    "for i in range(num_layers):\n",
    "    adadelta_base.update(base_layers[i], dW[i], db[i], i)\n",
    "adadelta_jit.update_layers(jit_layers, dW, db)\n",
    "\n",
    "tolerance = 1e-7\n",
    "for i in range(num_layers):\n",
    "    assert np.allclose(adadelta_base.E_g2, adadelta_jit.E_g2)\n",
    "    assert np.allclose(adadelta_base.E_delta_x2, adadelta_jit.E_delta_x2)\n",
    "\n",
    "    weights_close = np.allclose(base_layers[i].weights, jit_layers[i].weights, atol=tolerance)\n",
    "    if not weights_close:\n",
    "        print(f\"\\nLayer {i} weights are not close.\")\n",
    "        diff_index = np.where(np.abs(base_layers[i].weights - jit_layers[i].weights) > 1e-7)[0][0]\n",
    "        print(f\"Difference found at index {diff_index}: \\n\\t{base_layers[i].weights[:, diff_index]} vs \\n\\t{jit_layers[i].weights[:, diff_index]}\")\n",
    "\n",
    "    biases_close = np.allclose(base_layers[i].biases, jit_layers[i].biases, atol=tolerance)\n",
    "    if not biases_close:\n",
    "        print(f\"\\nLayer {i} biases are not close.\")\n",
    "        diff_index = np.where(np.abs(base_layers[i].biases - jit_layers[i].biases) > 1e-7)[0][0]\n",
    "        print(f\"Difference found at index {diff_index}: \\n\\t{base_layers[i].biases[diff_index]} vs \\n\\t{jit_layers[i].biases[diff_index]}\")\n",
    "\n",
    "    assert weights_close\n",
    "    assert biases_close\n",
    "\n",
    "    print(f\"Layer {i} weights and biases are within tolerance: {tolerance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sega_learn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
