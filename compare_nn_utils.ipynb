{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from sega_learn.neural_networks.neuralNetwork import *\n",
    "from sega_learn.neural_networks.numba_utils import (\n",
    "    relu as relu_nb, \n",
    "    relu_derivative as relu_derivative_nb,\n",
    "    leaky_relu as leaky_relu_nb,\n",
    "    leaky_relu_derivative as leaky_relu_derivative_nb,\n",
    "    tanh as tanh_nb,\n",
    "    tanh_derivative as tanh_derivative_nb,\n",
    "    sigmoid as sigmoid_nb,\n",
    "    sigmoid_derivative as sigmoid_derivative_nb,\n",
    "    softmax as softmax_nb,\n",
    ")\n",
    "\n",
    "from sega_learn.neural_networks.numba_utils import sum_axis0, sum_reduce\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_outputs(func1, func2, *args):\n",
    "    output1 = func1(*args)\n",
    "    output2 = func2(*args)\n",
    "    \n",
    "    tolerance = 1e-7\n",
    "    if np.allclose(output1, output2, atol=tolerance):\n",
    "        # print(f\"{func1.__name__} and {func2.__name__} outputs within tolerance of {tolerance}.\")\n",
    "        pass\n",
    "    else:\n",
    "        fail = True\n",
    "        print(f\"\\n{func1.__name__} and {func2.__name__} outputs are not within tolerance of {tolerance}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare Activation and Activation Derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.random.randn(1000, 2)\n",
    "\n",
    "# ReLU and ReLU Derivative\n",
    "compare_outputs(Activation.relu, relu_nb, z)\n",
    "compare_outputs(Activation.relu_derivative, relu_derivative_nb, z)\n",
    "\n",
    "# Leaky ReLU and Leaky ReLU Derivative\n",
    "compare_outputs(Activation.leaky_relu, leaky_relu_nb, z)\n",
    "compare_outputs(Activation.leaky_relu_derivative, leaky_relu_derivative_nb, z)\n",
    "\n",
    "# Tanh and Tanh Derivative\n",
    "compare_outputs(Activation.tanh, tanh_nb, z)\n",
    "compare_outputs(Activation.tanh_derivative, tanh_derivative_nb, z)\n",
    "\n",
    "# Sigmoid and Sigmoid Derivative\n",
    "compare_outputs(Activation.sigmoid, sigmoid_nb, z)\n",
    "compare_outputs(Activation.sigmoid_derivative, sigmoid_derivative_nb, z)\n",
    "\n",
    "# Softmax\n",
    "compare_outputs(Activation.softmax, softmax_nb, z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results match!\n"
     ]
    }
   ],
   "source": [
    "# Generate random data\n",
    "X = np.random.randn(1000, 1000)\n",
    "\n",
    "# Result for np.sum\n",
    "np_sum_result = np.sum(X, axis=0, keepdims=True)\n",
    "\n",
    "# Result for sum_axis0\n",
    "sum_axis0_result = sum_axis0(X)\n",
    "\n",
    "# Verify that results are the same\n",
    "tolerance = 1e-6\n",
    "if np.allclose(np_sum_result, sum_axis0_result, atol=tolerance):\n",
    "    print(\"Results match!\")\n",
    "else:\n",
    "    print(\"Results do not match!\")\n",
    "    diff_index = np.where(np.abs(np_sum_result - sum_axis0_result) > tolerance)[0][0]\n",
    "    print(f\"Difference found at index {diff_index}: {np_sum_result[0, diff_index]} vs {sum_axis0_result[diff_index]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the loss functions from your modules\n",
    "from sega_learn.neural_networks.loss import CrossEntropyLoss, BCEWithLogitsLoss\n",
    "from sega_learn.neural_networks.loss_jit import JITCrossEntropyLoss, JITBCEWithLogitsLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Entropy Loss Comparison:\n",
      "---------------------------------------------------------------------------\n",
      "Losses are equal to within tolerance of 1e-07\n",
      "Base Loss     : 1.3912039674724226\n",
      "JIT Loss      : 1.391203967472429\n",
      "Difference    : 6.439293542825908e-15\n"
     ]
    }
   ],
   "source": [
    "# Compare Cross Entropy Loss\n",
    "\n",
    "# Generate some dummy data for multi-class classification\n",
    "n_samples, n_classes = 5, 3\n",
    "\n",
    "logits_ce = np.random.randn(n_samples, n_classes)\n",
    "\n",
    "# Generate integer targets and convert to one-hot\n",
    "targets_int = np.random.randint(0, n_classes, size=n_samples)\n",
    "targets_onehot = np.eye(n_classes)[targets_int]\n",
    "\n",
    "# Instantiate loss function objects\n",
    "base_ce_loss = CrossEntropyLoss()\n",
    "jit_ce_loss = JITCrossEntropyLoss()\n",
    "\n",
    "# Calculate losses\n",
    "loss_base_ce = base_ce_loss(logits_ce, targets_onehot)\n",
    "loss_jit_ce = jit_ce_loss.calculate_loss(logits_ce, targets_onehot)\n",
    "\n",
    "tolerance = 1e-7\n",
    "print(\"Cross Entropy Loss Comparison:\")\n",
    "print(\"-\"*75)\n",
    "if np.allclose(loss_base_ce, loss_jit_ce, atol=tolerance):\n",
    "    print(\"Losses are equal to within tolerance of\", tolerance)\n",
    "print(\"Base Loss     :\", loss_base_ce)\n",
    "print(\"JIT Loss      :\", loss_jit_ce)\n",
    "print(\"Difference    :\", abs(loss_base_ce - loss_jit_ce))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BCE With Logits Loss Comparison:\n",
      "--------------------------------------------------\n",
      "Losses are equal to within tolerance of 1e-07\n",
      "Base Loss     : 0.5771172002684145\n",
      "JIT Loss      : 0.5771172002684144\n",
      "Difference    : 1.1102230246251565e-16\n"
     ]
    }
   ],
   "source": [
    "# Compare Binary Cross Entropy Loss\n",
    "\n",
    "# Generate some dummy data for binary classification\n",
    "n_samples_bce = 10\n",
    "logits_bce = np.random.randn(n_samples_bce)\n",
    "\n",
    "# Generate binary targets (0 or 1)\n",
    "targets_bce = np.random.randint(0, 2, size=n_samples_bce)\n",
    "\n",
    "# Instantiate loss function objects\n",
    "base_bce_loss = BCEWithLogitsLoss()\n",
    "jit_bce_loss = JITBCEWithLogitsLoss()\n",
    "\n",
    "# Calculate losses\n",
    "loss_base_bce = base_bce_loss(logits_bce, targets_bce)\n",
    "loss_jit_bce = jit_bce_loss.calculate_loss(logits_bce, targets_bce)\n",
    "\n",
    "\n",
    "tolerance = 1e-7\n",
    "print(\"\\nBCE With Logits Loss Comparison:\")\n",
    "print(\"-\"*50)\n",
    "if np.allclose(loss_base_bce, loss_jit_bce, atol=tolerance):\n",
    "    print(\"Losses are equal to within tolerance of\", tolerance)\n",
    "\n",
    "print(\"Base Loss     :\", loss_base_bce)\n",
    "print(\"JIT Loss      :\", loss_jit_bce)\n",
    "print(\"Difference    :\", abs(loss_base_bce - loss_jit_bce))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sega_learn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
