# Importing the required libraries
from math import log

import numpy as np
from scipy import linalg


class LinearDiscriminantAnalysis:
    """Implements Linear Discriminant Analysis.

    A classifier with a linear decision boundary, generated by fitting class conditional densities to the data and using Bayes' rule.

    Args:
        solver: (str) - {'svd', 'lsqr', 'eigen'}, default='svd'. Solver to use for the LDA.
        priors: (np.ndarray), optional - Prior probabilities of the classes (default is None).
    """

    def __init__(self, solver="svd", priors=None):
        """Initializes the Linear Discriminant Analysis model.

        Args:
            solver: (str) - {'svd', 'lsqr', 'eigen'}, default='svd'. Solver to use for the LDA.
            priors: (np.ndarray), optional - Prior probabilities of the classes (default is None).
        """
        self.solver = solver
        self.priors = priors

    def fit(self, X, y):
        """Fits the LDA model to the training data.

        Args:
            X: (np.ndarray) - Training feature data.
            y: (np.ndarray) - Training target data.
        """
        self.classes_ = np.unique(y)  # Unique classes, i.e., distinct class labels
        self.means_ = {}  # Mean of each feature per class
        self.covariance_ = np.cov(X, rowvar=False)  # Covariance matrix of all features
        if self.priors is None:  # Prior probabilities of each class
            self.priors_ = {}
        else:
            self.priors_ = self.priors

        # Compute mean and prior for each class, and covariance
        for cls in self.classes_:
            X_cls = X[y == cls]  # Data points corresponding to class cls
            self.means_[cls] = np.mean(X_cls, axis=0)  # Mean of each feature per class
            self.priors_[cls] = (
                X_cls.shape[0] / X.shape[0]
            )  # Prior probability of class cls

        if self.solver == "svd":
            self._fit_svd(X, y)
        elif self.solver == "lsqr":
            self._fit_lsqr(X, y)
        elif self.solver == "eigen":
            self._fit_eigen(X, y)
        else:
            raise ValueError(f"Solver '{self.solver}' is not yet supported.")

    def _fit_svd(self, X, y):
        """Fits the LDA model using Singular Value Decomposition (SVD).

        Args:
            X: (np.ndarray) - Training feature data.
            y: (np.ndarray) - Training target data.
        """
        X_centered = X - np.mean(X, axis=0)  # Center the data
        U, S, Vt = np.linalg.svd(
            X_centered, full_matrices=False
        )  # Perform SVD, U and Vt are the left and right singular vectors, S is the singular values
        rank = np.sum(
            S > 1e-10
        )  # Compute the rank of the matrix, i.e., the number of singular values greater than 1e-10

        # Select only the top components
        U = U[:, :rank]
        S = S[:rank]
        Vt = Vt[:rank, :]

        self.scalings_ = (
            Vt.T / S
        )  # Compute the transformation matrix, i.e., the scalings
        self.means_ = {
            cls: mean @ self.scalings_ for cls, mean in self.means_.items()
        }  # Transform the means, i.e., the mean of each feature per class
        self.covariance_ = np.diag(
            1 / S**2
        )  # Transform the covariance matrix, i.e., the inverse of the singular values squared

    def _fit_lsqr(self, X, y):
        """Fits the LDA model using Least Squares (LSQR).

        Args:
            X: (np.ndarray) - Training feature data.
            y: (np.ndarray) - Training target data.
        """
        # Create matrix for class-specific means
        _mean_matrix = np.vstack([self.means_[cls] for cls in self.classes_])

        # Solve least squares problem
        X_centered = X - np.mean(X, axis=0)
        Y = np.zeros((X.shape[0], len(self.classes_)))
        for i, cls in enumerate(self.classes_):
            Y[:, i] = (y == cls).astype(float)

        # Solve the normal equations using least squares
        coef, residuals, rank, singular_values = linalg.lstsq(X_centered, Y)

        # Compute scalings
        self.scalings_ = coef

        # Transform class means and store them back in the dictionary
        self.means_ = {
            cls: (self.means_[cls] @ self.scalings_) for cls in self.classes_
        }

        # Update the covariance matrix to match the transformed data
        self.covariance_ = np.cov(X_centered @ self.scalings_, rowvar=False)

    def _fit_eigen(self, X, y):
        """Fits the LDA model using eigenvalue decomposition.

        Args:
            X: (np.ndarray) - Training feature data.
            y: (np.ndarray) - Training target data.
        """
        # Center the data by subtracting the global mean
        _X_centered = X - np.mean(X, axis=0)

        # Compute the total scatter matrix (within-class + between-class)
        # We'll compute this by first computing the within-class scatter matrix
        Sw = np.zeros((X.shape[1], X.shape[1]))
        Sb = np.zeros((X.shape[1], X.shape[1]))

        for cls in self.classes_:
            X_cls = X[y == cls]
            X_cls_centered = X_cls - self.means_[cls]

            # Within-class scatter matrix
            Sw += X_cls_centered.T @ X_cls_centered

            # Between-class scatter matrix
            cls_mean_centered = self.means_[cls] - np.mean(X, axis=0)
            Sb += len(X_cls) * (
                cls_mean_centered[:, np.newaxis] @ cls_mean_centered[np.newaxis, :]
            )

        # Solve the generalized eigenvalue problem: Sb @ w = Î» * Sw @ w
        # This requires inverting the within-class scatter matrix
        eigenvalues, eigenvectors = linalg.eigh(np.linalg.inv(Sw) @ Sb)

        # Sort eigenvalues and eigenvectors in descending order
        idx = np.argsort(eigenvalues)[::-1]
        eigenvalues = eigenvalues[idx]
        eigenvectors = eigenvectors[:, idx]

        # Select top eigenvectors
        n_components = len(self.classes_)
        self.scalings_ = eigenvectors[:, :n_components]

        # Transform class means
        self.means_ = {
            cls: (self.means_[cls] @ self.scalings_) for cls in self.classes_
        }

        # Transform the covariance matrix
        # Use the inverse of the eigenvalues as the diagonal of the transformed covariance
        self.covariance_ = np.diag(1.0 / (eigenvalues[:n_components] + 1e-11))

    def predict(self, X):
        """Predicts class labels for the input data.

        Args:
            X: (np.ndarray) - Test feature data.

        Returns:
            predictions: (np.ndarray) - Predicted class labels.
        """
        scores = self.decision_function(X)  # Compute the decision function
        return self.classes_[
            np.argmax(scores, axis=1)
        ]  # Return the class with the highest score

    def decision_function(self, X):
        """Computes the log-likelihood of each class for the input data. The decision function is the log-likelihood of each class.

        Args:
            X: (np.ndarray) - Test feature data.

        Returns:
            scores: (np.ndarray) - Log-likelihood of each class for the input samples.
        """
        scores = []
        # Compute log-likelihood of each class
        for cls in self.classes_:
            mean = self.means_[cls]  # Mean of each feature per class
            prior = self.priors_[cls]  # Prior probability of class cls

            # Score is the log-likelihood of each class
            score = (
                X @ np.linalg.inv(self.covariance_) @ mean.T
                - 0.5 * mean @ np.linalg.inv(self.covariance_) @ mean.T
                + log(prior)
            )

            scores.append(score)  # Append the score to the list of scores
        return np.array(
            scores
        ).T  # Return the scores as a numpy array, with each row corresponding to a sample and each column corresponding to a class


class QuadraticDiscriminantAnalysis:
    """Implements Quadratic Discriminant Analysis.

    The quadratic term allows for more flexibility in modeling the class conditional
    A classifier with a quadratic decision boundary, generated by fitting class conditional densities to the data and using Bayes' rule.

    Args:
        priors: (np.ndarray), optional - Prior probabilities of the classes (default is None).
        reg_param: (float), optional - Regularization parameter (default is 0.0).
    """

    def __init__(self, priors=None, reg_param=0.0):
        """Initialize the Quadratic Discriminant Analysis model with the specified prior probabilities and regularization parameter.

        Args:
            priors: (np.ndarray), optional - Prior probabilities of the classes (default is None).
            reg_param: (float), optional - Regularization parameter (default is 0.0).
        """
        self.priors = priors

        assert reg_param >= 0.0, "Regularization parameter must be non-negative."
        self.reg_param = reg_param

    def fit(self, X, y):
        """Fit the model according to the given training data. Uses the means and covariance matrices of each class.

        Args:
            X: (np.ndarray) - Training feature data.
            y: (np.ndarray) - Training target data.
        """
        self.classes_ = np.unique(y)  # Unique classes, i.e., distinct class labels
        self.means_ = {}  # Mean of each feature per class
        self.covariances_ = {}  # Covariance matrix of each class
        if self.priors is None:  # Prior probabilities of each class
            self.priors_ = {}
        else:
            self.priors_ = self.priors

        # Compute mean and prior for each class, and covariance
        for cls in self.classes_:
            X_cls = X[y == cls]  # Data points corresponding to class cls
            self.means_[cls] = np.mean(X_cls, axis=0)  # Mean of each feature per class
            self.priors_[cls] = (
                X_cls.shape[0] / X.shape[0]
            )  # Prior probability of class cls

            cov = np.cov(X_cls, rowvar=False)  # Covariance matrix of all features

            if self.reg_param > 0.0:
                self.covariances_[cls] = (
                    cov + np.eye(cov.shape[0]) * self.reg_param
                )  # Add regularization term to diagonal
            else:
                self.covariances_[cls] = (
                    cov + np.eye(cov.shape[0]) * 1e-6
                )  # Add small value to diagonal

    def predict(self, X):
        """Perform classification on an array of test vectors X.

        Args:
            X: (np.ndarray) - Test feature data.

        Returns:
            predictions: (np.ndarray) - Predicted class labels.
        """
        scores = self.decision_function(X)
        return self.classes_[np.argmax(scores, axis=1)]

    def decision_function(self, X):
        """Apply decision function to an array of samples.

        The decision function is the log-likelihood of each class.

        Args:
            X: (np.ndarray) - Test feature data.

        Returns:
            scores: (np.ndarray) - Log-likelihood of each class for the input samples.
        """
        scores = []
        # Compute log-likelihood of each class
        for cls in self.classes_:
            mean = self.means_[cls]  # Mean of each feature per class
            cov = self.covariances_[cls]  # Covariance matrix of each class
            prior = self.priors_[cls]  # Prior probability of class cls

            # Compute the inverse of the covariance matrix and the log-determinant of the covariance matrix
            inv_cov = np.linalg.inv(cov)
            log_det_cov = np.log(np.linalg.det(cov))

            # Compute the log-likelihood
            score = -0.5 * np.sum(
                (X - mean) @ inv_cov * (X - mean), axis=1
            )  # Quadratic term
            score -= 0.5 * log_det_cov  # Log-determinant term
            score += log(prior)  # Prior term

            scores.append(score)  # Append the score to the list of scores
        return np.array(
            scores
        ).T  # Return the scores as a numpy array, with each row corresponding to a sample and each column corresponding to a class


def make_sample_data(
    n_samples, n_features, cov_class_1, cov_class_2, shift=None, seed=0
):
    """Generates sample data for testing LDA and QDA models.

    Args:
        n_samples: (int) - Number of samples per class.
        n_features: (int) - Number of features.
        cov_class_1: (np.ndarray) - Covariance matrix for class 1.
        cov_class_2: (np.ndarray) - Covariance matrix for class 2.
        shift: (list), optional - Shift applied to class 2 data (default is [1, 1]).
        seed: (int), optional - Random seed for reproducibility (default is 0).

    Returns:
        X: (np.ndarray) - Generated feature data.
        y: (np.ndarray) - Generated target labels.
    """
    if shift is None:
        shift = [1, 1]
    rng = np.random.RandomState(seed)
    X = np.concatenate(
        [
            # Data points for class 1, generated by multiplying a random matrix with the covariance matrix of class 1
            rng.randn(n_samples, n_features) @ cov_class_1,
            # Data points for class 2, generated by multiplying a random matrix with the covariance matrix of class 2 and adding [1, 1]
            rng.randn(n_samples, n_features) @ cov_class_2 + np.array(shift),
        ]
    )
    y = np.concatenate([np.zeros(n_samples), np.ones(n_samples)])
    return X, y
